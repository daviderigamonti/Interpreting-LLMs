\documentclass[aspectratio=169, 12pt]{beamer}

\usepackage[bgphoto]{Configuration_Files/beamerthemepolimi}

% Full instructions available at:
% https://github.com/elauksap/beamerthemepolimi

% Set custom font (requires to compile with XeLaTeX).
\usepackage{ifxetex}
\ifxetex
    \usepackage{fontspec}
    \setsansfont[Scale=0.95]{Arial}
\fi

% STANDARD MATH PACKAGES
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[overload]{empheq}  % For braced-style systems of equations

% REFERENCES PACKAGES
\usepackage[capitalize]{cleveref}

% COLOR
\usepackage{xcolor}

\newcommand{\gbm}[1]{\bm{\mathbf{#1}}} % General Bold Math for both greek and latin letters
\newcommand{\rowcolorhang}[1]{\rowcolor{#1}[\dimexpr\tabcolsep+0.1pt\relax]} % Add overang to table row coloring, to avoid vertical white lines
\creflabelformat{equation}{#2\textup{#1}#3} % Remove parentheses from cleveref equations
\newcommand{\T}{\mathrm{T}} % Transpose

\title{Interpreting Large Language Models Through the Lens of Embedding-Oriented Visualizations: Markov Models, Sankey Diagrams and Comparative Approaches}
%\subtitle{}
\author{Davide Rigamonti}
\advisor{Prof. Mark Carman}
\coadvisors{Nicolò Brunello, Vincenzo Scotti}
\date{03/04/2025}

\begin{document}

    \begin{frame}
        \maketitle
        % If the theme option "nologo" is specified, a custom logo
        % can be added with the following commands:
        %\begin{tikzpicture}[overlay, remember picture]
        %    \node at (current page.north) [anchor=north, inner sep=2cm]
        %    {
        %        \includegraphics[width=0.3\paperwidth]{logo_centrato_BN_negativo.png}
        %    };
        %\end{tikzpicture}
    \end{frame}
    
    \begin{frame}{Table of Contents}
      \tableofcontents
    \end{frame}
    
    \section{Introduction}

    \begin{frame}{}
        \textcolor{red}{TODO}
    \end{frame}

    \begin{frame}{}
        \begin{itemize}[<+|visible@+->]
            \item \textbf{Question:} How can we visualize internal states of LLMs in an immediate and interactive way?
            \item \textbf{Proposed Solution:} Development of an interactive visualization tool for LLMs, \emph{InTraVisTo (Inside Transformer Visualization Tool)}, designed to support machine learning researchers in hypothesis formulation by offering a unique perspective on the internal representations of LLMs.
        \end{itemize}
    \end{frame}

    \begin{frame}{}
        \begin{itemize}[<+|visible@+->]
            \item<1-1> \textbf{Question:} Do autoregressive LLMs retain linear properties in their embedding spaces?
            \item<2-4> \textbf{Proposed Solution:} Use experiments based on analogies to assess the relationship between distance and semantic/morphological similarity on the embedding spaces of recent models.
            \begin{itemize}[<+|visible@+->]
                \item<3-3> Identify key differences in the embedding spaces generated by classic transformer architectures against autoregressive LLMs.
                \item<4-4> Clarify how the shift from the input representation to the output one affects the embedding spaces involved through the layers of an LLM.
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}{}
        \begin{itemize}[<+|visible@+->]
            \item<1-1> \textbf{Question:} Is it possible to obtain first-order predictions by concatenating embedding spaces in LLMs?
            \item<2-4> \textbf{Proposed Solution:} Compare a trained bigram Markov model and an identity matrix against the transition matrix of a First Order Model (FOM) obtained by multiplying the embedding and unembedding matrices of an LLM.
            \begin{itemize}[<+|visible@+->]
                \item<3-3> Evaluate similarity or dissimilarity between the transition matrices using direct matrix comparison and set similarity metrics.
                \item<4-4> Evaluate similarity or dissimilarity between the transition matrices using probabilistic metrics such as perplexity and KL divergence.
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}{}
        \textcolor{red}{TODO}
    \end{frame}

    \section{State of the Art}

    \begin{frame}{}
        \textcolor{red}{TODO}
        One of the most direct methods to comprehend a model's hidden representations is by employing its own vocabulary to derive plausible interpretations.
        \textbf{Vocabulary space decoding} techniques are founded on this principle, by utilizing the model's existing vocabulary they can generate outputs that are immediately understandable and may unveil hidden patterns inside the model's generation process.
    \end{frame}

    \begin{frame}{}
        \textcolor{red}{TODO}
        A more recent iteration of the vocabulary space decoding approach is the \emph{LM Transparency Tool (LM-TT)}, which is an exceptional toolkit offering interactive tools for analyzing the internal workings of Transformer models.
        LM-TT builds on top of the circuital interpretation of the Transformer to visualize the information flow.
        The LM-TT tool focuses on the visualization of the most relevant attention paths leading to the production of an embedding in the internal states of the Transformer.
        Additionally, it also provides useful decoding information that can aid the interpretation of intermediate representations at varying degrees of granularity.
    \end{frame}

    \section{Transformer Visualization}

    \subsection{Methodology}

    \begin{frame}{}
        \textcolor{red}{TODO}
        InTraVisTo allows decoding and inspection of the \emph{main four vectors} that compose each layer, while offering a human-interpretable representation of each hidden state by performing a decoding operation.
        The four vectors correspond respectively to the output of the attention component, the intermediate state given by the addition of the attention component to the residual stream, the output of the feed-forward network component, and the layer output.
        On the other hand, the decoding operation is carried out using a specialized decoder which, given a hidden state as input, finds related tokens from the model's vocabulary with the goal of returning an interpretable output.
    \end{frame}

    \subsubsection{Decoding Interface}
    \begin{frame}{}
        Decoding the meaning of hidden state vectors at various depths of a Transformer stack is essential for providing an intuition as to how the model is working.

        \begin{equation*}
        \begin{gathered}
            \uncover<+|visible@1+->{
                P(\ \cdot \mid \gbm{x}, d_{\textit{dec}}, n_{\textit{norm}}) = P(\ \cdot \mid \gbm{x}, \gbm{W}_{d_{\textit{dec}}}, N_{n_{\textit{norm}}}) = \operatorname{softmax}\Bigl(N_{n_{\textit{norm}}}(\gbm{x}) \cdot \gbm{W}_{d_{\textit{dec}}}\Bigr)
            }\\[15pt]
            %
            \uncover<+|visible@2+->{
                N_{n_{\textit{norm}}}(\gbm{x}) = 
                \left\{
                \begin{array}{cl}
                    \gbm{x} &\ \text{if}\ n_{\textit{norm}} = \text{`no normalization'} \\
                    \mathcal{N}{(\gbm{x})} &\ \text{if}\ n_{\textit{norm}} = \text{`normalize only'} \\
                    \gbm{\gamma}_\ell \cdot \mathcal{N}{(\gbm{x})} + \gbm{\beta}_\ell &\ \text{if}\ n_{\textit{norm}} = \text{`normalize and scale'}
                \end{array}
                \right.
            }\\[15pt]
            %
            \uncover<+|visible@3+->{
                d_{\textit{dec}} \in \bigl\{\text{`input'}, \text{`output'}, \text{`linear'}, \text{`quadratic'}, \text{`max-prob'} \bigr\}
            }\\
        \end{gathered}
        \end{equation*}
    \end{frame}

    \begin{frame}{}
        \begin{itemize}
            \item Natural choices for decoders are the transpose of the \emph{input embedding matrix} $\gbm{W}_{\textit{in}}^\T$, and the \emph{output unembedding matrix} $\gbm{W}_{\textit{out}}$.
            \item InTraVisTo also offers the possibility of decoding hidden states by \emph{interpolating} the input and output decoders based on the layer depth $\ell\in\{0,\ldots,L\}$:
        \end{itemize}
        \begin{equation*}
        \begin{aligned}
                \gbm{W}_{\textit{linear}}^{(\ell)} &=\left(1-\frac{\ell}{L}\right) \cdot \gbm{W}_{\textit{in}}^\T + \frac{\ell}{L} \cdot \gbm{W}_{\textit{out}} \\
                \gbm{W}_{\textit{quadratic}}^{(\ell)} &=\left(1-\left(\frac{\ell}{L}\right)^2\right) \cdot \gbm{W}_{\textit{in}}^\T + \left(\frac{\ell}{L}\right)^2 \cdot \gbm{W}_{\textit{out}} \\
                \gbm{W}_{\textit{max\_p}} &=\operatornamewithlimits{argmax}_{\gbm{W} \in \{\gbm{W}_{\textit{in}}^\T, \gbm{W}_{\textit{out}}\}} \operatornamewithlimits{max}_{v \in \mathcal{V}} P(v \mid \gbm{x}, \gbm{W}, N_{n_{\textit{norm}}}) \\
        \end{aligned}
        \end{equation*}
    \end{frame}

    \begin{frame}{}
        \textcolor{red}{TODO}
        Layer-by-layer model interpretation.Layers are stacked vertically, from the embedding layer at the bottom to the normalized outputs at the top, forming a grid where the x-axis represents token positions in the sequence and the y-axis represents layer numbers, resulting in a heatmap where each cell represents a token-layer combination containing the main decoded hidden state in token form.
        By hovering on a cell, a pop-up with other possible decoded tokens (secondary representation) along with additional information appears.

        The generation of the heatmap requires the user to choose a \emph{target embedding}, a \emph{decoding strategy} and a \emph{probability} to display.
        The embedding refers to the position of the hidden state vector to be decoded within the layer.
        Conversely, the decoding strategy choice refers to the decoding matrix employed during the decoding process of each visualized hidden state.
        Finally, the probability selector directly affects the quantity used to weight the color grading in the heatmap, and is normally tied to the probability of the obtained decoding for each cell.
    \end{frame}

    \subsubsection{Flow Interface}
    \begin{frame}{}
        \textcolor{red}{TODO}
        The second visualization provided by InTraVisTo is a \emph{Sankey diagram} that aims to depict the information flow through the Transformer network.
    \end{frame}

    \begin{frame}{}
        \textcolor{red}{TODO}
        Nodes in the diagram depict all hidden states contained in each layer, visualizing the four vectors at the same time.
        Like in the heatmap, nodes display the main decoding result as their label and, when hovered upon, show additional information in the form of a pop-up tooltip containing secondary representations.
        Output nodes' tooltips also include the \emph{decoded difference from the previous layer}, obtained between the current and previous output states with the goal of trying to visualize in a human interpretable way the information added by the current layer relative to the previous one.
    \end{frame}

    \begin{frame}{}
        \textcolor{red}{TODO}
        On the other hand, edges represent the amount of relevance carried by the residual stream, showing how components accumulate or disperse this flow as they process information scattered across the model.
        The flow originates from the topmost layer of nodes, equally split between them, and is recursively computed considering the contributions of each encountered node in a way that no amount of flow is ever lost between layers.
        If the user inspects a specific cell in the heatmap by clicking on it, the Sankey diagram adapts by recalculating the flow considering the clicked node as the sole topmost node, consequently accounting for $100\%$ of the flow.
        Nodes are subdivided into three color-coded categories for convenience: \emph{intermediate and output nodes} in blue, \emph{attention nodes} in green and \emph{feed-forward nodes} in pink.
        Moreover, flows exiting from each node inherit the color of their originating node and are given a further shading factor that is proportional to the \emph{Kullback-Leibler divergence} between the decoded hidden state distributions of the nodes that they connect.
        This allows users to appreciate in an immediate way states that exhibit rapid changes in distribution, thus providing a better localization for possible zones of interest.
    \end{frame}

    \subsubsection{Dinamically Changing the Network}
    \begin{frame}{}
        \textcolor{red}{TODO}
        InTraVisTo is designed as an interactive tool, and \emph{embedding injection} is another feature to enhance understanding of the of Transformers internals.
        Injections substitute hidden states with custom embedding representations, forcing the model to adjust its behavior based on the injected information.
        They are performed by clicking on a cell in the heatmap; this action opens a pop-up menu prompting the user for a string of text with the purpose of being encoded into an embedding representation and injected inside the selected hidden state, the injection technique specifying how the new embedding is integrated into the preexisting state, the position of the injection inside the selected layer, the decoding technique used to interpret the aforementioned string of text, and for the option to normalize the injected embedding.
        Once a valid injection is compiled and added, a small card summarizing the injection appears at the top of the interface.
    \end{frame}

    \begin{frame}{}
        \textcolor{red}{TODO}
        Injections can also be performed in the Sankey diagram by clicking on any visible node, triggering the same pop-up menu described above.
        If the chosen node corresponds to a feed-forward or an attention node, there is an additional option that allows the user to remove the node, performing an \emph{ablation}.
        Ablations are processed similarly to injections and remove the selected node by nullifying its contribution to the residual, so its hidden state is still decoded and visible from the heatmap but does not influence the rest of the model.
    \end{frame}

    \subsection{Results}
    \begin{frame}{}
        \textcolor{red}{TODO}
    \end{frame}

    \section{Embedding Analysis}

    \subsection{Methodology}
    \begin{frame}{}
        \textcolor{red}{TODO}
        In this experiment we establish whether geometric relationships can still be modeled inside the input or output embedding space of recent LLM architectures by direct experimentation on \emph{word analogy tasks}, as originally illustrated in.
        Solving word analogies showcases an embedding space's ability to model word semantics in a relatively consistent way, implying the existence of embedding dimensions with associated meanings (even if overlapping or under superposition).
    \end{frame}

    \begin{frame}{}
        \textcolor{red}{TODO}
        As a starting point, we directly compare the performance of the input embeddings belonging to various state-of-the art and older models over the defined analogy task.
        We also take into consideration the unembedding layer of models, which shares a common structure with the actual embedding matrix and should provide meaningful experimental results.
        Additionally, inspired by the decoding approach introduced for InTraVisTo, we also perform analogy experiments on interpolated embeddings.
    \end{frame}

    \subsection{Results}
    \begin{frame}{}
        \textcolor{red}{TODO}
    \end{frame}

    \section{First Order Prediction}

    \subsection{Methodology}
    \begin{frame}{}
        \textcolor{red}{TODO}
        A \emph{First Order Model (FOM)} is derived by removing all intermediate architectural components from an LLM, retaining only the input and output embedding layers along with the residual connections joining them.
        This transformation can be seen as the creation of a \emph{Markov model} whose transition matrix is given by the product of the input and output embedding matrices.
        Although past work has theorized that FOMs represent Markov models on a small scale, in this experiment, we investigate if FOMs derived from actual LLMs exhibit varying degrees of Markovian behavior, while also considering the possibility of FOM transition matrices approximating identity matrices, effectively modeling \emph{weight tying}.
    \end{frame}

    \begin{frame}[plain]{}
        \textcolor{red}{TODO}
        We introduce various approaches to explore the main inquiry: we start by performing a \emph{naïve direct comparison} between transition matrices, followed by the use of \emph{set similarity metrics} (such as top-$k$ accuracy, Jaccard similarity and overlap coefficient) to obtain more reliable estimates, and ending with the computation of \emph{probabilistic metrics} such as perplexity and the Kullback-Leibler divergence (KL divergence).
        Following on the normalization intuition that proved effective for InTraVisTo, we also explore an alternative (FOM with RMS) that incorporates a single \emph{RMS normalization} step between the standard FOM embeddings.
    \end{frame}

    \subsection{Results}
    \begin{frame}{}
        \textcolor{red}{TODO}
    \end{frame}

    \section{Conclusions}

    \begin{frame}{}
        \textcolor{red}{TODO}
    \end{frame}

    \begin{frame}[plain]{}
        \vspace{2cm}
        \begin{minipage}[t][\baselineskip]{\textwidth}
            {
                \center
                \Large\bf Thank You for Your Attention! \par
            }
        \end{minipage}
    \end{frame}

\end{document}
