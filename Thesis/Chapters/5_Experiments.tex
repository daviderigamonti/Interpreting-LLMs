This chapter presents the experiments conducted to evaluate the methodology proposed in \Cref{ch:methodology} for the reference scenarios described in \Cref{ch:research_questions}.
The experiments were initially designed to shed light onto some specific aspects of LLM internal state interpretability via vocabulary decoding, although after some interesting findings, more attention was given to the actual initial and final embedding representations in LLMs.

This chapter is organized similarly to \Cref{ch:research_questions,ch:methodology}, following the three main research questions as a baseline.
Each research question contemplates multiple experiments aimed at providing empirical evidence to elicit a deeper understanding of large transformer architectures through the perspective of the question at hand.
Additionally, each experiment includes a fixed set of key points to formalize the analysis in a structured way.
These points include:
\begin{itemize}
    \item \textbf{Experimental Setup}: complete description of the experiment performed in reference of the theoretical background provided in \Cref{ch:methodology}.
Parameter combinations, hardware specifications and resources employed may also be mentioned in the setup description.
    \item \textbf{Dataset}: summary of data and information utilized for the experiment, \todo{either} for training models or as direct testing material.
    \item \textbf{Models}: list of models, along with brief \todo{specifics}, that have been utilized to perform the experiment or have been employed for auxiliary tasks. 
    \item \textbf{Results}: set of results emerged from the experiment paired with comments, explanations and a breakdown of the findings' possible implications.
\end{itemize}
If the previously identified points overlap for all the identified experiments of any given research question then, those are directly incorporated inside the main section belonging to the question in order to avoid meaningless repetition.
Furthermore, each research question has a final discussion section, which is aimed at comparing the results of all the performed experiments and providing final remarks considering the overall outcomes for the specific inquiry.

\section{Transformer Visualization}

As anticipated in \Cref{sec:rq_intravisto} and explored in \Cref{sec:method_intrvisto}, the experiments tied to this research question are centered around the InTraVisTo tool.
In each experiment we are going to provide various prompts to different models, and explore specific aspects of the extracted internal representations and information flows utilizing InTraVisTo.

\subsection{Experiment 1}

\subsubsection{Experimental Setup}
\todo[green]{experimental setup}

\subsubsection{Dataset}
\todo[green]{dataset}

\subsubsection{Models}
\todo[green]{models}

\subsubsection{Results}

\todo[purple!20]{
We are going to do a walkthrough of the main features of the tool by analyzing the visualizations from a given input prompt and probing the model with embedding injection.

%\citet{DBLP:conf/nips/LiuAGKZ23}
claimed that some model reasoning errors could be tied back to \emph{attention glitches}, minor errors propagated throughout the attention pattern in the model, leading to imperfect state information transferred through the layers.
They tested the model on the Flip-Flop language, but a simpler yet effective use case turned out to be \emph{reversing sequences}.
Thus, the input prompt for our example is: \textit{Write numbers in reverse order. Number: 13843234 Reverse:}.

As depicted in
%\Cref{fig:example_settings}
, the output of the model is not correct as it is \emph{43234\textbf{38}1} instead of \emph{43234\textbf{83}1}.
Let's now start inspecting the models' internals with InTraVisTo.

Observing %
%\Cref{fig:example_heatmap_wrong}
, it is immediately clear that these models (in this case \texttt{mistralai/Mistral-7B-Instruct-v0.2}) present a clear step in the magnitude of probabilities when passing from the intermediate layers to the last layers.
Through manual trials, we saw that this step tends to happen earlier in the layers as the next token becomes more obvious (e.g. see
%\Cref{fig:heatmap}
in the paper the column of token \texttt{\_capital} or \texttt{\_Italy}).
In this case, there exists a difference between the layers where the step for ``correct output tokens'' and the step for the ``wrong output tokens'' (highlighted in red) occur: the latter seems to be one layer late which, assuming our behavioral assumption is correct, implies that this result is not so obvious according to the model.
Then, by inspecting the magnitude of the probabilities for the wrong token, this conclusion seems legitimate due to a lower probability (lighter background color) compared with the correct token probabilities, even at the last layer of the network.

Knowing that in
%\Cref{fig:example_heatmap_wrong} 
the correct final token in the red highlighted box should be \texttt{8} instead of \texttt{3}, let's now check if the model has ever introduced an \texttt{8} during its reasoning.
%\Cref{fig:example_heatmap_multiple}
represents the same slice of the heatmap inside the red box in 
%\Cref{fig:example_heatmap_wrong}
(just shifted below the purple horizontal line and showing nine layers instead of eight), but with different embeddings shown: the embedding coming from the attention, the one after the summation with the residual and the embedding coming from the Feed Forward component (the final embedding, the FF plus the residual is already shown in 
%\Cref{fig:example_heatmap_wrong}
).
Despite the tokens decoded from the attention embeddings not being very informative and the ones coming from the embedding of the attention plus the residuals being similar to the final one, we can notice that in the embeddings coming from the FF, there is an 8 in the fourth-last layer.
Now we can conjecture that the model result was probably wrong due to an incorrect or imprecise internal representation around the $29^{th}$ layer.

To better profile the problem, we can inspect the information flow representation.
%\Cref{fig:example_flow_perc}
shows the flow percentages of the same FF block at the $29^{th}$ layer highlighted before: it is 1.7\% where the other is at least 1.8\%.
This indicates that that particular block is slightly below the average in terms of contributions to the overall information flow.

From these considerations, it seems that at a certain point, the model probably makes the correct computation, but actually forgets the final target of the task (i.e. reversing the sequence).
We can check this conjecture by injecting an embedding and forcing the model to have the embedding of the token \texttt{8} as output embedding of the block at the $29^{th}$ layer.

The injection procedure starts when the user clicks on a cell in the heatmap, writes in the pop-up block the token to inject and presses again the generation button.
The generation process runs as before and redraws all the components, but this time while it is generating the embedding injecting is substituted to the real one at the specified location.
The output after this procedure is \emph{432348313}.
At first, it seems that the problem concerning the two swapped digits is solved, due to the fact that now the \texttt{8} and the \texttt{3} are in the correct positions, but this time the model added a digit at the end of the generation, instead of the new line character as before.
By inspecting the heatmap again,
%\Cref{fig:example_heatmap_inj}
, the user can notice that the probability of token \texttt{8} is notably increased.
Moreover, concerning the last wrong token, it is also clear that in the lower layer the model converged to the new line token (probably due to the end of the length of the sequence), but as the layer level reaches and is above the level of the injection, the output changed to a suboptimal token with respect to the final task.
This could be expected since we are injecting an embedding that was not crafted from the network at that particular layer, so there could be minor modifications that, through causal attention, are influencing the next tokens negatively.
}

\subsection{Discussion}

\todo[purple!20]{
We presented InTraVisTo, a tool to visualize the internal states and the information flow of Transformer Neural Network, the core of LLMs.
We developed InTraVisTo to provide a tool that NLP practitioners can use to understand the internal reasoning steps carried out by a LLM and, possibly, track down the causes of errors like hallucinations.
Ultimately, we hope our tool will help improve the reliability of LLMs.
Our focus at the moment is on providing better decoding and manipulation of hidden states, with focus on information injection, and improving the interface, to make InTraVisTo more user-friendly.
}


\section{Embedding Analysis}

Following the path laid out in \Cref{sec:rq_embeddings,sec:method_embeddings}, the experiments of this section will gravitate around the possibility of identifying linear properties modeling semantic relationships inside the input and output embedding spaces of LLMs.

To explore this inquiry, we have set up various experiments aimed at replicating some of the well-established embeddings properties~\cite{mikolov2013} in recent state-of-the-art architectures.
Additionally, we expand upon these ideas and provide further explanations for the behaviors emerged from the performed experiments.

\subsection{Dataset}

The dataset used for all the experiments under this research question was created by including the original analogy dataset employed by word2vec~\cite{mikolov2013} and the \textit{BATS (Bigger Analogy Test Set)}~\cite{drozd2016}.
The word2vec dataset (also known as Google analogy dataset) is structured in two main files: \textit{question-words} and \textit{question-phrases}.
Question-words contains $14$ categories for a total of \todo{$x$} analogies spacing from linguistic relations to semantic relations, while question-phrases only features $5$ classes and a total of \todo{$x$} analogies regarding common-knowledge relations involving proper nouns.
BATS features a more complex hierarchy since it is meant to be an improvement over the Google dataset: it covers $4$ main relation types (inflectional morphology, derivational morphology, lexicographic semantics and encyclopedic semantics), each one being subdivided into $10$ further sub-categories containing $50$ unique word pairs each.
In addition, BATS provides multiple correct answers for word pairs when applicable. 

Due to the different format in which the two datasets have been released (BATS containing word pairs, while Google analogy dataset being comprised of already constructed analogies), some work has been put into collecting, standardizing and unifying analogies.
To this end, BATS word pairs have been combined into a total of $(50 \times 49) \times 10 \times 4 = 98000$ unique analogies to be combined with the \todo{$x + x = x$} analogies provided by the Google dataset.
After the removal of duplicate entries, the final comprehensive total of unique analogies amounts to \todo{$x$}.

\todo[green]{handling of multiple correct answers}
\todo[green]{address possible selection of some sub-categories for some experiments}
\todo[green]{address dataset reduction to single tokens for some experiments}

% Some of the entries within the dataset that featured underscores to delimit compound words, where modified to use spaces instead.

\subsection{Experiment 1}

As a starting point, we directly compare the performance of various state-of-the art models over the defined analogy task.
We expect to observe similar results for architectures that are similar to one another, while accounting for the vocabulary and vector sizes of embeddings.

Interestingly, \citet{drozd2016} finds that scaling the vector size of embeddings has mixed effects in terms of performance when evaluating analogies with our chosen metrics.
Whereas logically, an increase in vocabulary size should always constitute an improvement over analogy resolution.
In this experiment we will verify the validity of these statements on the embeddings contained in large scale and newer models, while considering possible limitations in embedding expressiveness of these novel architectures.
Therefore, we expect to see a clear distinction in the results achieved by newer models against older architectures used as a baseline, that is an inverse trend which favors the embeddings built and trained over old frameworks in analogy resolution.

\subsubsection{Models}

Experiments were conducted using the input embeddings of bert-large-cased \todo[orange]{cite}, GPT-2 \todo[orange]{cite}, Gemma 2 \todo[orange]{cite}, LLaMA-2-7B \todo[orange]{cite}, LLaMA-3-7B \todo[orange]{cite}, Mistral-7B \todo[orange]{cite} and Phi-3-small-8k-instruct \todo[orange]{cite}.
Additionally, the word embeddings generated by Word2Vec \todo[orange]{cite} and GloVe \todo[orange]{cite} will also be included to be evaluated as a baseline.

\todo[green]{describe model architectures with reference to a table}
% [Mistral, llama2] (both with a vocabulary size of 32,000 and an embedding size of 4,096 dimensions).
% Additionaly, GPT-2 (with a vocabulary size of 50,257 tokens and embedding size of 768 dimensions) was also included to serve as a benchmark due to it being a non-LLM decoder-only transformer model.

The evaluation of embeddings sourced from such a diverse set of models cannot be fair in nature.
More specifically, no effort has been made to align the datasets \todo{with which} these architectures were trained, opting for comparing their most popular iterations instead.
The chosen approach has clear limitations, but it is not within our scope to evaluate these models with the intention of finding the \todo{best performing one}.
Rather, we are interested in assessing the presence of high-level trends that justify the employment of an approach of less granular nature.

\subsubsection{Results}

\todo[purple!20]{
It becomes apparent that the majority of LLMs seem to underperform with respect to GPT-2, this notable deviation in performance could stem from various factors.
However, by inspecting Fig., which displays the same results, but for the single-token version of dataset, we can observe that the performance of GPT-2 alignes more closely with that of the other models.
This mismatch in performance may be attributed on the tokenization strategy and vocabulary size of the chosen models, in fact all three models employ a Byte-Pair Encoding (BPE) tokenizer 
%\cite{DBLP:conf/aaai/WangCG20}
, however the tokenizers utilized by both Mistral and LLaMA are based on SentencePiece 
%\cite{DBLP:conf/emnlp/KudoR18}
, presenting a smaller overall vocabulary (32,000 tokens against GPT-2's 50,257 tokens), thus possibly implying a reduced selection of full-word tokens in favor of an increased number of sub-word tokens.
By analyzing results on the single-only dataset in Fig., we are able to infer that the exclusion of multi-token words from the dataset also mitigates differences arising from how tokenizers segment words.
This effectively normalizes away instances of analogies where GPT-2 may have held an advantage over other LLMs by being able to fully encode certain words.
For the same reason, GPT-2 would have had both better representations encompassing semantic meanings of words without additional noise caused by multiple words sharing the same sub-word token, but also not having to be subject to the token-reconstruction previously mentioned strategies as ways to deal with multi-token words.
}

\subsection{Experiment 2}

For this second experiment we wish to delve deeper into the actual capabilities of the embeddings belonging to recent decoder-only LLMs.

\todo[green]{explain}

\subsubsection{Models}

% It is important to note that, whenever feasible, both input and output embeddings (derived from the weights of the decoder at the end of the model) were used and analyzed independently.

Experiments were conducted using the input embeddings of bert-large-cased \todo[orange]{cite}, GPT-2 \todo[orange]{cite}, Gemma 2 \todo[orange]{cite}, LLaMA-2-7B \todo[orange]{cite}, LLaMA-3-7B \todo[orange]{cite}, Mistral-7B \todo[orange]{cite} and Phi-3-small-8k-instruct \todo[orange]{cite}.
Additionally, the word embeddings generated by Word2Vec \todo[orange]{cite} and GloVe \todo[orange]{cite} will also be included to be evaluated as a baseline.

\todo[green]{describe model architectures with reference to a table}


The evaluation of embeddings sourced from such a diverse set of models cannot be fair in nature.
More specifically, no effort has been made to align the datasets \todo{with which} these architectures were trained, opting for comparing their most popular iterations instead.
The chosen approach has clear limitations, but it is not within our scope to evaluate these models with the intention of finding the \todo{best performing one}.
Rather, we are interested in assessing the presence of high-level trends that justify the employment of an approach of less granular nature.

\subsubsection{Results}

\todo[purple!20]{
Our analysis also reveals that for increasing $k$ values, output embeddings seem to consistently outperform input embeddings (contextually to the model performance), while for smaller $k$ values input embeddings demonstrate greater accuracy.
This phenomenon suggests that input embeddings provide more reliable estimates for analogies results, but their vectors are laying in a sparser latent spaces, thus making it less likely to accidentaly include the correct vector out of a wrong prediction by extending the range given by $k$.
Whereas output embeddings latent spaces are more compressed and benefit more from greater $k$ values.
Such theory is corroborated by the fact that the trend that was previously identified is much less relevant in Fig., where only single-token analogies are considered.
Furthermore, we observe that the difference between input and output embeddings at earlier $k$ values is more accentuated, and the performance of output embeddings almost never surpasses the one of input embeddings.
This may be due to the fact that overall (and in particular for input embeddings), it is less likely for the model to output a wrong result for any given single-token only analogy, therefore the margin of analogies with wrong answers according to input embeddings and only correct for output embeddings due to the fact that we are utilizing a large $k$ is greatly reduced.

Finally, we show that input and output embedding pairs belonging to the same model tend to have the same trend, and are generally more prone to end up having similar accuracies, which is not a completely trivial result since, despite having the same dimensionality and possibly sharing some of the datasets used to train them (depending on the training setup and initialization values), they still operate with completely different weight matrices and carry out different tasks inside the model.
}

\subsection{Discussion}

\todo[purple!20]{
In the end, we can say that recent LLMs still retain a certain amount of facts inside their embeddings, albeit in a noticeably less developed and more noisy way with regards to older models.
}

\section{First Order Prediction}

Building on the foundation established in \Cref{sec:rq_fom}, the experiments in this section will focus on exploring the implications of creating a First Order Model (FOM).
A FOM is obtained by removing the all intermediate architectural components from an LLM, leaving behind the input and output embedding layers, and the residual connections joining them.
As explained in \Cref{sec:method_fom}, this operation can be seen as the creation of a Markov model possessing a transition matrix formed by the product between the input and output embedding matrices.

Most of the experiments in this section are geared towards understanding if the FOM actually represents a faithful bigram Markov model over the original model's vocabulary.
Although past work has theorized and partially demonstrated on a restricted set of models the possibility of this hypothesis being \todo{correct}~\cite{elhage2021}, we suspect that FOMs extracted from different LLMs may have different degrees of \todo{`Markovness'} and we strive to explore this intuition in this section.
\todo[green]{Add part on weight tying}

\subsection{Experiment 1}


\subsubsection{Dataset}

\todo[purple!20]{
We trained the unigram Markov model on the \emph{WikiText 2} 
%\cite{DBLP:conf/iclr/MerityX0S17}
language modelling dataset.
We ran the evaluations using each considered model's tokenizers and vocabulary.
Both tokenizers have a vocabulary of 32,000 tokens, yielding a total of 3,499,853 tokens if considering LlaMA's tokenizer vocabulary and 3,423,005 tokens if considering Mistral's tokenizer vocabulary.
}

\subsubsection{Models}

\todo[purple!20]{
Experiments were conducted on LLaMA-2-7B 
%\cite{DBLP:journals/corr/abs-2307-09288}
and Mistral-7B 
 %\cite{DBLP:journals/corr/abs-2310-06825}
models, since they both present different input and output embeddings.
We focused solely on the extraction of input and output embedding weights to construct the FOM, discarding the remaining components of the model infrastructure.
}

\subsubsection{Results}

\todo[purple!20]{
By observing Fig. 
%\ref{fig:RQ2_mistral}
 that depicts Mistral's performance on the self-regression and unigram-regression tasks, we can notice how, for any given value of $k$, the predictions generated by the FOM appear to align more closely with the inputs of the FOM rather than the most probable outputs generated by the unigram Markov model.
This observation holds even when considering that, for the sake of this comparison, all traces (besides the one having $k_2 = 1$) are essentially more lenient variants of the corresponding top-$k_1$ prediction, achieved by expanding the range of valid subsequent tokens generated by the Markov model.
This seems to suggest that the embeddings may exhibit a closer relationship to their inverse, rather than serving as representations of the subsequent token prediction based on the input.
Unfortunately, this observation appears to contradict the recorded matrix distances, since we have a distance of 178.7063 between the FOM transition matrix and the identity matrix and a distance of 76.9151 between the FOM and Markov model transition matrices.

However, these results are relative to LLaMA's performance on the same exact tasks.
By looking at Fig. 
%\ref{RQ2:llama_self}
, which illustrates its perfomance on the self-regression task and Fig. 
%\ref{RQ2:llama_markov}
 representing its performance on the unigram-regression, we notice opposite results, the LLaMA FOM exhibits greater top-$k$ accuracy when compared to the unigram Markov model rather than its own inputs.
This peculiarity is noticeable by empirically examining the LLaMA FOM predictions for some common tokens, which appear to be plausible next tokens:
}

\begin{table}
\centering
\begin{tabular}{||c || c | >{\columncolor[gray]{0.9}}c | c||} 
\hline
Input token/Model   & Mistral FOM   & LLaMA FOM & Markov models \\ 
\hline\hline
mount               & gorith        & clock     &  s            \\ 
easy                & /******/      & going     & to            \\
hair                & teenth        & loss      & ,             \\
\hline
\end{tabular}
\end{table}

\todo[purple!20]{
Furthermore, the recorded matrix distances reveal a notable disparity in scale compared to those obtained using Mistral's FOM.
Specifically, a distance of 522.3115 observed between the FOM transition matrix and the identity matrix and a distance of 509.2659 recorded between the FOM and Markov model transition matrices.
Even for a model so clearly predisposed towards predicting the next word rather than returning the input word itself, the difference between the distances is minimal.
This suggests a potential bias towards lower distance estimates between the FOM transition matrix and the Markov transition matrix, possibly due smaller quantities of null values present in the latter matrix with regard to the identity matrix.
}

\subsection{Discussion}

\todo[purple!20]{
Overall, the feasibility of a FOM approximation appears highly dependent on the specific model under analysis, although overall, all analyzed First Order Models comprised by the combination of their input and output embeddings seem to exhibit a slight bias towards trying to approximate a unigram first order Markov model.
Extracting the embedding weights from the transition matrix of a unigram Markov model and utilizing them to initialize the embedding layer and decoder weights of an LLM would be an interesting potential development of this idea in future work.
}