This chapter presents the experiments conducted to evaluate the methodology proposed in \cref{ch:methodology} for the reference scenarios described in \cref{ch:research_questions}.
The experiments were initially designed to shed light onto some specific aspects of LLM internal state interpretability via vocabulary decoding, although after some interesting findings, more attention was given to the actual initial and final embedding representations in LLMs.

This chapter is organized similarly to \cref{ch:research_questions,ch:methodology}, following the three main research questions defined as a baseline.
Each research question contemplates multiple experiments aimed at providing empirical evidence to elicit a deeper understanding of large transformer architectures.
This understanding is achieved through the perspective of the question at hand and, more generally, with the aid of InTraVisTo.
Where applicable, experiments include a fixed set of key points to formalize the analysis in a structured way.
These points include:
\begin{itemize}
    \item \textbf{Experimental Setup}: complete description of the experiment performed in reference of the theoretical background provided in \cref{ch:methodology}.
Parameter combinations, hardware specifications and resources employed may also be mentioned in the setup description.
    \item \textbf{Dataset}: summary of data and information utilized for the experiment, \todo{either} for training models or as direct testing material.
    \item \textbf{Models}: list of models, along with brief \todo{specifics}, that have been utilized to perform the experiment or have been employed for auxiliary tasks. 
    \item \textbf{Results}: set of results emerged from the experiment paired with comments, explanations and a breakdown of the findings' possible implications.
\end{itemize}
If the previously identified points overlap for all the \todo{identified} experiments of any given research question then, those are directly incorporated inside the main section belonging to the question in order to avoid meaningless repetition.
Furthermore, each research question has a final discussion section, which is aimed at comparing the results of all the performed experiments and providing final remarks considering the overall outcomes for the specific inquiry.

\section{Transformer Visualization}\label{sec:exp_intravisto}

As anticipated in \cref{sec:rq_intravisto} and explored in \cref{sec:method_intrvisto}, the experiments tied to this research question are centered around the InTraVisTo tool.
The first points (\cref{sec:exp_intravisto_exp1,sec:exp_intravisto_exp2}) within this section provide an exhaustive overview of the interface, diving into the technical aspects and explaining the role of components that are present in the application.
Consequently, these points do not present the experimental structure defined in the introductory section of this chapter.
Whereas, the last point (\cref{sec:exp_intravisto_exp3}) is dedicated to the exploration of the proposed tool's actual capabilities, taking into consideration specific use cases and small investigations to provide concrete examples of possible usage scenarios.

\subsection{Decoding Interface}\label{sec:exp_intravisto_exp1}

Decoding the meaning of hidden state vectors at various depths of a transformer stack is essential for providing an intuition as to how the model is working.
InTraVisTo allows decoding and inspection of the main four vectors (defined in \cref{ssec:method_intravisto_decoding}) that compose each layer, while offering a human-interpretable representation of each hidden state by performing a decoding operation.
This decoding operation is carried out using a specialized decoder which, given a hidden state as input, finds related tokens from the model's vocabulary with the goal of returning an interpretable output.

In this section, we present the first visual output of InTraVisTo.
Our goal revolves around a layer-by-layer interpretation of the model, thus layers are stacked vertically starting from the bottom with the embedding layer up to the top with the normalized outputs of the last layer.
Due to the inference process of the transformer architecture, each stack of layers is repeated for every token, resulting in a grid where the x-axis represents token positions in the sequence and the y-axis represents layer numbers.
A natural visual representation for this grid-like structure is a heatmap where each cell represents a token-layer combination.
Visually, inside each cell we can find the main decoded hidden state, and by hovering on it, a pop-up with its secondary representations along with additional information appears.
In addition, for the sake of being able to tell apart input tokens (prompted by the user) from output tokens (autonomously generated by the model), a vertical line is put in place to divide the former from the latter.
As it is possible to notice from \todo[orange]{fig}, the heatmap features two additional layers: one at the beginning (bottom) and one at the end (top).
The first layer is used to represent states before entering the transformer stack, right after the embedding layer.
Whereas the last layer offers a representation that is forcibly normalized and decoded using the output embedding, in order for it to align with the raw generation output provided by the model.
The generation of the heatmap requires the user to choose a target embedding, a decoding strategy and a probability to display.

The embedding refers to the position of the hidden state vector to be decoded within the layer.
Therefore, by selecting different embedding positions, the user can inspect different subcomponents for each a single layer and understand which \todo{information} is propagated between layers. 
In order to avoid saturating the visualization with information, only a single embedding position can be chosen to be \todo{visualized} at any time from the heatmap.

On the other hand, the decoding strategy choice refers to the decoding matrix utilized in the decoding process of each visualized hidden state, fulfilling the role of ``perspective'' for the interpretation.
Choosing a meaningful decoding strategy is not a trivial task and can be considered the key \todo{aspect} of the vocabulary decoding approach to interpretability.
As already reviewed in \cref{ssec:related_vocab}, many different approaches for decoding hidden states already exist in literature.
InTraVisTo's decoding operation is formalized in \cref{eq:method_intravisto_decoding} and employs a decoding matrix chosen by the user.
Besides some immediate choices such as input embeddings or output embeddings, which focus on providing meaningful decoding for layers at the bottom and layers at the top of the network respectively; InTraVisTo offers the possibility of decoding states using interpolated ( \cref{eq:method_intravisto_linear-interp,eq:method_intravisto_quadratic-interp}) and \todo[green]{max-probability} decoders.
These novel decoding techniques are built in order to give the user a semantically meaningful representation for every layer of the model at the same time.

\todo{Lastly}, the probability selector directly affects the quantity used to weight the color grading in the heatmap.
The four main options consist of P(argmax term), entropy, attention contribution and feedforward contribution.
Additionally, a secondary control labeled as ``Residual Contribution'' affects the attention contribution and feedforward contribution options for the probability selector.
This ``Residual Contribution'' control determines the metric used to evaluate the concept of contribution between transformer components and the residual stream according to \cref{eq:method_intravisto_norm-contrib,eq:method_intravisto_kl-contrib}.
The mathematical background for these selectors has been \todo{treated in depth} in \cref{sssec:method_intravisto_decoding_metrics}.

\todo{On the other hand}, users are given plenty of secondary controls to further explore the models' generation process.
For example, the `Embedding normalization' control gives users the possibility of choosing the type of normalization to perform on hidden states before the decoding, as defined in \cref{eq:method_intravisto_normalization}.
Whereas, another selector is dedicated to handling the strategy for decoding secondary tokens (referencing \cref{sssec:method_intravisto_decoding_tokens}), offering either a `top-5' approach or by using the iterative decoding algorithm, as illustrated in \cref{alg:method_intravisto_iter-dec}.
In addition, users are given the option to ignore the column corresponding to the \emph{<start>} token from the visualization, as decoding it often does not provide meaningful results.
Furthermore, users can affect the generation process by directly choosing a different model from a customizable selection pool or altering the number of generated tokens by using the appropriate selectors.

InTraVisTo is thought to be an interactive tool, so another class of features to aid in the understanding of the internal workings of transformers is the embedding injection.
As previously formalized in \cref{ssec:method_intravisto_injection}, injection \todo{consists in} substituting a hidden state with a custom embedding representation, forcing the model to change its behavior based on the injected information.
Injections can be performed by clicking on a cell in the grid-layout heatmap, this actions opens a pop-up menu related to the cell which prompt the user for the following information:
\begin{itemize}
    \item A string of text with the purpose of being encoded into an embedding representation and injected inside the chosen hidden state.
The application automatically converts such string to an embedding using the inverse transformation of the selected decoder.
If the string contains multiple tokens, then all encoded representations are averaged to obtain a single embedding in accordance to \cref{eq:method_intravisto_emb-avg}.
    \item The injection technique, which controls the way in which the new embedding \todo{gets incorporated} into the preexisting state as specified in \todo[orange]{ref}.
    \item The position of the injection inside the selected layer, allowing users to inject embeddings in every position without changing the heatmap visualization.
    \item The decoding technique used to interpret the aforementioned string of text.
    \item The option to normalize the injected embedding according to the chosen injection technique. 
\end{itemize}

When a valid injection is compiled and added, a small card containing a summary of the injection is created and displayed in a dedicated section at the top of the interface.
This operation prompts an immediate reload of the current generated result by the model in order to include the newly defined injection.
When the heatmap visualization displays the result of a generation process which contained an injection, the cell location corresponding to the injection location is highlighted in \emph{green}.
Injections can be removed from the generation process at any time by pressing the appropriate `X' button on their card.

\todo{subdivide in smaller sections}
\todo[green]{add images}

\subsection{Flow Interface}\label{sec:exp_intravisto_exp2}

The second visualization provided by InTraVisTo is a Sankey diagram that aims to depict the information flow through the transformer network (Fig).

Nodes in the diagram depict all the hidden states contained in each layer, visualizing each one of the four vectors referenced in \cref{ssec:method_intravisto_decoding} at the same time.
Similarly to the heatmap visualization, nodes display the main decoding result as their label and, when hovered upon, provide additional information in the form of a pop-up tooltip containing secondary tokens.
One additional piece of information, exclusively found in the tooltip of output nodes, is the decoded difference from the previous layer.
This detail shows the primary and secondary tokens obtained from decoding the difference between the output state of the current layer with the output state of the previous layer as if it were a separate state.
The goal of this representation is trying to visualize in a human interpretable way, the information added by the current layer with respect to the previous one.

On the other hand, edges represent the amount of relevance carried by the residual stream and visualizes how much components accumulate or disperse it by performing their tasks utilizing information scattered through the model.
Being a Sankey diagram, no amount of flow is ever lost between layers, as the flow corresponding to all tokens in a horizontal section (reprsenting a layer) can be added to obtain $100\%$.
The flow starts from the topmost layer of nodes, and is recursively computed considering the contributions of each encountered node.
More specifically, flow computation is discussed \todo{in depth} in \cref{ssec:method_intravisto_flow} and can be formalized using \cref{eq:method_intravisto_flows}.

When considering a \todo{fresh} generation run, the totality of the flow is evenly split between the output nodes at the end of the transformer stack.
However, if the user decides to inspect a specific cell in the heatmap by clicking on it, the Sankey diagram also adapts by recalculating itself considering the node corresponding to the selected as the sole topmost node, resulting in \todo{it} bearing $100\%$ of the flow.
We subdivide nodes into three categories, color-coded for visualization convenience: intermediate and output nodes in blue, attention nodes in green and feedforward nodes in pink.
Moreover, flows exiting from each node inherit their color from \todo{it}, leading to a clear display of the main information paths and their direction.
Additionally, flows are given a further shading factor that is proportional to the KL divergence between the decoded hidden state distributions of the nodes that they connect.
This allows users to appreciate in an immediate way states that exhibit rapid changes in distribution, thus providing a better localization for possible zones of interest.

The only component that is able to redistribute the flow through various tokens is the attention node, which does so according to the attention weights computed for all preceding tokens.
Furthermore, for each aggregation node (intermediate and output), the flow contribution of the preceding nodes is computed following the ``Residual contribution'' user control mentioned in \cref{sec:exp_intravisto_exp1} according to \cref{eq:method_intravisto_norm-contrib,eq:method_intravisto_kl-contrib}.

InTraVisTo is equipped with additional settings exclusively dedicated to the Sankey diagram visualization.
\todo{First}, we provide a flag for hiding the starting token similar to the one defined in \cref{sec:exp_intravisto_exp1}.
However, in the Sankey's case it comes with an additional control that allows users to reapport the hidden flow to remaining nodes, meaning that the values of hidden flows are set to $0$ and the missing \todo{percentage} is redistributed to visible tokens.
Another important setting is the ``Attention highlight'', which controls the criteria for highlighting attention traces, affecting the number of visible flows related to each attention node.
Users can choose between visualizing all flows, only the top-$k$ flows (considering attention weights), \todo{only} flows with a corresponding attention weight greater than a certain threshold, or no flows at all.
Other graphic-oriented options allow users to remove node labels for nodes that do not constitute the output of a layer, adjust the scale of the diagram and select the depth of the visualization by choosing the number of visible layers.

As mention beforehand in \cref{sec:exp_intravisto_exp1}, it is also possible to perform injections in the Sankey diagram by clicking on any visible node, triggering the same injection pop-up menu referenced before.
If the chosen node corresponds to a feed-forward \todo{node} or an attention node, there is an additional option that allows the user to remove the node, performing an ablation.
Ablations are handled the same way as injections are, therefore a card is created in the top section of the interface, and the generation process is repeated to include the selected changes.
Removing a node is handled by nullifying its contribution to the residual, therefore its hidden state is still properly decoded and can be analyzed from the heatmap, but does not influence the rest of the model.
Similarly to injections, ablations are highlighted in \emph{red} inside the heatmap visualization.

\todo{subdivide in smaller sections}
\todo[green]{add images}

\subsection{Experiment 3}\label{sec:exp_intravisto_exp3}

For this experiment we submit various prompts \todo{a number} of different models and explore specific aspects of the extracted internal representations and information flows utilizing InTraVisTo.

Part of the prompts are set up to induce models \todo{in performing} computations of numerical nature.
It is a well known fact that current general-purpose language models have \todo{demonstrated} poor overall performance on tasks that included the use of numbers and mathematical operators \todo[orange]{cite}.

\subsubsection{Experimental Setup}

By directly experimenting with the proposed tool, we establish a basic workflow that enables researchers to ultimately collect new insights about how LLMs generate tokens in a layer-by-layer fashion.
The first step consists in exploring the secondary representations of internal states in order to find additional information relevant for the task at hand, \todo{as it} is possible that the model is encoding it in the latent dimensions of hidden states alongside the main token, utilizing the residual stream as a communication channel between modules.

Based on the information previously gathered, observing the \todo{overall influence} on hidden states of interest generated by the model is the next step of the inspection process.
This can be achieved by utilizing the Sankey diagram visualization of InTraVisTo, piecing together the contribution of tokens and components that had a major role in the creation of certain intermediate states, enabling the formation of conjectures about the model's inner workings.

Finally, by acting upon these conjectures using the state injection and component ablation tools provided by InTraVisTo, it is possible to generate new knowledge by identifying the root causes that determine certain internal behaviors manifested in the preliminary inspection.
This knowledge can be further generalized by replicating the experiments on multiple models and observing possible similar mechanics at play, even between different architectures.

\subsubsection{Dataset}

The dataset employed for this experiment consists of a small set of curated examples used to elicit model predictions in order to observe meaningful internal states.
As previously mentioned, most prompts revolve around the \todo{execution of} tasks of mathematical and numerical nature.

The following constitutes a comprehensive list containing all main prompts used in the experiment.

\todo[green]{list of prompts}
%\begin{multicols}{3}
%    \begin{itemize}
%        \item Item 1
%        \item Item 2
%        \item Item 3
%        \item Item 4
%        \item Item 5
%        \item Item 6
%        \item Item 7
%        \item Item 8
%        \item Item 9
%        \item Item 10
%    \end{itemize}
%\end{multicols}


\subsubsection{Models}

For the sake of our analysis we propose a limited number of models to be analyzed through InTraVisTo.
In particular, we consider the $4$-bit quantized versions of Mistral Instruct 0.2 7B, LLama 2 7B and the full and the unquantized version of GPT 2 \todo[orange]{cite all}.
However, the application can be deployed in such a way to include most popular model architectures available on huggingface \todo[orange]{link}.
\todo[green]{models}

\subsubsection{Results}

One of the earliest discoveries made via InTraVisTo was the fact that models which employ dedicated tokens for each digit, when asked to perform arithmetic operations, happen to represent decimal positional information along with digits of the result.
For example, the number $1492$ could be represented by having $1$ + ``thousand'', $4$ + ``hundred'', $9$ + ``ninety'' and $2$.
This was initially discovered through the unique usage of the iterative decoding technique \todo[orange]{cref} to inspect secondary decodings of internal states, although it may also be observed in certain primary decodings where the `decimal term' interpretation takes precedence over the digit one. 
One peculiarity of this technique used by models to keep track of the current decimal position of the result, is the fact that tokens used to indicate decimal positions can have peculiar interpretations:
\begin{itemize}
    \item Mistral was reported to use ``century'' to indicate hundreds, 
\end{itemize}

- Mistral 
month names (i.e. ``July'' for 7 and ``February'' for 2), roman numerals, plain text representations in english and other languages and teen equivalents (i.e. ``nineteen'' for 9 and ``eleven'' for 1) as alternatives to the corresponding digits 

\todo[purple!20]{

Thanks to this visualisation, the user is equipped with an overview of the model during generation, for example in Fig, by briefly inspecting the first output layers, one can clearly see a symbolic reasoning fashion taking place: in order to answer to the query (i.e. \emph{What is the capital of Italy?}) the model reached to correct answer (i.e. \emph{Rome}) already at the first generated token, by firstly encoding \emph{city} then \emph{capital} and finally \emph{Rome} into the embeddings.
Just at the very last layer, the answer changed to stick to a more formally correct answer (i.e. starting the answer with \emph{The}), we remind to Section for further examples using the tool.

Referring again to the example in Fig, one could replace the first occurrence of the token \texttt{\_Rome} with the vector for the token \texttt{\_Paris} to see the consequences on the following internal states and the generated completion.
(Doing so results in the model subsequently placing low probability on the injected embedding and ``stopping'' the generation with a placeholder, in order to be able to return to the generation of a correct response containing the token \texttt{\_Rome} instead of \texttt{\_Paris}).

In Fig, the LLM was prompted with the question \emph{``What is the capital of Italy?''}.
The visualisation of decoded internal states helps us to follow the answer generation process, which appears to resemble some form of reasoning process.
The model initially produces an embedding of the token \texttt{\_Rome} in layer 27 of position 10 (immediately following the input text), but then proceeds to generate a more complete response \emph{``The capital city of ...''}, with the token \texttt{\_Rome} postponed to position 16.

For example, supposing in a layer, say layer \emph{28} the token decoded for the final embedding \emph{Residual + feed-forward} is \emph{\_Rome}, one may substitute such vector with the embedding representation of \emph{\_Paris} and see how the generated output and internal state change consequently.

}

\todo[purple!20]{
We are going to do a walkthrough of the main features of the tool by analyzing the visualizations from a given input prompt and probing the model with embedding injection.

%\citet{DBLP:conf/nips/LiuAGKZ23}
claimed that some model reasoning errors could be tied back to \emph{attention glitches}, minor errors propagated throughout the attention pattern in the model, leading to imperfect state information transferred through the layers.
They tested the model on the Flip-Flop language, but a simpler yet effective use case turned out to be \emph{reversing sequences}.
Thus, the input prompt for our example is: \emph{Write numbers in reverse order. Number: 13843234 Reverse:}.

As depicted in
%\cref{fig:example_settings}
, the output of the model is not correct as it is \emph{43234\textbf{38}1} instead of \emph{43234\textbf{83}1}.
Let's now start inspecting the models' internals with InTraVisTo.

Observing %
%\cref{fig:example_heatmap_wrong}
, it is immediately clear that these models (in this case \texttt{mistralai/Mistral-7B-Instruct-v0.2}) present a clear step in the magnitude of probabilities when passing from the intermediate layers to the last layers.
Through manual trials, we saw that this step tends to happen earlier in the layers as the next token becomes more obvious (e.g. see
%\cref{fig:heatmap}
in the paper the column of token \texttt{\_capital} or \texttt{\_Italy}).
In this case, there exists a difference between the layers where the step for ``correct output tokens'' and the step for the ``wrong output tokens'' (highlighted in red) occur: the latter seems to be one layer late which, assuming our behavioral assumption is correct, implies that this result is not so obvious according to the model.
Then, by inspecting the magnitude of the probabilities for the wrong token, this conclusion seems legitimate due to a lower probability (lighter background color) compared with the correct token probabilities, even at the last layer of the network.

Knowing that in
%\cref{fig:example_heatmap_wrong} 
the correct final token in the red highlighted box should be \texttt{8} instead of \texttt{3}, let's now check if the model has ever introduced an \texttt{8} during its reasoning.
%\Cref{fig:example_heatmap_multiple}
represents the same slice of the heatmap inside the red box in 
%\cref{fig:example_heatmap_wrong}
(just shifted below the purple horizontal line and showing nine layers instead of eight), but with different embeddings shown: the embedding coming from the attention, the one after the summation with the residual and the embedding coming from the Feed Forward component (the final embedding, the FF plus the residual is already shown in 
%\cref{fig:example_heatmap_wrong}
).
Despite the tokens decoded from the attention embeddings not being very informative and the ones coming from the embedding of the attention plus the residuals being similar to the final one, we can notice that in the embeddings coming from the FF, there is an 8 in the fourth-last layer.
Now we can conjecture that the model result was probably wrong due to an incorrect or imprecise internal representation around the $29^{th}$ layer.

To better profile the problem, we can inspect the information flow representation.
%\Cref{fig:example_flow_perc}
shows the flow percentages of the same FF block at the $29^{th}$ layer highlighted before: it is 1.7\% where the other is at least 1.8\%.
This indicates that that particular block is slightly below the average in terms of contributions to the overall information flow.

From these considerations, it seems that at a certain point, the model probably makes the correct computation, but actually forgets the final target of the task (i.e. reversing the sequence).
We can check this conjecture by injecting an embedding and forcing the model to have the embedding of the token \texttt{8} as output embedding of the block at the $29^{th}$ layer.

The injection procedure starts when the user clicks on a cell in the heatmap, writes in the pop-up block the token to inject and presses again the generation button.
The generation process runs as before and redraws all the components, but this time while it is generating the embedding injecting is substituted to the real one at the specified location.
The output after this procedure is \emph{432348313}.
At first, it seems that the problem concerning the two swapped digits is solved, due to the fact that now the \texttt{8} and the \texttt{3} are in the correct positions, but this time the model added a digit at the end of the generation, instead of the new line character as before.
By inspecting the heatmap again, Fig, the user can notice that the probability of token \texttt{8} is notably increased.
Moreover, concerning the last wrong token, it is also clear that in the lower layer the model converged to the new line token (probably due to the end of the length of the sequence), but as the layer level reaches and is above the level of the injection, the output changed to a suboptimal token with respect to the final task.
This could be expected since we are injecting an embedding that was not crafted from the network at that particular layer, so there could be minor modifications that, through causal attention, are influencing the next tokens negatively.
}

\subsection{Discussion}

\todo[purple!20]{
We presented InTraVisTo, a tool to visualize the internal states and the information flow of Transformer Neural Network, the core of LLMs.
We developed InTraVisTo to provide a tool that NLP practitioners can use to understand the internal reasoning steps carried out by an LLM and, possibly, track down the causes of errors like hallucinations.
Ultimately, we hope our tool will help improve the reliability of LLMs.
Our focus at the moment is on providing better decoding and manipulation of hidden states, with focus on information injection, and improving the interface, to make InTraVisTo more user-friendly.
}

\section{Embedding Analysis}

Following the path laid out in \cref{sec:rq_embeddings,sec:method_embeddings}, the experiments performed in this section will gravitate around the possibility of identifying linear properties modeling semantic, linguistic and factual relationships within the input and output embedding spaces of LLMs.

To explore this inquiry, we have set up various experiments aimed at replicating some of the well-established embeddings properties~\cite{mikolov2013} in recent state-of-the-art architectures.
Additionally, we expand upon these ideas and provide further explanations for the behaviors emerged from the performed experiments.

\subsection{Dataset}

The dataset used for all the experiments under this research question was created by \todo{unifying} the original analogy dataset employed by word2vec~\cite{mikolov2013} and the \emph{BATS (Bigger Analogy Test Set)}~\cite{drozd2016}.
The word2vec dataset (also known as Google analogy dataset) is structured in two main files: \emph{question-words} and \emph{question-phrases}.
Question-words contains $14$ categories for a total of \todo{$x$} analogies spacing from linguistic relations to semantic relations, while question-phrases only features $5$ classes and a total of \todo{$x$} analogies regarding common-knowledge relations involving proper nouns.
BATS features a more complex hierarchy since it is meant to be an improvement over the Google dataset: it covers $4$ main relation types (inflectional morphology, derivational morphology, lexicographic semantics and encyclopedic semantics), each one being subdivided into $10$ further sub-categories containing $50$ unique word pairs each.
In addition, BATS provides multiple correct answers for word pairs when applicable. 

Due to the different format in which the two datasets have been released (BATS containing word pairs, while Google analogy dataset being comprised of already constructed analogies), \todo{some} work has been put into collecting, standardizing and unifying analogies.
To this end, BATS word pairs have been combined into a total of $(50 \times 49) \times 10 \times 4 = 98000$ unique analogies to be combined with the \todo{$x + x = x$} analogies provided by the Google dataset.
After the removal of duplicate entries, the final comprehensive total of unique analogies amounts to \todo{$x$}.

A peculiarity of the BATS dataset is the presence of multiple correct answers for a subset of word pairs.
These multiple correct answers include synonyms, alternative spellings or terms that are equivalent to the original answer.
Given the computational constraints of the experiments, only the first correct answer for any given analogy was taken into consideration.
Nonetheless, small empirical tests on a restricted set of models and a fraction of the dataset were performed using multiple answers \todo{in order to determine the impact of this choice}. \todo[green]{handling of multiple correct answers}

Additionally, a marginal set of entries belonging to the chosen datasets featured \todo{entities} composed of multiple words separated by the underscore character `\_'.
In order to improve the parsing process for the models' tokenizers, these occurrences were changed to use the whitespace character (` ') instead.

\subsection{Experiment 1}\label{ssec:exp_emb_exp1}

As a starting point, we directly compare the performance of the input embeddings belonging to various state-of-the art and older models over the defined analogy task.
The comparison between the embeddings of old and new models is crucial, since it allows us to make assessments over the effectiveness of newer architectural paradigms in creating meaningful embedding spaces.
We expect to observe similar results for architectures that are \todo{similar} to one another, while also accounting for differences in vocabulary and vector sizes of embeddings.

Interestingly, \citet{drozd2016} finds that scaling the vector size of embeddings has mixed effects in terms of performance when evaluating analogies with our chosen metrics.
On the other hand, an increase in vocabulary size should always constitute an improvement over analogy resolution.
However, as \citet{elhage2022} highlights, feature sparsity is the main cause for superposition, which can cause positive interference and negative biases, hindering the expressiveness of linear operations in the embedding space.
Therefore, we also question if models with large vocabularies that are not backed by an appropriate embedding size may be at a disadvantage for the task at hand.

\todo{In substance}, this experiment aims to verify the validity of the proposed inquiries, while taking into consideration possible limitations in embedding expressiveness for novel architectures.
\todo{Therefore, we expect to see a clear distinction in the results achieved by newer models against older architectures, not necessarily by virtue of pure embedding quality but by sheer dimensional dissimilarities.}

\subsubsection{Experimental Setup}\label{sssec:exp_emb_exp1_expset}

At its core, this class of experiments evaluates the resolution of analogies between sets of four terms utilizing the embedding layer of a model to encode words and compute distances.

Once the dataset and model are loaded, each batch of word analogies is processed, treating every single analogy as an independent computation.
All words that compose the input section of the analogy are appropriately encoded following \cref{eq:method_embeddings_multitok-in}.
Which words belong to the input for each batch of analogies \todo{are} determined by a hyperparameter containing the analogy layout (e.g. obtaining $w_1 - w_2 + w_4 = w_3$ from $w_1 : w_2 = w_3 : w_4$).
The provided layout also defines the arithmetic operations to be performed on the encoded words in order to obtain the encoded output term.

Afterwards, by following \cref{eq:method_embeddings_analogy}, we perform a search on the model's embedding space using a hyperparameter-defined distance metric as shown in \cref{eq:method_embeddings_distance}, obtaining the $k$ closest elements to the result of our embedding arithmetic.
Finally, in order to extrapolate a concrete result, we compare the set of computed tokens with the set of tokens obtained through the application of \cref{eq:method_embeddings_multitok-out} to the output word defined by the layout.
The comparison is performed through the use of the appropriate metrics formalized in \cref{eq:method_embeddings_topk-accuracy,eq:method_embeddings_rankscore}.

The complete set of hyperparameters and their possible values is as follows:
\begin{itemize}
    \item $\gbm{k}$ with \emph{positive integer values greater than $0$}: represents the maximum numbers of tokens considered when computing the closest tokens to the embedding returned by the analogy computation.
    \item \textbf{Distance metric} with values \emph{cosine}, \emph{L2}: spatial distance metric to measure element closeness inside the embedding space.
    \item \textbf{Embedding strategy} with values \emph{first\_only}: strategy used to handle input words composed of multiple tokens.
As it will be clarified later, this experiment only considers analogies where all words can be encoded using single tokens, therefore there is no need to define specific embedding strategies.
    \item \textbf{Multitoken solution strategy} with values \emph{first\_only}, \emph{subdivide}: similar concept to the embedding strategy, but handles output multi-token words by either considering the first token or every token.
Even with the assumption of single-token analogies, this hyperparameter is still relevant due to the addition of capitalized or non-capitalized alternatives to the output word, which may not result in a single-token word as their counterpart.
    \item \textbf{Pre-normalize embeddings} with \emph{boolean values}: flag that controls the use of normalized embeddings for all computations and comparisons, referencing \cref{eq:method_embeddings_normalization}.
    \item \textbf{Layout} with values $w_1 - w_2 + w_4 = w_3$, $w_2 - w_1 + w_3 = w_4$, $w_1 + \Delta(w_4 - w_3) = w_2$: analogy resolution templates considering both classic analogies, reverse analogies and delta analogies.
Delta analogies follow a slightly different resolution process, illustrated in \cref{eq:method_embeddings_delta-analogy-function}.
\end{itemize}

As mentioned before, due to the fact that models of different nature and with \todo{different} tokenization strategies are being compared, a filtering operation over the dataset entries is applied.
This operation takes place after the dataset preprocessing pipeline and removes all analogies which contain even a single word that cannot be encoded into a single token by the model's tokenizer.

Additionally, it is possible to reduce the entire dataset to a common set of single-token analogies for all models by considering the tokenizers of multiple models at the same time.
Although, by doing this, the total amount of entries is drastically reduced, hindering the reliability of experiments.
Therefore, when applying single-token filtering, we compute a reduced dataset for each model separately.
This approach comes at the cost of having to take into account the dataset support for each model when evaluating results obtained from the experiments.

\subsubsection{Models}

Experiments are performed using the input embeddings of bert-large-uncased \todo[orange]{cite}, GPT-2 \todo[orange]{cite}, Gemma 2 \todo[orange]{cite}, LLaMA-2-7B \todo[orange]{cite}, LLaMA-3-7B \todo[orange]{cite}, Mistral-7B \todo[orange]{cite} and Phi-3-mini-instruct \todo[orange]{cite}.
Additionally, the word embeddings generated by Word2Vec \todo[orange]{cite} and GloVe \todo[orange]{cite} are also included to be evaluated as a baseline.

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c c c c |}
        \rowcolorhang{bluepoli!40}
        \hline
            \textbf{Model} & \makecell{\textbf{Parameter}\\\textbf{Count}} & \makecell{\textbf{Vocabulary}\\\textbf{Size}} & \makecell{\textbf{Embedding}\\\textbf{Size}} & \makecell{\textbf{Tied}\\\textbf{Embeddings}} \\
		\hline \hline
            \textbf{Word2Vec} & - & $3$M & $300$ & Yes \\[2px]
            \textbf{GloVe} & - & $400$K & $300$ & Yes \\[2px]
            \textbf{BERT large uncased} & $336$M & $30.5$K & $1024$ & Yes \\[2px]
            \textbf{GPT 2} & $124$M & $50.3$K & $768$ & Yes \\[2px]
            \textbf{Gemma 2} & $9$B & $256$K & $3584$ & Yes \\[2px]
            \textbf{Llama 2} & $7$B & $32$K & $4096$ & No \\[2px]
            \textbf{Llama 3} & $8$B & $128.3$K & $4096$ & No \\[2px]
            \textbf{Mistral v0.3} & $7$B & $32.8$K & $4096$ & No \\[2px]
            \textbf{Phi 3.5 mini} & $3.8$B & $32.1$K & $3072$ & No \\[2px]
        \hline
    \end{tabular}
    \caption{\todo[red]{complete table}}
    \label{table:exp_emb_models}
\end{table}

\todo[green]{describe model architectures with reference to a table}

The evaluation of embeddings sourced from such a diverse set of models cannot be fair in nature.
More specifically, no effort has been made to align the datasets \todo{with which} these architectures were trained, opting for comparing their most popular iterations instead.
The chosen approach has clear limitations, but it is not within our scope to evaluate these models with the intention of finding the \todo{best performing one}.
Rather, we are interested in assessing the presence of high-level trends which justifies the employment of an approach of less granular nature.

\subsubsection{Results}

\todo[cyan]{top-k llama2/mistral/llama3, gemma2/phi3/gpt2/bert}

By observing the preliminary set of results it is apparent that Llama 2 and Mistral present the best overall performance against other models; although their \todo{performance difference} against Gemma 2, Phi 3, and surprisingly, GPT 2 and BERT is minimal.
The fact that Mistral and Llama 2 behave similarly is not \todo{surprising}, given the fact that they share most of their architecture.
However, despite the fact that Llama 2 and Llama 3 have also similar architectures, Llama 3 demonstrates an exceptionally low performance on the experiment.
Lastly, Word2Vec and GloVe, the oldest models, share the worst performance overall.

\todo[cyan]{top-k llama3/gemma2/bert}

If we observe the accuracy trends of various models as $k$ increases, we can notice that the performance of some models grows in an abnormal way.
\todo{For instance:} while Llama 3, Mistral, Phi and other newer models demonstrate an exceptional growth rate for increasing values of $k$, this behavior is not found for some older models such as GloVe and Word2Vec.
On the other hand, most models seem to present a plateau in performance around $k = 30$ except for Llama 3, which accuracy appears to be steadily increasing even at $k = 50$.
This patterns can be interpreted according to the vocabulary dimension the analyzed models, excluding older ones.
In fact, it is possible observe a direct correlation between model vocabulary size and jump in performance between $k$ values from $1$ to $50$, which can be explained by virtue of exact tokens being more difficult to find in a more ``populated'' embedding space.
As directly shown in \todo[orange]{top-k llama3/gemma2/bert}, some extreme examples are Llama 3 and Gemma with the largest vocabularies resulting in the widest accuracy jumps, and BERT with the smallest vocabulary inducing the narrowest accuracy jump.

As previously noted, Llama 3 constitutes a performance outlier for newer models, since its accuracy does not hold up to other similar models.
This surprising result cannot be completely blamed upon the \todo{vocabulary dimension nor the embedding size}, since Gemma 2 shows results comparable with the remaining models while having a vocabulary that is two times bigger and an embedding size slightly smaller than Llama 3.
We suggest that this is a direct consequence of the explicit multilingual training performed on Llama 3, which resulted in the creation of a token vocabulary that does not only reflect English language statistics, but also a \todo{meaningful portion of a variety of other languages}.
This property negatively impacts the embedding space expressiveness in two main ways.
\todo{For once}, tokens representing concepts in other languages are present in the vocabulary, resulting in a noisier embedding space and possibly fragmenting longer English words that contain them as sub-tokens.
Then, tokens that are shared between languages but have different senses or are part of words with different definitions determine the presence of embeddings that are in direct competition to give additional separate meanings to the same tokens.

\todo[cyan]{categories ...}

Another important fact that must be taken into consideration is the support of the resulting dataset for each model.
As anticipated in \todo[orange]{ref}, models have been evaluated on different subsets of the overall dataset in order to extract analogies that could be expressed solely using single-token words.
In \todo[orange]{categories ...} we can observe a breakdown by dataset categories of model performance represented by colored bars, with the additional information provided by dots constituting an indication for the percentage of dataset considered by the model for each category.
By \todo{looking} at \todo[orange]{categories ...} it is possible to discern the reason for the underperformance of Word2Vec and GloVe with respect to newer models, as we can see that these older models offer a near complete support for all the dataset categories.
This means that their vocabulary contains a great number of tokens encompassing single words, which drastically expands the number of analogies considered for the experiment, including analogies that are not easily solved by model in the process.
On a more \todo{general scale}, we can also observe the fact that some certain categories never seem to have support for any model.
These categories \todo{mostly represent} the additional portion of the original Google dataset called \emph{question-phrases} modeling analogies between entities that are described by multiple words, which intuitively do not fit the single-token constraint chosen for this experiment.
Another general pattern observable in \todo[orange]{categories ...} concerns the fact that most models show better performance on the categories belonging to the original Google dataset.
This behavior can be easily explained by both the simplistic nature of the dataset and the fact that, being a well-known dataset, most models have seen it during training.
Lastly, \todo[orange]{categories ...} shows an exceptionally low support for all categories \todo{for} Llama 3, \todo{tying back into} our previous hypothesis of a fragmented vocabulary due to explicit multilingual training.

\todo[cyan]{top-k ...}
\todo[cyan]{categories ...}

As a final note, we are going to discuss the effect of hyperparameters on this experiment.
The hyperparameter that causes the most visible effects is the analogy layout, representing \todo{the methodology} used to combine words present in analogies.
In \todo[orange]{top-k ...} we can notice that analogies of the type $w_1 - w_2 + w_4 = w_3$, seems to always return worst results than default ones: $w_2 - w_1 + w_3 = w_4$; this is likely the result of most analogies being created with a specific directionality in mind.
Nonetheless, by observing \todo[orange]{categories ...} we can see that this effect is mostly limited to categories that belong to the BATS dataset, as the performance for the Google dataset appears to be mostly unaffected by hyperparameter combinations.
On the other hand, the pre-normalization of embeddings does not seem to have any major effect on any of the models, besides a slight generalized increase in performance.
\todo[green]{delta analogies}

\subsection{Experiment 2}\label{ssec:exp_emb_exp2}

For this second experiment we wish to delve deeper into the actual capabilities of the embeddings belonging to recent decoder-only LLMs.
To this end, we will also take into consideration the unembedding layer of said models.
As mentioned in \cref{ssec:background_transf_structure}, decoder-only transformers feature a reversed embedding matrix inside their language modeling head.
This matrix is used to translate the last hidden state produced by the transformer stack into the index for the next predicted token.
Since it shares a common structure with the actual embedding matrix, it should be possible to run the same analogy experiments and obtain meaningful, albeit different, results.

However, not all model architectures provide different embedding and unembedding matrices as already hinted in \cref{ssec:background_transf_structure}.
Consequently, in this experiment we will also observe the differences between these architectural choices through the lens of analogies.

Additionally, inspired by the decoding approach introduced for the first research question (\cref{sec:rq_intravisto}), we are also going to take into consideration interpolated embeddings.
Embedding interpolation, defined in \cref{ssec:method_intravisto_decoding} and formalized through \cref{eq:method_intravisto_linear-interp,eq:method_intravisto_quadratic-interp}, is a novel technique that exploits linear properties of embedding spaces and hidden representations~\cite{park2023, mikolov2013, drozd2016} to generate intermediate unembedding matrices with the purpose of decoding said intermediate states.
Logically, since we are performing a linear interpolation operation, we expect to observe a trend in the analogy resolution accuracy of the interpolated embeddings that goes from the input embedding to the output one according to the interpolation percentage.
However, it could be possible that the actual rate of change for the interpolated performance might not be constant between the two end points. \todo[green]{what does this imply?}

\subsubsection{Experimental Setup}\label{sssec:exp_emb_exp2_expset}

The experimental setup for this second experiment presents only few differences with respect to the previous one (\cref{ssec:exp_emb_exp1}).
As a starting point, the core algorithm used to perform analogy resolution defined in \cref{sssec:exp_emb_exp1_expset} is left \todo{invariant}, as the fundamental task is the same between the two experiments.
On the other hand, one of the main differences is the absence of any kind of filtering for the dataset, implying that all models share the same amount of test cases.
This change \todo{also} sets some restrictions on the choice of models, which will be tackled in a dedicated section (\cref{sssec:exp_emb_exp2_models}).

Due to the fact that models may encounter multi-token words, the \emph{Embedding strategy} hyperparameter identified within the experimental setup of the previous experiment gains additional values to handle the encoding of multiple tokens converging inside a single word.
These new values are \emph{average} and \emph{sum}, which correspond to taking respectively the mean and the sum of the embeddings belonging to the tokens that compose the multi-token word in question.
As previously mentioned, these techniques are formalized in \cref{eq:method_embeddings_multitok-in}.

\subsubsection{Models}\label{sssec:exp_emb_exp2_models}

For the experiment at hand we select a smaller set of models to compare due to the fact that we wish to observe the differences between input and output embeddings.
All changes to the models will be made referencing \cref{table:exp_emb_models}, defined for \cref{ssec:exp_emb_exp1}.

As a first step, we are going to discard models without proper sub-word tokenization such as Word2Vec and GloVe. \todo[green]{BERT?}
This allows us to also perform tests on the whole dataset, through the implementation of proper encoding strategies as defined in \cref{eq:method_embeddings_multitok-in,eq:method_embeddings_multitok-out}.
In addition, we are not going to focus on models without distinct input and output embeddings such as GPT-2 and Gemma-2, therefore only GPT-2 will be included in order to provide a baseline evaluation.

\subsubsection{Results}

\todo[cyan]{top-k gpt/mistral.in/mistral.out/phi.in/phi.out, top-k gpt/llama2.in/llama2.out/llama3.in/llama3.out}

By a preliminary observation of the results we can grasp that there are some slight difference in performance between input and output embeddings across all models.
More specifically, our analysis reveals that for increasing values of $k$, output embeddings seem to consistently outperform input embeddings (contextually to overall model performance), while for smaller $k$ values input embeddings demonstrate greater accuracy.
This phenomenon suggests that input embeddings provide more reliable estimates for analogies results, but their vectors are laying in a sparser latent spaces, thus making it less likely to accidentally include the correct vector out of a wrong prediction by extending the range given by $k$.
Whereas output embeddings latent spaces are more compressed and benefit more from greater $k$ values.
Finally, we show that input and output embedding pairs belonging to the same model tend to have the same trend, and are generally more prone to end up having similar accuracies.
This is not a completely trivial result since, despite having the same dimensionality and possibly sharing some of the datasets used to train them (depending on the training setup and initialization values), they still operate with completely different weight matrices and carry out different tasks inside models.

By comparing the performance of Llama 3 against the previous experiment it is possible to notice an outstanding increase in accuracy, which is now comparable to \todo{that} of the other analyzed models.
This result is in clear contrast with the trends displayed by \todo{other models} between the two experiments, as they tend to have a slightly worse performance when considering the whole dataset.
We can still speculate that the reason for this surprising behavior resides in the multilingual nature of Llama 3, and how it affected its tokenization process.
In fact, it is very possible that a part of the analogies easily solvable by the model were not included in the dataset of the first experiment due to uncommon tokenization patterns.
Another perspective which corroborates this theory is the fact that the Llama 3's baseline also grows in accordance with its recorded performance, implying that the positive impact of the new dataset is not necessarily tied with the analogy resolution \todo{in} itself.
Nonetheless, Llama 3's accuracy still \todo{scores as the worst} between all considered models. 

\todo[cyan]{top-k ...}

Another interesting result that can be garnered from the performed experiments relates to the effects of interpolation on the semantic expressiveness of embeddings.
As it is possible to observe in \todo[orange]{top-k ...}, we can say that our expectations on the general layout of the results for interpolated test are indeed met, since they always score between the two extremes that produced the interpolated variations.
In addition, it is also possible to spot an interesting diverging pattern in
\todo[green]{interpolation}

\todo[cyan]{top-k ...}
\todo[cyan]{categories ...}

\todo{Conversely}, we can appreciate some particular model behaviors regarding the choice of hyperparameters, especially those defining the multi-token embedding strategy.
By observing \todo[orange]{top-k ...} we can see that for the classic analogy resolution pattern ($w_2 - w_1 + w_3 = w_4$) taking the average of embeddings seems the overall best way to encode multi-token elements, followed by considering only the embedding of the first token and directly summing the embeddings.
This result \todo{reconfirms} the findings \todo{obtained} by \todo[orange]{ref} for newer model paradigms and is in accordance with the hypothesis of a linear embedding space.
\todo[green]{hyperparameters sum/average/first}

On the other hand, we can see that the expected drop in performance of reverse analogies ($w_1 - w_2 + w_4 = w_3$) is much more accentuated when analyzing the complete dataset rather than when filtering for single-token analogies.
This is noticeable to the point that the performance of most models seems to be \todo{equal or worse} than the baseline for some specific sets of hyperparameters that include reverse analogies in the specified format.
In addition, by observing \todo[orange]{categories ...} we can notice that the drop in performance appears to be more generalized \todo{throughout} all dataset categories, rather than targeting the BATS analogies as found in the previous experiment.
Newer models seem to be more susceptible to these \todo{types} of hyperparameter variations, whereas GPT-2 gives the impression of being only marginally affected by them.
On the topic of pre-normalization, the effects seem to be negligible and similar to what was already stated in \todo[orange]{ref}.
\todo{Surprisingly}, pre-normalization appears to have slightly better results when considering the summation of embeddings as a strategy to resolve multi-token analogies.
This behavior can be intuitively explained by considering the fact that directly summing dimensions without dividing by the variable number of elements (\todo{as for} the averaging case) returns inflated values, which get slightly mitigated by the pre-normalization operation.

\todo[green]{delta analogies}

\subsection{Discussion}

In the end, we can affirm that LLMs are able to retain a surprising amount of semantic relationships inside their embeddings, despite the fact that sub-word tokenization actively works against the accumulation of meaning for token representations.

Interestingly, we observed that the only LLM that clearly underperformed on the proposed task was the only one that had an explicit multilingual training.
As previously stated, this finding suggests that multiple languages compete in giving different meaning to the same tokens, resulting in worse overall performance for the given tasks.
We must acknowledge the fact that these results do not constitute the focal point of our analysis, and most definitely require further experimentation to provide meaningful answers by expanding upon the provided conjectures.
Nonetheless, our observations still \todo{hold importance} for the defined research question as we observed that an increase in scale of the model does not directly influence the performance on the proposed tasks and, by extension, the expressiveness of the embeddings.

On the topic of input and outupt embeddings, we can conclude that \todo[green]{conclude}.

\todo[green]{insert}
In fact, these outcomes were fundamental in the development of the embedding interpretation portions of InTraVisTo, presented in \todo[orange]{ref} for the first research question, as they served as an intuitive way to group multiple...

\todo[purple!20]{
It becomes apparent that the majority of LLMs seem to underperform with respect to GPT-2, this notable deviation in performance could stem from various factors.
However, by inspecting Fig., which displays the same results, but for the single-token version of dataset, we can observe that the performance of GPT-2 aligns more closely with that of the other models.
This mismatch in performance may be attributed on the tokenization strategy and vocabulary size of the chosen models, in fact all three models employ a Byte-Pair Encoding (BPE) tokenizer 
%\cite{DBLP:conf/aaai/WangCG20}
, however the tokenizers utilized by both Mistral and LLaMA are based on SentencePiece 
%\cite{DBLP:conf/emnlp/KudoR18}
, presenting a smaller overall vocabulary (32,000 tokens against GPT-2's 50,257 tokens), thus possibly implying a reduced selection of full-word tokens in favor of an increased number of sub-word tokens.
By analyzing results on the single-only dataset in Fig., we are able to infer that the exclusion of multi-token words from the dataset also mitigates differences arising from how tokenizers segment words.
This effectively normalizes away instances of analogies where GPT-2 may have held an advantage over other LLMs by being able to fully encode certain words.
For the same reason, GPT-2 would have had both better representations encompassing semantic meanings of words without additional noise caused by multiple words sharing the same sub-word token, but also not having to be subject to the token-reconstruction previously mentioned strategies as ways to deal with multi-token words.
}

\section{First Order Prediction}

Building on the foundation established in \cref{sec:rq_fom}, the experiments in this section will focus on exploring the implications of creating a First Order Model (FOM).
A FOM is obtained by removing all the intermediate architectural components from an LLM, leaving behind the input and output embedding layers, and the residual connections joining them.
As explained in \cref{sec:method_fom}, this operation can be seen as the creation of a Markov model possessing a transition matrix formed by the product between the input and output embedding matrices.
\todo[green]{mention logit matrix}

Most of the experiments in this section are geared towards understanding if the FOM actually represents a faithful bigram Markov model over the original model's vocabulary.
Although past work has theorized and partially demonstrated on a restricted set of models the possibility of this hypothesis being \todo{correct}~\cite{elhage2021}, we suspect that FOMs extracted from different LLMs may have different degrees of \todo{`Markovness'} and we strive to explore \todo{this} intuition in this section.
\todo[green]{Add part on weight tying}

\todo[green]{RMS version}

\subsection{Dataset}\label{ssec:exp_fom_dataset}

The dataset employed for this set of experiments is \emph{WikiText 103} \todo[orange]{cite} (hereby referred simply as \emph{WikiText}), containing a total of $1.81M$ rows of full articles sourced from Wikipedia \todo[orange]{cite}.
This dataset is used to train the unigram Markov model and to perform additional tests on various models by using a separate validation+test split.

In addition to \emph{WikiText}, we also use a set of $10000$ sentences randomly extracted from the training split of \emph{OpenWebText} \todo[orange]{cite}.
This was \todo{done} in order to provide a less biased evaluation of models that were directly trained on \emph{WikiText}.

\subsubsection{Models}

Similarly to the models taken in consideration for \cref{ssec:exp_emb_exp2}, the main \todo{chosen} transformer architectures have different input and output embeddings.
From these models we choose to analyze LLaMA-2-7B \todo[orange]{cite}, LLaMA-3-7B \todo[orange]{cite}, Mistral-7B \todo[orange]{cite} and Phi-3-small-8k-instruct \todo[orange]{cite}.

Whereas, the unigram Markov model is trained utilizing the training split of \emph{WikiText} for a total of $1.81M$ rows.
The vocabulary of the unigram Markov model is constructed using the vocabulary of the transformer model that is currently being compared to, in order to enable 1-to-1 token comparisons for the transition matrices.
This is achieved by parsing the training text for the Markov model and manually processing the frequencies using the tokenizer of the transformer model at hand.

\subsection{Experiment 1}

The first experiment explores the main inquiry by means of direct matrix comparison, implying the computation of immediate metrics starting from the FOM and Markov model transition matrices.
This rather simplistic approach still provides surprising experimental results despite not being conventionally used for the comparison of probability distributions.

In order to provide a more reliable estimate we also choose to use set similarity metrics as an alternate way to evaluate similarity between approaches.
As it will be discussed \todo{further} in later sections, these metrics are based on the comparison between sets of top-k model predictions over all vocabulary terms.
Despite \todo{this approach} not taking into consideration the nuances of probability distributions, it still provides insight on the comparison of greedy estimates between \todo{model predictions}.

\subsubsection{Experimental Setup}

As introduced before, the FOM transition matrix is computed by merging input and output embeddings of the inspected models following \cref{eq:method_fom_fom-matrix}.
On the other hand, the Markov model equivalent is obtained by computing bigram statistics of tokens over the training dataset referenced in \cref{ssec:exp_fom_dataset}.
In order to make the models comparable, we directly use the LLM's tokenizer to encode the corpus of text used to train the Markov model, in this way both models share the same vocabulary.

Once the two main models are loaded as transition matrices, we proceed to compare them \todo{between each other} and against the identity matrix.
As mentioned before, the identity matrix represents the transition matrix of a FOM extracted from a transformer-based language model with different input and output embeddings.
The comparison is performed by quantifying the norm of the difference between two matrices as indicated in \cref{eq:method_fom_fom-i-comp,eq:method_fom_fom-markov-comp}.

On the other hand, \todo{set metrics} are computed starting from a probabilistic token-by-token view of transition matrices, meaning that each row is represented as a set containing the column indexes corresponding to the top-$k$ values.
Starting from this representation we adopt set metrics to compute similarities between sets of distributions (transition matrices) as defined in \todo[orange]{}{cref}. \todo[green]{list of set metrics}
Technically speaking, it is important to point out that for the experiments involving set metrics, comparing actual transition matrices modeling probabilities to logit matrices does not make any difference as both the \emph{softmax} and \emph{logarithm} operators are monotonic, thus they preserve the ordering of elements.

\todo[green]{training text token counts for each tokenizer}

\subsubsection{Results}

We can begin our analysis by observing \todo[orange]{fig}, which depicts Llama 2's performance on the self-regression and unigram-regression tasks using the top-$k$ accuracy metric.
It is possible to notice how, for any given value of $k$, the accuracy of the model in predicting the identity matrix appears to be lower than any of the values reported for the unigram Markov model accuracy comparison.
This trend is followed by most other analyzed models, except for Mistral.
In Mistral's case, depicted in \todo[orange]{fig}, the predictions generated by the FOM appear to align more closely with the inputs of the model rather than the most probable outputs generated by the unigram Markov model ($k_2 = 1$).
This series of observations for Mistral seems to hold even when considering all traces corresponding to more lenient variants that compute the accuracy by comparing the top-$k_1$ predictions of the FOM against the top-$k_2$ predictions of the Markov model.

The previous observations seem to suggest that for some specific models the embeddings may exhibit a closer relationship to their inverse, rather than serving as representations of the subsequent token prediction based on the input. 
Unfortunately this observation appears to contradict the recorded matrix distances, observable in \todo[orange]{table}.
The matrix distance metric seems to indicate that the FOM based on Mistral is actually better at representing its Markov model than any of the other models since it has the lowest FOM/Markov distance and one of the highest FOM/identity distances out of all the analyzed models.
However, direct comparative statements over the matrix distance should be taken with a grain of salt as it is not a reliable or commonly used metric in itself.
In fact, we can notice that all LLMs present the same trend regarding their matrix distance experiments, as 

\todo[purple!20]{
This seems to suggest that the embeddings 


However, these results are relative to LLaMA's performance on the same exact tasks.
By looking at Fig., which illustrates its perfomance on the self-regression task and Fig. representing its performance on the unigram-regression, we notice opposite results, the LLaMA FOM exhibits greater top-$k$ accuracy when compared to the unigram Markov model rather than its own inputs.
This peculiarity is noticeable by empirically examining the LLaMA FOM predictions for some common tokens, which appear to be plausible next tokens:
}

\begin{table}
\centering
\begin{tabular}{||c || c | >{\columncolor[gray]{0.9}}c | c||} 
\hline
Input token/Model   & Mistral FOM   & LLaMA FOM & Markov models \\ 
\hline\hline
mount               & gorith        & clock     &  s            \\ 
easy                & /******/      & going     & to            \\
hair                & teenth        & loss      & ,             \\
\hline
\end{tabular}
\end{table}

\todo[purple!20]{
Furthermore, the recorded matrix distances reveal a notable disparity in scale compared to those obtained using Mistral's FOM.
Specifically, a distance of 522.3115 observed between the FOM transition matrix and the identity matrix and a distance of 509.2659 recorded between the FOM and Markov model transition matrices.
Even for a model so clearly predisposed towards predicting the next word rather than returning the input word itself, the difference between the distances is minimal.
This suggests a potential bias towards lower distance estimates between the FOM transition matrix and the Markov transition matrix, possibly due smaller quantities of null values present in the latter matrix with regard to the identity matrix.
}

\subsection{Experiment 2}

The second experiment still focuses on the evaluation of transition matrices generated by concatenating input and output embeddings from transformer models with respect to actual unigram Markov models, but via deeper means of analysis.
For this experiment we decide to shift our view of the transition matrices from sets of elements corresponding to binary predictions to actual probability distributions.
This approach enables a more accurate analysis, and considers the models for how they would concretely be used outside from these experimental scenarios.

In \todo[orange]{ref} we defined the two main mathematical tools that we intend to use for this experiment, which are the KL divergence \todo[orange]{ref} and the perplexity \todo[orange]{ref}.
These evaluations allow us to understand how similar or dissimilar \todo{are} two distributions over a vocabulary by either directly comparing them using the KL divergence or by measuring their \todo{surprise} over a corpus of text using the perplexity metric. 

\subsubsection{Experimental Setup}

\todo[green]{kl div experimental setup}

On the other hand, overall perplexity for a model is obtained by averaging the perplexity computed following \todo[orange]{ref} over all sentences \todo{present} in the test dataset.
As already \todo{indicated} in the related equation, the perplexity of each sentence is calculated by cumulating \todo{for all tokens} the perplexity obtained by comparing the model's \todo{prediction} logits to the one-hot encoded identifier of the following token.

For this experiment we take into consideration four perplexity scores for each model choice.
The main two results which are used for the central comparisons are the FOM and Markov model perplexity scores, while results obtained using a uniform distribution over the model's vocabulary and the identity matrix model are also taken into account as baselines.

\subsubsection{Results}

By a preliminary analysis of the average perplexities we can immediately notice that all models present similar trends in how their FOM performs with respect to the uniform baseline and the computed Markov model counterpart.
In fact, we observe that the average perplexity from the FOM is consistently \todo{slightly lower} than the uniform baseline while being greatly higher than the Markov model's performance.
We hypothesize that this tendency is caused by a mixture between the improper nature of the FOM and the possible biases in the training set of the Markov model.
This last point is only marginally circumvented by the use of a different test set as we can detect only a modest raise in perplexity between the tests using the two datasets.

Unsurprisingly we observe the perplexity results of the identity matrix predictions being disproportionately higher than all other models, as it represents the certainty of all tokens repeating and is not fit to actually represent a meaningful Markov transition matrix.
However, we still value this outcome since it provides a \todo{ground of comparison} for the FOM extracted from each LLM, indeed confirming the results obtained in the previous experiment (\todo[orange]{ref}) implying the FOM being closer to the actual Markov model rather than the identity matrix.

One peculiar trend observable in \todo[orange]{ref} is the fact that, when plotting the perplexity over multiple sentences in a test dataset, a clear pattern with the shape of multiple `horizontal lines' emerges for the FOM version of most LLMs.
The lines vary in density and are placed in the loose proximity of the perplexity of for the uniform model.
The number and position of lines \todo{in which} the FOM perplexity splits seems dependent on both the underlying LLM and the test dataset, while in general it appears to be in line with the uniform prediction.
However, the \todo{found} consistencies imply the existence of macro-groups of sentences to which a model \todo{punctually} scores a perplexity that is either marginally \todo{worse or better} than random guessing.

\todo[cyan]{explore some sentences}
\todo{Summarily} we can affirm that the difference between FOM and uniform performance is much more \todo{definite} when looking at the plotted results rather than numerical averages, as it is possible to better identify outliers present in the datasets from the plots.

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c c c c c |}
        \rowcolorhang{bluepoli!40}
        \hline
             & \multicolumn{2}{c}{\textbf{FOM}} & & & \\[-0.1pt]
        \rowcolorhang{bluepoli!40}
            \multirow{-2}{*}{\textbf{Model}} & \textbf{no RMS} & \textbf{RMS} & \multirow{-2}{*}{\makecell{\textbf{Markov}\\\textbf{model}}} & \multirow{-2}{*}{\makecell{\textbf{Uniform}\\\textbf{probability}}} & \multirow{-2}{*}{\makecell{\textbf{Identity}\\\textbf{matrix}}} \\
		\hline \hline
            \textbf{Llama 2} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
            \textbf{Llama 3} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
            \textbf{Mistral v0.3} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
            \textbf{Phi 3.5 mini} & $30.83 \times 10^3$ & $6.03 \times 10^9$ & $247.13$ & $32.06 \times 10^3$ & $676.79 \times 10^9$ \\[2px]
        \hline
    \end{tabular}
    \caption{\todo[red]{complete table}}
    \label{table:exp_fom_wikitext}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c c c c c |}
        \hline
        \rowcolorhang{bluepoli!40}
             & \multicolumn{2}{c}{\textbf{FOM}} & & & \\[-0.1pt]
        \rowcolorhang{bluepoli!40}
            \multirow{-2}{*}{\textbf{Model}} & \textbf{no RMS} & \textbf{RMS} & \multirow{-2}{*}{\makecell{\textbf{Markov}\\\textbf{model}}} & \multirow{-2}{*}{\makecell{\textbf{Uniform}\\\textbf{probability}}} & \multirow{-2}{*}{\makecell{\textbf{Identity}\\\textbf{matrix}}} \\
		\hline \hline
            \textbf{Llama 2} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
            \textbf{Llama 3} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
            \textbf{Mistral v0.3} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
            \textbf{Phi 3.5 mini} & $29.74 \times 10^3$ & $152.72 \times 10^6$ & $2.22 \times 10^3$ & $32.06 \times 10^3$ & $591.73 \times 10^9$ \\[2px]
        \hline
    \end{tabular}
    \caption{\todo[red]{complete table}}
    \label{table:exp_fom_openwebtext}
\end{table}

On the other hand, results obtained by computing the KL divergence metric over the vocabulary can be considered more straightforward to interpret as we can observe that there is a clearer difference between the performance of models.
The distribution dissimilarity between FOM and Markov model is clearly substantially lower than \todo{that of} any other pair of model distributions for all analyzed LLMs.

We can also notice that $KL(Markov||FOM)$ \todo{formula} is consistently smaller than the reverse ($KL(FOM||Markov)$ \todo{formula}) for all LLMs, implying that the FOM is a better approximation of the Markov model than \todo{the opposite}.
This result is likely tied to the difference in probability magnitude over the majority of the vocabulary terms.
Since the Markov model was trained by analyzing a restricted dataset with a set of terms smaller than the vocabulary, we believe that its distribution neglects tokens that were never seen during training, even despite the smoothing coefficient applied to token counts.
On the other hand, the FOM possesses a much more uniform-like baseline as already shown in the previous experiment (\todo[orange]{ref}), thus it \todo{acts better at being considered} the reference distribution in the KL divergence computation.

Although the results are much more aligned between models with regard to previous experiments, it is still possible to partially observe the same properties tied to the FOM approximations of specific LLMs.
For instance, we can notice a slightly higher $KL(Markov||FOM)$ \todo{formula} value and a slightly lower $KL(FOM||Markov)$ \todo{formula} value for the Mistral model compared against the others.
Additionally, the KL divergence values related to the comparisons between identity matrix and FOM for the Mistral model seem to present the same trends.
This series of outcomes is not necessarily in line with previous observations, however, it is of marginal nature and can be ascribed to a direct different in the magnitude of model weights that is .

\begin{table}[H]
    \centering
    \begin{tabular}{| >{\columncolor{bluepoli!40}}c || c c c c c |}
        \hhline{-||-----}
        \rowcolorhang{bluepoli!40}
            \todo{\textbf{Mean DKL Llama 2}} & \textbf{FOM} & \makecell{\textbf{FOM}\\\textbf{with RMS}} & \makecell{\textbf{Markov}\\\textbf{model}} & \makecell{\textbf{Identity}\\\textbf{matrix}} & \makecell{\textbf{Uniform}\\\textbf{probability}} \\
		\hhline{=::=====}
        \textbf{FOM} & $-$ & $-$ & $0.202626$ & $10.373777$ & $-$ \\[2px]
        \textbf{FOM with RMS} & $-$ & $-$ & $2.099694$ & $12.416668$ & $-$ \\[2px]
        \textbf{Markov model} & $0.054688$ & $2.262181$ & $-$ & $10.388289$ & $-$ \\[2px]
        \textbf{Identity matrix} & $17.260256$ & $19.609880$ & $17.458118$ & $-$ & $-$ \\[2px]
        \textbf{Uniform probability} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
        \hhline{-||-----}
    \end{tabular}
    \caption{\todo[red]{complete table}}
    \label{table:kld}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{| >{\columncolor{bluepoli!40}}c || c c c c c |}
        \hhline{-||-----}
        \rowcolorhang{bluepoli!40}
            \todo{\textbf{Mean DKL Mistral v0.3}} & \textbf{FOM} & \makecell{\textbf{FOM}\\\textbf{with RMS}} & \makecell{\textbf{Markov}\\\textbf{model}} & \makecell{\textbf{Identity}\\\textbf{matrix}} & \makecell{\textbf{Uniform}\\\textbf{probability}} \\
		\hhline{=::=====}
        \textbf{FOM} & $-$ & $-$ & $0.163428$ & $10.375000$ & $-$ \\[2px]
        \textbf{FOM with RMS} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
        \textbf{Markov model} & $0.075116$ & $-$ & $-$ & $10.409687$ & $-$ \\[2px]
        \textbf{Identity matrix} & $17.659594$ & $-$ & $17.417395$ & $-$ & $-$ \\[2px]
        \textbf{Uniform probability} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
        \hhline{-||-----}
    \end{tabular}
    \caption{\todo[red]{complete table}}
    \label{table:kld}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{| >{\columncolor{bluepoli!40}}c || c c c c c |}
        \hhline{-||-----}
        \rowcolorhang{bluepoli!40}
            \todo{\textbf{Mean DKL Phi 3.5 mini}} & \textbf{FOM} & \makecell{\textbf{FOM}\\\textbf{with RMS}} & \makecell{\textbf{Markov}\\\textbf{model}} & \makecell{\textbf{Identity}\\\textbf{matrix}} & \makecell{\textbf{Uniform}\\\textbf{probability}} \\
		\hhline{=::=====}
        \textbf{FOM} & $-$ & $-$ & $0.201826$ & $10.410433$ & $-$ \\[2px]
        \textbf{FOM with RMS} & $-$ & $-$ & $9.795715$ & $21.472530$ & $-$ \\[2px]
        \textbf{Markov model} & $0.059503$ & $7.225024$ & $-$ & $10.390044$ & $-$ \\[2px]
        \textbf{Identity matrix} & $17.257671$ & $24.772488$ & $17.455151$ & $-$ & $-$ \\[2px]
        \textbf{Uniform probability} & $-$ & $-$ & $-$ & $-$ & $-$ \\[2px]
        \hhline{-||-----}
    \end{tabular}
    \caption{\todo[red]{complete table}}
    \label{table:kld}
\end{table}

\todo[green]{RMS FOM versions}

\subsection{Discussion}

Overall, the feasibility of using a FOM as a Markov model approximation seems to be promising, although it might vary according to the specific choice of LLM, as some appear to be \todo{more predisposed} than others.
Nevertheless, every analyzed FOM seems to exhibit a slight bias towards trying to approximate an actual unigram Markov model rather than the identity matrix.

This collection of results provides partial empirical confirmation of what was initially theorized by \todo[orange]{cite} and extends \todo[green]{?}.
Additionally, it provides solid evidence that the output embeddings do not naturally tend to drift towards modeling input embeddings.
This implies that current transformer-based LLMs might not benefit as much from weight tying as some older language models, reducing the quantifiable improvements to a marginal reduction in the number of parameters.

It is possible to \todo{further contextualize} the results obtained in the current section into the InTraVisTo framework since it is based around the concept of residual flow as previously mentioned in \todo[orange]{ref}.
The tendency of the FOM to act as a Markov model fits perfectly into the perspective of a residual flow that, starting from the current token, reaches a probability distribution over the vocabulary for the next token through a process of continuous refinement enacted by various modules \todo{that comprise} transformer architectures.
If we strip the model of its modules, we are left with a direct communication channel that provides the simplest possible prediction given the available transformations.
By extending this interpretation and capitalizing on the fact that operations on the residual stream are performed additively, we can rationalize the purpose of InTraVisTo as a tool that can be utilized to extract partial interpretations of a state that is progressively drifting towards a final prediction.

\todo{In addition}, we theorize the possibility of extracting unembedding weights from the transition matrix of a unigram Markov model.
These weights could see use as initialization values for the decoder weights of an LLM at training time.
This is one of the possibly interesting potential developments stemming from the findings of the current section, which may be explored in future work.
This as an interesting potential development of this idea in future work.