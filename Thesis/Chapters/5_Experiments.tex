This chapter presents the experiments conducted to evaluate the methodology proposed in \Cref{ch:methodology} for the reference scenarios described in \Cref{ch:research_questions}.
The experiments were initially designed to shed light onto some specific aspects of LLM internal state interpretability via vocabulary decoding, although after some interesting findings, more attention was given to the actual initial and final embedding representations in LLMs.

This chapter is organized similarly to \Cref{ch:research_questions,ch:methodology}, following the three main research questions as a baseline.
Each research question contemplates multiple experiments aimed at providing empirical evidence to elicit a deeper understanding of large transformer architectures through the perspective of the question at hand.
Additionally, each experiment includes a fixed set of key points to formalize the analysis in a structured way.
These points include:
\begin{itemize}
    \item \textbf{Experimental Setup}: complete description of the experiment performed in reference of the theoretical background provided in \Cref{ch:methodology}.
Parameter combinations, hardware specifications and resources employed may also be mentioned in the setup description.
    \item \textbf{Dataset}: summary of data and information utilized for the experiment, \todo{either} for training models or as direct testing material.
    \item \textbf{Models}: list of models, along with brief \todo{specifics}, that have been utilized to perform the experiment or have been employed for auxiliary tasks. 
    \item \textbf{Results}: set of results emerged from the experiment paired with comments, explanations and a breakdown of the findings' possible implications.
\end{itemize}
If the previously identified points overlap for all the identified experiments of any given research question then, those are directly incorporated inside the main section belonging to the question in order to avoid meaningless repetition.
Furthermore, each research question has a final discussion section, which is aimed at comparing the results of all the performed experiments and providing final remarks considering the overall outcomes for the specific inquiry.

\section{Transformer Visualization}\label{sec:exp_intravisto}

As anticipated in \Cref{sec:rq_intravisto} and explored in \Cref{sec:method_intrvisto}, the experiments tied to this research question are centered around the InTraVisTo tool.
In each experiment we are going to provide various prompts to different models, and explore specific aspects of the extracted internal representations and information flows utilizing InTraVisTo.

By directly experimenting with the proposed tool, we established a basic workflow that enables researchers to ultimately collect new insights about how LLMs generate tokens in a layer-by-layer fashion.  
The first step consists in exploring the secondary representations of internal states in order to find additional information relevant for the task at hand, as it is possible that the model is encoding it in the latent dimensions of hidden states alongside the main token, utilizing the residual stream as a communication channel between modules.
\todo[green]{describe secondary representation}

Based on the information gathered previously, observing the \todo{overall influence} on hidden states of interest generated by the model is the next step of the inspection process.
This can be achieved by utilizing the Sankey diagram visualization of InTraVisTo, piecing together the contribution of tokens and components that had a major role in the creation of certain intermediate states, enabling the formation of conjectures about the model's inner workings.

Finally, by acting upon these conjectures using the state injection and component ablation tools provided by InTraVisTo, it is possible to generate new knowledge by identifying the root causes that determine certain internal behaviors manifested in the preliminary inspection.
This knowledge can be further generalized by replicating the experiments on multiple models and observing possible similar mechanics at play, even between different architectures.

\subsection{Experiment 1}

\todo[green]{describe interface for decoding hidden states}

\subsection{Experiment 2}

\todo[green]{describe interface for decoding hidden states}

\subsection{Experiment 3}

In this first experiment we are going to use InTraVisTo to analyze how models represent and perform computations of numerical nature.
It is a well known fact that current general-purpose language models have \todo{demonstrated} poor overall performance on tasks that included the use of numbers and mathematical operators \todo[orange]{cite}.
Thus, we decided to initially focus on this set of tasks to showcase the \todo{inspection} capabilities of our \todo{developed} tool.

\subsubsection{Dataset}

The dataset employed for this experiment consists of a small set of curated examples used to elicit model predictions in order to observe meaningful internal states.
As previously mentioned, all prompts revolve around the \todo{execution of} tasks of mathematical and numerical nature.

The following constitutes a comprehensive list containing all main prompts used in the experiment.
Numbers are 

\todo[green]{list of prompts}
%\begin{multicols}{3}
%    \begin{itemize}
%        \item Item 1
%        \item Item 2
%        \item Item 3
%        \item Item 4
%        \item Item 5
%        \item Item 6
%        \item Item 7
%        \item Item 8
%        \item Item 9
%        \item Item 10
%    \end{itemize}
%\end{multicols}


\subsubsection{Models}
\todo[green]{models}

\subsubsection{Results}

\todo[purple!20]{
We are going to do a walkthrough of the main features of the tool by analyzing the visualizations from a given input prompt and probing the model with embedding injection.

%\citet{DBLP:conf/nips/LiuAGKZ23}
claimed that some model reasoning errors could be tied back to \emph{attention glitches}, minor errors propagated throughout the attention pattern in the model, leading to imperfect state information transferred through the layers.
They tested the model on the Flip-Flop language, but a simpler yet effective use case turned out to be \emph{reversing sequences}.
Thus, the input prompt for our example is: \textit{Write numbers in reverse order. Number: 13843234 Reverse:}.

As depicted in
%\Cref{fig:example_settings}
, the output of the model is not correct as it is \emph{43234\textbf{38}1} instead of \emph{43234\textbf{83}1}.
Let's now start inspecting the models' internals with InTraVisTo.

Observing %
%\Cref{fig:example_heatmap_wrong}
, it is immediately clear that these models (in this case \texttt{mistralai/Mistral-7B-Instruct-v0.2}) present a clear step in the magnitude of probabilities when passing from the intermediate layers to the last layers.
Through manual trials, we saw that this step tends to happen earlier in the layers as the next token becomes more obvious (e.g. see
%\Cref{fig:heatmap}
in the paper the column of token \texttt{\_capital} or \texttt{\_Italy}).
In this case, there exists a difference between the layers where the step for ``correct output tokens'' and the step for the ``wrong output tokens'' (highlighted in red) occur: the latter seems to be one layer late which, assuming our behavioral assumption is correct, implies that this result is not so obvious according to the model.
Then, by inspecting the magnitude of the probabilities for the wrong token, this conclusion seems legitimate due to a lower probability (lighter background color) compared with the correct token probabilities, even at the last layer of the network.

Knowing that in
%\Cref{fig:example_heatmap_wrong} 
the correct final token in the red highlighted box should be \texttt{8} instead of \texttt{3}, let's now check if the model has ever introduced an \texttt{8} during its reasoning.
%\Cref{fig:example_heatmap_multiple}
represents the same slice of the heatmap inside the red box in 
%\Cref{fig:example_heatmap_wrong}
(just shifted below the purple horizontal line and showing nine layers instead of eight), but with different embeddings shown: the embedding coming from the attention, the one after the summation with the residual and the embedding coming from the Feed Forward component (the final embedding, the FF plus the residual is already shown in 
%\Cref{fig:example_heatmap_wrong}
).
Despite the tokens decoded from the attention embeddings not being very informative and the ones coming from the embedding of the attention plus the residuals being similar to the final one, we can notice that in the embeddings coming from the FF, there is an 8 in the fourth-last layer.
Now we can conjecture that the model result was probably wrong due to an incorrect or imprecise internal representation around the $29^{th}$ layer.

To better profile the problem, we can inspect the information flow representation.
%\Cref{fig:example_flow_perc}
shows the flow percentages of the same FF block at the $29^{th}$ layer highlighted before: it is 1.7\% where the other is at least 1.8\%.
This indicates that that particular block is slightly below the average in terms of contributions to the overall information flow.

From these considerations, it seems that at a certain point, the model probably makes the correct computation, but actually forgets the final target of the task (i.e. reversing the sequence).
We can check this conjecture by injecting an embedding and forcing the model to have the embedding of the token \texttt{8} as output embedding of the block at the $29^{th}$ layer.

The injection procedure starts when the user clicks on a cell in the heatmap, writes in the pop-up block the token to inject and presses again the generation button.
The generation process runs as before and redraws all the components, but this time while it is generating the embedding injecting is substituted to the real one at the specified location.
The output after this procedure is \emph{432348313}.
At first, it seems that the problem concerning the two swapped digits is solved, due to the fact that now the \texttt{8} and the \texttt{3} are in the correct positions, but this time the model added a digit at the end of the generation, instead of the new line character as before.
By inspecting the heatmap again, Fig, the user can notice that the probability of token \texttt{8} is notably increased.
Moreover, concerning the last wrong token, it is also clear that in the lower layer the model converged to the new line token (probably due to the end of the length of the sequence), but as the layer level reaches and is above the level of the injection, the output changed to a suboptimal token with respect to the final task.
This could be expected since we are injecting an embedding that was not crafted from the network at that particular layer, so there could be minor modifications that, through causal attention, are influencing the next tokens negatively.
}

\subsection{Discussion}

\todo[purple!20]{
We presented InTraVisTo, a tool to visualize the internal states and the information flow of Transformer Neural Network, the core of LLMs.
We developed InTraVisTo to provide a tool that NLP practitioners can use to understand the internal reasoning steps carried out by a LLM and, possibly, track down the causes of errors like hallucinations.
Ultimately, we hope our tool will help improve the reliability of LLMs.
Our focus at the moment is on providing better decoding and manipulation of hidden states, with focus on information injection, and improving the interface, to make InTraVisTo more user-friendly.
}


\section{Embedding Analysis}

Following the path laid out in \Cref{sec:rq_embeddings,sec:method_embeddings}, the experiments of this section will gravitate around the possibility of identifying linear properties modeling semantic relationships inside the input and output embedding spaces of LLMs.

To explore this inquiry, we have set up various experiments aimed at replicating some of the well-established embeddings properties~\cite{mikolov2013} in recent state-of-the-art architectures.
Additionally, we expand upon these ideas and provide further explanations for the behaviors emerged from the performed experiments.

\subsection{Dataset}

The dataset used for all the experiments under this research question was created by \todo{including} the original analogy dataset employed by word2vec~\cite{mikolov2013} and the \textit{BATS (Bigger Analogy Test Set)}~\cite{drozd2016}.
The word2vec dataset (also known as Google analogy dataset) is structured in two main files: \textit{question-words} and \textit{question-phrases}.
Question-words contains $14$ categories for a total of \todo{$x$} analogies spacing from linguistic relations to semantic relations, while question-phrases only features $5$ classes and a total of \todo{$x$} analogies regarding common-knowledge relations involving proper nouns.
BATS features a more complex hierarchy since it is meant to be an improvement over the Google dataset: it covers $4$ main relation types (inflectional morphology, derivational morphology, lexicographic semantics and encyclopedic semantics), each one being subdivided into $10$ further sub-categories containing $50$ unique word pairs each.
In addition, BATS provides multiple correct answers for word pairs when applicable. 

Due to the different format in which the two datasets have been released (BATS containing word pairs, while Google analogy dataset being comprised of already constructed analogies), some work has been put into collecting, standardizing and unifying analogies.
To this end, BATS word pairs have been combined into a total of $(50 \times 49) \times 10 \times 4 = 98000$ unique analogies to be combined with the \todo{$x + x = x$} analogies provided by the Google dataset.
After the removal of duplicate entries, the final comprehensive total of unique analogies amounts to \todo{$x$}.

\todo[green]{handling of multiple correct answers}
\todo[green]{address possible selection of some sub-categories for some experiments}
\todo[green]{address dataset reduction to single tokens for some experiments}

% Some of the entries within the dataset that featured underscores to delimit compound words, where modified to use spaces instead.

\subsection{Experiment 1}\label{ssec:exp_emb_exp1}

As a starting point, we directly compare the performance of various state-of-the art models over the defined analogy task.
We expect to observe similar results for architectures that are similar to one another, while accounting for the vocabulary and vector sizes of embeddings.

Interestingly, \citet{drozd2016} finds that scaling the vector size of embeddings has mixed effects in terms of performance when evaluating analogies with our chosen metrics.
Whereas logically, an increase in vocabulary size should always constitute an improvement over analogy resolution.
In this experiment we will verify the validity of these statements on the embeddings contained in large scale and newer models, while considering possible limitations in embedding expressiveness of these novel architectures.
Therefore, we expect to see a clear distinction in the results achieved by newer models against older architectures used as a baseline, that is an inverse trend which favors the embeddings built and trained over old frameworks in analogy resolution.

\subsubsection{Models}

Experiments were conducted using the input embeddings of bert-large-cased \todo[orange]{cite}, GPT-2 \todo[orange]{cite}, Gemma 2 \todo[orange]{cite}, LLaMA-2-7B \todo[orange]{cite}, LLaMA-3-7B \todo[orange]{cite}, Mistral-7B \todo[orange]{cite} and Phi-3-mini-instruct \todo[orange]{cite}.
Additionally, the word embeddings generated by Word2Vec \todo[orange]{cite} and GloVe \todo[orange]{cite} will also be included to be evaluated as a baseline.

\begin{table}[H]
    \centering
    \begin{tabular}{| c | c c c c |}
        \rowcolorhang{bluepoli!40}
        \hline
            \textbf{Model} & \makecell{\textbf{Parameter}\\\textbf{Count}} & \makecell{\textbf{Vocabulary}\\\textbf{Size}} & \makecell{\textbf{Embedding}\\\textbf{Size}} & \makecell{\textbf{Tied}\\\textbf{Embeddings}} \\
		\hline \hline
            \textbf{Word2Vec} & 1 & 1 & 1 & Yes \\[2px]
            \textbf{GloVe} & 1 & 1 & 1 & Yes \\[2px]
            \textbf{BERT large cased} & $336$M & $29$K & $1024$ & Yes \\[2px]
            \textbf{GPT 2} & $124$M & $50.3$K & $768$ & Yes \\[2px]
            \textbf{Gemma 2} & $9$B & $256$K & $3584$ & Yes \\[2px]
            \textbf{Llama 2} & $7$B & $32$K & $4096$ & No \\[2px]
            \textbf{Llama 3} & $8$B & $128.3$K & $4096$ & No \\[2px]
            \textbf{Mistral v0.3} & $7$B & $32.8$K & $4096$ & No \\[2px]
            \textbf{Phi 3.5 mini} & $3.8$B & $32.1$K & $3072$ & No \\[2px]
        \hline
    \end{tabular}
    \caption{\todo[red]{complete table}}
    \label{table:exp_emb_models}
\end{table}

\todo[green]{describe model architectures with reference to a table}

The evaluation of embeddings sourced from such a diverse set of models cannot be fair in nature.
More specifically, no effort has been made to align the datasets \todo{with which} these architectures were trained, opting for comparing their most popular iterations instead.
The chosen approach has clear limitations, but it is not within our scope to evaluate these models with the intention of finding the \todo{best performing one}.
Rather, we are interested in assessing the presence of high-level trends that justify the employment of an approach of less granular nature.

\subsubsection{Results}

\todo[purple!20]{
It becomes apparent that the majority of LLMs seem to underperform with respect to GPT-2, this notable deviation in performance could stem from various factors.
However, by inspecting Fig., which displays the same results, but for the single-token version of dataset, we can observe that the performance of GPT-2 alignes more closely with that of the other models.
This mismatch in performance may be attributed on the tokenization strategy and vocabulary size of the chosen models, in fact all three models employ a Byte-Pair Encoding (BPE) tokenizer 
%\cite{DBLP:conf/aaai/WangCG20}
, however the tokenizers utilized by both Mistral and LLaMA are based on SentencePiece 
%\cite{DBLP:conf/emnlp/KudoR18}
, presenting a smaller overall vocabulary (32,000 tokens against GPT-2's 50,257 tokens), thus possibly implying a reduced selection of full-word tokens in favor of an increased number of sub-word tokens.
By analyzing results on the single-only dataset in Fig., we are able to infer that the exclusion of multi-token words from the dataset also mitigates differences arising from how tokenizers segment words.
This effectively normalizes away instances of analogies where GPT-2 may have held an advantage over other LLMs by being able to fully encode certain words.
For the same reason, GPT-2 would have had both better representations encompassing semantic meanings of words without additional noise caused by multiple words sharing the same sub-word token, but also not having to be subject to the token-reconstruction previously mentioned strategies as ways to deal with multi-token words.
}

\subsection{Experiment 2}\label{ssec:exp_emb_exp2}

For this second experiment we wish to delve deeper into the actual capabilities of the embeddings belonging to recent decoder-only LLMs.
To this end, we will also take into consideration the unembedding layer of said models.
As mentioned in \Cref{ssec:background_transf_structure}, decoder-only transformers feature a reversed embedding matrix inside their language modeling head.
This matrix is used to translate the last hidden state produced by the transformer stack into the index for the next predicted token.
Since it shares a common structure with the actual embedding matrix, it should be possible to run the same analogy experiments and obtain meaningful, albeit different, results.

However, not all model architectures provide different embedding and unembedding matrices as already hinted in \Cref{ssec:background_transf_structure}.
Consequently, in this experiment we will also observe the differences between these architectural choices through the lens of analogies.

Additionally, inspired by the decoding approach introduced for the first research question (\Cref{sec:rq_intravisto}), we are also going to take into consideration interpolated embeddings.
Embedding interpolation, defined in \Cref{ssec:method_intravisto_decoding} and formalized through \Cref{eq:method_intravisto_linear-interp,eq:method_intravisto_quadratic-interp}, is a novel technique that exploits linear properties of embedding spaces and hidden representations~\cite{park2023, mikolov2013, drozd2016} to generate intermediate unembedding matrices with the purpose of decoding said intermediate states.
Logically, since we are performing linear interpolation operation, we expect to observe a trend in the analogy resolution accuracy of the interpolated embeddings that goes from the input embedding to the output one according to the interpolation percentage.
However, it could be possible that the actual rate of change for the interpolated performance might not be constant between the two end points. \todo[green]{what does this imply?}

\subsubsection{Models}

For the experiment at hand we select a smaller set of models to compare due to the fact that we wish to observe the differences between input and output embeddings.
All changes to the models will be made referencing \todo[orange]{model table in exp.1}, defined for \Cref{ssec:exp_emb_exp1}.

As a first step, we are going to discard models without proper sub-word tokenization such as Word2Vec and GloVe.
This allows us to also perform tests on the whole dataset, through the implementation of proper encoding strategies as defined in \Cref{eq:method_embeddings_multitok-in,eq:method_embeddings_multitok-out}.
In addition, we are not going to focus on models without distinct input and output embeddings such as GPT-2 and Gemma-2, therefore only GPT-2 will be included in order to provide a baseline evaluation.

\subsubsection{Results}

\todo[purple!20]{
Our analysis also reveals that for increasing $k$ values, output embeddings seem to consistently outperform input embeddings (contextually to the model performance), while for smaller $k$ values input embeddings demonstrate greater accuracy.
This phenomenon suggests that input embeddings provide more reliable estimates for analogies results, but their vectors are laying in a sparser latent spaces, thus making it less likely to accidentaly include the correct vector out of a wrong prediction by extending the range given by $k$.
Whereas output embeddings latent spaces are more compressed and benefit more from greater $k$ values.
Such theory is corroborated by the fact that the trend that was previously identified is much less relevant in Fig., where only single-token analogies are considered.
Furthermore, we observe that the difference between input and output embeddings at earlier $k$ values is more accentuated, and the performance of output embeddings almost never surpasses the one of input embeddings.
This may be due to the fact that overall (and in particular for input embeddings), it is less likely for the model to output a wrong result for any given single-token only analogy, therefore the margin of analogies with wrong answers according to input embeddings and only correct for output embeddings due to the fact that we are utilizing a large $k$ is greatly reduced.

Finally, we show that input and output embedding pairs belonging to the same model tend to have the same trend, and are generally more prone to end up having similar accuracies, which is not a completely trivial result since, despite having the same dimensionality and possibly sharing some of the datasets used to train them (depending on the training setup and initialization values), they still operate with completely different weight matrices and carry out different tasks inside the model.
}

\subsection{Discussion}

\todo[purple!20]{
In the end, we can say that recent LLMs still retain a certain amount of facts inside their embeddings, albeit in a noticeably less developed and more noisy way with regards to older models.
}

\section{First Order Prediction}

Building on the foundation established in \Cref{sec:rq_fom}, the experiments in this section will focus on exploring the implications of creating a First Order Model (FOM).
A FOM is obtained by removing the all intermediate architectural components from an LLM, leaving behind the input and output embedding layers, and the residual connections joining them.
As explained in \Cref{sec:method_fom}, this operation can be seen as the creation of a Markov model possessing a transition matrix formed by the product between the input and output embedding matrices.

Most of the experiments in this section are geared towards understanding if the FOM actually represents a faithful bigram Markov model over the original model's vocabulary.
Although past work has theorized and partially demonstrated on a restricted set of models the possibility of this hypothesis being \todo{correct}~\cite{elhage2021}, we suspect that FOMs extracted from different LLMs may have different degrees of \todo{`Markovness'} and we strive to explore this intuition in this section.
\todo[green]{Add part on weight tying}

\subsection{Dataset}

The dataset employed for this set of experiments is \emph{WikiText 103} \todo[orange]{cite}, containing a total of $1.81M$ rows of full articles sourced from Wikipedia \todo[orange]{cite}.
This dataset is used to train the unigram Markov model and to perform additional tests on various models by reserving a separate test split.

\subsection{Experiment 1}

The first experiment explores the main inquiry by means of direct matrix comparison.
The transition matrix for the model at hand is computed following \Cref{eq:method_fom_fom-matrix} for the purpose of being compared to the transition matrix belonging to an actual first order Markov model, and to the identity matrix.
This preliminary matrix comparison is performed through the norm difference as shown in \Cref{eq:method_fom_fom-markov-comp}.

As it will be later pointed out, the norm of the difference between transition matrices is not a reliable metric to measure their similarity.
Consequently, we choose to use set similarity metrics to
\todo[green]{set metrics and top-k}

\subsubsection{Models}

Similarly to the models taken in consideration for \Cref{ssec:exp_emb_exp2}, the main chosen transformer architectures have different input and output embeddings.
From these models we choose to analyze LLaMA-2-7B \todo[orange]{cite}, LLaMA-3-7B \todo[orange]{cite}, Mistral-7B \todo[orange]{cite} and Phi-3-small-8k-instruct \todo[orange]{cite}.

Whereas, the unigram Markov model is trained utilizing the training split of \emph{WikiText 103} \todo[orange]{cite} for a total of $1.8M$ rows.
The vocabulary of the unigram Markov model is constructed using the vocabulary of the transformer model that is currently being compared to, in order to enable 1-to-1 token comparisons for the transition matrices.
This is achieved by parsing the training text for the Markov model and manually processing the frequencies using the tokenizer of the transformer model at hand.

\todo[green]{training text token counts for each tokenizer}

\subsubsection{Results}

\todo[purple!20]{
By observing Fig. that depicts Mistral's performance on the self-regression and unigram-regression tasks, we can notice how, for any given value of $k$, the predictions generated by the FOM appear to align more closely with the inputs of the FOM rather than the most probable outputs generated by the unigram Markov model.
This observation holds even when considering that, for the sake of this comparison, all traces (besides the one having $k_2 = 1$) are essentially more lenient variants of the corresponding top-$k_1$ prediction, achieved by expanding the range of valid subsequent tokens generated by the Markov model.
This seems to suggest that the embeddings may exhibit a closer relationship to their inverse, rather than serving as representations of the subsequent token prediction based on the input.
Unfortunately, this observation appears to contradict the recorded matrix distances, since we have a distance of 178.7063 between the FOM transition matrix and the identity matrix and a distance of 76.9151 between the FOM and Markov model transition matrices.

However, these results are relative to LLaMA's performance on the same exact tasks.
By looking at Fig., which illustrates its perfomance on the self-regression task and Fig. representing its performance on the unigram-regression, we notice opposite results, the LLaMA FOM exhibits greater top-$k$ accuracy when compared to the unigram Markov model rather than its own inputs.
This peculiarity is noticeable by empirically examining the LLaMA FOM predictions for some common tokens, which appear to be plausible next tokens:
}

\begin{table}
\centering
\begin{tabular}{||c || c | >{\columncolor[gray]{0.9}}c | c||} 
\hline
Input token/Model   & Mistral FOM   & LLaMA FOM & Markov models \\ 
\hline\hline
mount               & gorith        & clock     &  s            \\ 
easy                & /******/      & going     & to            \\
hair                & teenth        & loss      & ,             \\
\hline
\end{tabular}
\end{table}

\todo[purple!20]{
Furthermore, the recorded matrix distances reveal a notable disparity in scale compared to those obtained using Mistral's FOM.
Specifically, a distance of 522.3115 observed between the FOM transition matrix and the identity matrix and a distance of 509.2659 recorded between the FOM and Markov model transition matrices.
Even for a model so clearly predisposed towards predicting the next word rather than returning the input word itself, the difference between the distances is minimal.
This suggests a potential bias towards lower distance estimates between the FOM transition matrix and the Markov transition matrix, possibly due smaller quantities of null values present in the latter matrix with regard to the identity matrix.
}

\subsection{Experiment 2}

The second experiment still focuses on the evaluation of transition matrices generated by concatenating input and output embeddings from transformer models with respect to actual unigram Markov models, but via deeper means of analysis.
For this experiment we decide to shift our view of the transition matrices from sets of elements to actual probability distributions.
This approach enables a more accurate analysis, and considers the models for how they would concretely be used outside from these experimental scenarios.

In \todo[orange]{ref} we defined the tools that we intend to use for this experiment, which are the Kullback-Leibler divergence \todo[orange]{ref} and the perplexity \todo[orange]{ref}.

\todo[green]{add description of KL div(?) and perplexity}

\subsubsection{Models}

\subsubsection{Results}



\subsection{Discussion}

\todo[purple!20]{
Overall, the feasibility of a FOM approximation appears highly dependent on the specific model under analysis, although overall, all analyzed First Order Models comprised by the combination of their input and output embeddings seem to exhibit a slight bias towards trying to approximate a unigram first order Markov model.
Extracting the embedding weights from the transition matrix of a unigram Markov model and utilizing them to initialize the embedding layer and decoder weights of an LLM would be an interesting potential development of this idea in future work.
}