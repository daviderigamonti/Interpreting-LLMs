In this section, we delineate the specific research questions regarding transformer interpretability that will be addressed in the present work.
The purpose of these research questions is to provide an overall informative analysis on the hidden representations and embedding space of recent transformer-based models.
The novelty of this work is not necessarily \todo{inherent in the} chosen approach, but resides in the combination of multiple known techniques to provide novel perspectives and interpretability tools with the ultimate purpose of confirming and possibly expanding upon previous findings.

\section{Do decoder-only LLMs retain linear properties in their embedding spaces?}

The first research question investigates whether decoder-only LLMs retain linear properties in their embedding spaces.
As mentioned in the previous chapters, the linearity of the embedding space is an established property both in the original word2vec embedding space \cite{mikolov2013} and in transformer-based encoder models such as BERT \todo[orange]{cite}.
A linear embedding space enables particular geometric properties such as the ability to solve word analogies (`king' - `man' + `woman' = `queen') and the possibility of modeling semantic similarity between tokens as a distance relationship.
Given the recent developments in transformer training and architecture, we aim to examine if these properties still hold in the embedding spaces of recent decoder-only LLMs and whether \todo{these properties} are consistent across various model architectures and sizes, while pointing out differences betwen the embedding and unembedding spaces in models \todo{where the two are different}.

\section{Is it possible to obtain first-order predictions by concatenating embedding spaces in LLMs?}

Following the investigation on different embedding weights between input and output spaces of a LM, the second research question explores the feasibility of obtaining first-order predictions by concatenating the two embedding spaces in LLMs.
A first-order prediction can be seen as a guess of the next token based solely on the previous one, similar to a first-order Markov model's predictive process, which employs a conditioned probability on the previous token to obtain the most likely next token.
As observed by \todo[orange]{cite}, indeed concatenating the input and output embeddings of a LM should theoretically yield the equivalent of a first-order Markov model when we concatenate the input and output embeddings of a LM, however, is that always the case?
We will test this hypothesis against new state-of-the-art models and provide novel insights into the role of different embedding spaces in recent LLMs.

\section{Interactive Visualization Tool for LLMs}

The third research question is centered around the development of an interactive visualization tool for LLMs.
The goal is to create a tool that allows users to visually explore and interpret the whole computation performed by an LLM when generating a sentence.
The chosen approach will be centered around the concept of vocabulary space decoding and will take the direction of the already mentioned logit lens \todo[orange]{cite}.
Additionally, new ideas will be introduced in the context of hidden representation decoding and visualization.
One of \todo{the central purposes} of the proposed tool is the visualization of the residual stream, making it possible to observe the path of influence composed of residuals and attention, that cumulates into each token prediction.
The creation of this transformer visualization framework is justified by an overall revisitation of the logit lens analysis from a new, \todo{mulifaceted} perspective with the ultimate goal of confirming established results from a new angle and possibly observing new phenomena. 
