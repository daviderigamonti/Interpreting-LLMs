In this section, we delineate the specific research questions regarding transformer interpretability that will be addressed in the present work.
The purpose of these research questions is to provide an overall informative analysis on the hidden representations and embedding space of recent transformer-based models.
The novelty of this work is not necessarily \todo{inherent in the} chosen approach, but resides in the combination of multiple known techniques to provide novel perspectives and interpretability tools with the ultimate purpose of confirming and possibly expanding upon previous findings.

\section{Interactive Visualization Tool for LLMs}

The first research question is centered around the development of an interactive visualization tool for LLMs.
The goal is to create a tool that allows users to visually explore and interpret the whole computation performed by an LLM when generating a sentence.
The chosen approach will be centered around the concept of vocabulary space decoding and will take the direction of the already mentioned logit lens \todo[orange]{cite}.
Additionally, new ideas will be introduced in the context of hidden representation decoding and visualization.
One of \todo{the central purposes} of the proposed tool is the visualization of the residual stream, making it possible to observe the path of influence composed of residuals and attention, that cumulates into each token prediction.

The creation of this transformer visualization framework is justified by an overall revisitation of the logit lens analysis from a new, \todo{mulifaceted} perspective with the ultimate goal of confirming established results from a new angle and possibly observing new phenomena.
This framework serves as a starting point for many of the inquiries that will be tackled in this work, as it provided the means of premiliminary analysis needed to \todo{kickstart} other, more \todo{targeted}, experiments.

\section{Do decoder-only LLMs retain linear properties in their embedding spaces?}

With the aid of the previously developed tool we noticed some peculiarities in the behavior of LLMs with respect to smaller and less \todo{sophisticated} language models, in particular, the embedding representations of LLMs seemed to encode less information about the actual word semantics.
From this starting point, the second research question investigates whether decoder-only LLMs retain linear properties in their embedding spaces.
As mentioned in the previous chapters, the linearity of the embedding space is an established property both in the original word2vec embedding space \cite{mikolov2013} and in transformer-based encoder models such as BERT \todo[orange]{cite}.
A linear embedding space enables particular geometric properties such as the ability to solve word analogies (`king' - `man' + `woman' = `queen') and the possibility of modeling semantic similarity between tokens as a distance relationship.
Given the recent developments in transformer training and architecture, we aim to examine if these properties still hold in the embedding spaces of recent decoder-only LLMs and whether \todo{these properties} are consistent across various model architectures and sizes, while pointing out differences between the embedding and unembedding spaces in models \todo{where the two are different}.

\section{Is it possible to obtain first-order predictions by concatenating embedding spaces in LLMs?}

Following the investigation on different embedding weights between input and output spaces of a LM, the third research question explores the feasibility of obtaining first-order predictions by concatenating the two embedding spaces in LLMs.
A first-order prediction can be seen as a guess of the next token based solely on the previous one, similar to a first-order Markov model's predictive process, which employs a conditioned probability on the previous token to obtain the most likely next token.
As observed by \todo[orange]{cite}, indeed concatenating the input and output embeddings of a LM should theoretically yield the equivalent of a first-order Markov model when we concatenate the input and output embeddings of a LM, however, is that always the case?
We will test this hypothesis against new state-of-the-art models and provide novel insights into the role of different embedding spaces in recent LLMs.
Moreover, this series of experiments should hopefully provide further understanding \todo{on} the practice of \emph{weight tying} \todo[orange]{cite} performed on the embedding layers of language models.