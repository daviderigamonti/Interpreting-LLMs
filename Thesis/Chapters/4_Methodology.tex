In this section we will explore the theoretical fundaments of our experiments.
To do so, we are going to subdivide our analysis into three main sections, where each one is paired with one of the previously identified research questions.

\section{Transformer Visualization}

The first section is dedicated to the exploration of the main features pertaining the proposed interactive tool for the exploration of autoregressive transformer architectures, \emph{InTraVisTo} (Inside Transformer Visualization Tool).

\todo[purple!20]{Start of paper section}

\emph{InTraVisTo} is an open-source visualization tool depicting the internal computations performed within a Transformer.
The tool provides visualizations of both the internal state of the LLM, using a heatmap of decoded embedding vectors for all layer/token positions, and the information flow between components of the LLM, using a Sankey diagram to depict paths through which information accumulates to produce next-token predictions.

\subsection{Decoding Internal States}\label{ssec:method_intravisto_decoding}

In this context, with the expression \emph{`decoding process'} we refer to the \emph{vocabulary decoding} process, which consists in the conversion of transformers hidden states, represented by vectors in a high-dimensional space, into human-readable content.
InTraVisTo enables the decoding and inspection of the main four vectors produced by each layer $\ell$ of a transformer:
\begin{itemize}
    \item $\gbm{\delta}_\textit{att}^{(\ell)}$ represents the output of the attention component.
    \item ${\gbm{x}'}^{(\ell)}$ is the \emph{intermediate state}, given by the addition of $\gbm{\delta}_\textit{att}^{(\ell)}$ to the residual stream.
    \item $\gbm{\delta}_\textit{ff}^{(\ell)}$ represents the output of the feedforward network component.
    \item $\gbm{x}^{(\ell)}$ is the \emph{layer output}, which can be seen as the residual stream with the contributions of both $\gbm{\delta}_\textit{ff}^{(\ell)}$ and $\gbm{\delta}_\textit{att}^{(\ell)}$.
\end{itemize}

Decoding the meaning of hidden state vectors at various depths of a transformer stack is essential for providing an intuition as to how the model is working.
We pose our focus on causal models, and assume the state-of-the-art LLM architecture, which involves a continuous \emph{decoration process} where each transformer layer adds the results of its computations to a residual embedding vector from the layer below.
InTraVisTo provides a human-interpretable representation of this internal decoration pipeline by decoding each hidden state with a specific decoder and displaying the most likely token from the model's vocabulary.

\subsubsection{Decoders and Normalization}

As for the decoding process, we compute the probability distribution over the vocabulary space for a hidden state $\gbm{x}$ in the following way:
\begin{equation}
    \label{eq:method_intravisto_decoding}
    P(\ \cdot \mid \gbm{x}, d_\textit{dec}, n_\textit{norm}) = \mathrm{softmax}\left(N_{n_\textit{norm}}(\gbm{x}) \cdot \gbm{W}_{d_\textit{dec}}\right)
\end{equation}
Where $\gbm{W}_{d_\textit{dec}}$ represents the matrix of decoder weights according to the user decoder choice $d_\textit{dec}$, and $N_{n_\textit{norm}}$ is used to identify the normalization operation selected by the user through $n_\textit{norm}$:
\begin{equation}
    N_{n_\textit{norm}}(\gbm{x}) = 
    \label{eq:method_intravisto_normalization}
    \left\{
    \begin{array}{cl}
        \gbm{x} &\ \text{if}\ n_\textit{norm} = \text{`no normalization'} \\
        \mathcal{N}{(\gbm{x})} &\ \text{if}\ n_\textit{norm} = \text{`normalize only'} \\
        \gbm{\gamma}_\ell \cdot \mathcal{N}{(\gbm{x})} + \gbm{\beta}_\ell &\ \text{if}\ n_\textit{norm} = \text{`normalize and scale'}
    \end{array}
    \right.
\end{equation}
\todo[cyan]{Fix notation}
Considering $\mathcal{N}{(\gbm{x})}$ the normalization component of the model's final normalization layer, being reliant on $\mu$ and $\sigma$ for LayerNorm implementations as defined in \Cref{eq:background_layernorm,eq:background_layernorm_extra}, and $RMS(\gbm{x})$ for RMSNorm implementations as defined in \Cref{eq:background_rmsnorm}

Two natural choices of decoders to use are the transpose of the \emph{input embedding matrix} $\gbm{W}_\textit{in}^\T$ used by the model to convert tokens to vectors on input, and the \emph{output decoder} $\gbm{W}_\textit{out}$ used upon output within the language modeling head.
Some models, like GPT-2 \todo[orange]{cite} and Gemma \todo[orange]{cite} tie these two parameter matrices together during training, while other popular models such as Mistral \todo[orange]{cite} and Llama \todo[orange]{cite} allow these two matrices to differ.
This structural weight difference will be analyzed more in depth in later sections, \todo{however,} in our current scope having different weight matrices for embedding and unembedding ($\gbm{W}_\textit{in}^\T \neq \gbm{W}_\textit{out}$) implies that earlier layers tend to be much more interpretable when decoded with the input embedding ($\gbm{W}_\textit{in}^\T$) while latter layers are more meaningful if the output decoder ($\gbm{W}_\textit{out}$) is used.

Previous work has looked to \emph{train specialized decoders} \todo[orange]{cite} for generating meaningful vocabulary distributions at any point in a model, at the cost of introducing a great deal of additional complexity and potential errors.
InTraVisTo employs a simpler and elegant alternative, by \emph{interpolating} the input and output decoders based on the depth $\ell\in\{0,\ldots,L\}$ of the model layer we wish to decode, we obtain a `hybrid' decoding weight matrix that acts as an equilibrium point calibrated on the current model depth.
We define various alternatives for decoder interpolation mainly focusing on \emph{linear interpolation} and \emph{quadratic interpolation}, defined as follows:
\begin{subequations}
    \begin{align}
        \gbm{W}_\textit{linear}^{(\ell)} =\left(1-\frac{\ell}{L}\right) \cdot \gbm{W}_\textit{in}^\T + \frac{\ell}{L} \cdot \gbm{W}_\textit{out} \label{eq:method_intravisto_linear-interp} \\
        \gbm{W}_\textit{quadratic}^{(\ell)} =\frac{{(L - \ell)}^2 \cdot \gbm{W}_\textit{in}^\T + \ell^2 \cdot \gbm{W}_\textit{out}}{L^2 - 2 \ell L + 2\ell^2} \label{eq:method_intravisto_quadratic-interp}
    \end{align}
\end{subequations}
\todo[cyan]{check again quadratic interpolation}
\todo[cyan]{It is important to note that the term `quadratic interpolation' comprises a small abuse of terminology, as it refers to a specific class of possible quadratic interpolations between the two decoders.
More specifically, the contributions given by the joint }

Any of the matrices $\gbm{W}_\textit{in}$, $\gbm{W}_\textit{out}$, $\gbm{W}_\textit{linear}$ and $\gbm{W}_\textit{quadratic}$ can be used as the decoder matrix $\gbm{W}_{d_\textit{dec}}$ to decode an embedding $\gbm{x}$ into a probability distribution over $\mathcal{V}$ as described in \Cref{eq:method_intravisto_decoding}.
As previously mentioned, this behavior selection is controlled by specifying $d_\textit{dec}$.

\subsubsection{Secondary Tokens}
    
As previously illustrated, each hidden representation is decoded by performing a normalization operation first, to then multiply the result for the chosen decoding matrix, and finally obtain a probability distribution over the model's vocabulary by applying a softmax function on the resulting logits.
At this point we utilize the same \emph{sampling policy} of the model in order to extract the `predicted' vocabulary token ID from the computed distribution.
We assume the policy to always be \emph{greedy decoding}, both as a simplifying factor and to reflect the default settings for causal models provided by Hugging Face's transformers library \todo[orange]{cite}.

Nevertheless, our interest in decoding the model's hypothetical intermediate predictions doesn't extend only to the first token, as important information that cannot be condensed into a single vocabulary token is usually withheld inside the hidden representation \todo[orange]{cite}.
In order to extract this `leftover information', we devise two main approaches that \todo{sublimate} it into \emph{secondary tokens}.
We define \emph{secondary tokens} as additional vocabulary tokens that are the result of a \emph{secondary decoding process}, aimed at obtaining tokens that hold less importance than the token obtained as a result of the \emph{primary decoding process} (as mentioned before) of the same hidden representation.

The first secondary decoding approach is \emph{Top-K probability decoding} and consists of expanding the number of selected tokens from the probability distribution to $k$, thus obtaining $k-1$ secondary tokens ordered by their probability values.
This is a rather simple and immediate technique, which is widely-used in many interpretability applications and non \todo[orange]{cite}.
The principal downside of this approach comes from the fact that the obtained secondary tokens might be overly similar to the primary token, resulting in redundant information.
This is likely caused by the fact that a large part of the hidden representation is used to represent the primary token, often skewing the embedding vector in favor of tokens that are semantically similar \todo[orange]{cite}.

To alleviate this issue we propose a novel way to extract secondary representations from a hidden representation: \emph{iterative decoding}.
The rationale behind the proposed approach is that hidden representations contain an overlap of concepts in an embedding space that is loosely related to both the input and output embedding spaces.
As a consequence, we postulate that hidden representations existing in these intermediate embedding spaces should retain the linear properties that have been ascertained to exist in the input and output embedding spaces of transformer architectures \todo[orange]{cite}.
Iterative decoding exploits these linear properties by performing a sequence of subtractions from the main hidden representation, removing the embedding of the most probable representation during each iteration.
\begin{algorithm}
    \caption{Iterative decoding algorithm}\label{alg:method_intravisto_iter-dec}
    \begin{algorithmic}
        \STATE{$tokens \gets \{\}$}
        \STATE{$norms \gets \{\}$}
        \STATE{$i \gets 0$}
        \WHILE{$i < {rep}_{max}$}
            \STATE{$id \gets \arg \max\{decode(emb)\}$}
            \STATE{${emb}_{real} \gets {(\gbm{W}_{d_{dec}})}_{id,\cdot}$}
            \IF{$\|emb\| \leq {norm}_{min} \vee \bigl( |norms| > 0 \wedge \|emb\| \geq norms\bigl[i-1\bigr] \wedge i \neq 0 \bigr)$}
                \STATE{\textbf{break}}
            \ENDIF{}
            \IF{$id \notin tokens$}
                \STATE{$tokens\bigl[|tokens|-1\bigr] \gets id$}
            \ENDIF{}
            \STATE{$norms\bigl[i\bigr] \gets \|emb\|$}
            \STATE{$emb \gets emb - {emb}_{real}$}
        \ENDWHILE{}
        \RETURN tokens
    \end{algorithmic}
\end{algorithm}
\todo[cyan]{check algorithm}
As it is possible to observe in \Cref{alg:method_intravisto_iter-dec}, we perform at most ${rep}_{max}$ iterations obtaining one primary token and between $0$ to ${rep}_{max} - 1$ secondary tokens.
This is due to the fact that a secondary token that is found in two separate iterations is recorded only on the first one, and the presence of a stopping condition that triggers in case the norm of the resulting hidden representation is under a certain threshold ${norm}_{min}$ or is higher than the norm found at the previous iteration.
This last condition is used to avoid situations where the embedding vector of the hidden state `flips' after the computation of the difference with the most probable representation, resulting in a vector that is not informative and can possibly reiterate the effect until ${rep}_{max}$ is reached, thus generating predictions based purely on noise. \todo[cyan]{check if necessary}

One downside of this approach, besides the linearity assumption of the intermediate embedding spaces \todo{which is based upon}, is the fact that there exists a notable shift in representation magnitudes throughout the layers of most state-of-the-art transformer models \todo[orange]{cite}.
This typically results in a steady increase in the norms of the embedding vectors, proportional to the layer number.
While this has only a marginal impact on the decoding process, it can significantly disrupt the embedding vector subtraction operation within the iterative decoding approach, as the quantities involved may differ in magnitude.

\subsubsection{Decoding Metrics}

Other meaningful quantities that are shown through the InTraVisTo visualization are tied to the actual probability distributions obtained through the decoding process.
The first option is \emph{``P(argmax term)''}, which directly translates into the probability of the most probable token output from the chosen decoder.
It gives an immediate idea of how much the model is sure about the token that has been greedily sampled.
On the other hand, a complementary measure is the \emph{entropy} of the probability distribution over the vocabulary space, which the higher it is, the more unsure the model is of the next token.
Entropy for an embedding $\gbm{x}$ is computed in the following way:
\begin{equation}
    \label{eq:method_intravisto_entropy}
    H_{\gbm{x}} = -\sum{P_{\gbm{x}} \cdot \log{P_{\gbm{x}}}}
\end{equation}
\todo[cyan]{check}
Where $P_{\gbm{x}} = P(\ \cdot \mid \gbm{x}, d_\textit{dec}, n_\textit{norm})$ as computed in \Cref{eq:method_intravisto_decoding}.

Other showcased metrics are the \emph{attention contribution} and \emph{feedforward contribution}, which measure respectively how much the output of the attention block, or feed forward, contributes in its summation with the residual stream of a transformer block.
The purpose of these metrics is to highlight where the main information of that block is coming from, whether from the attention or feed forward components.
We devised two main approaches to weigh the contribution of each component to the residual stream: one uses the \emph{norms} of hidden state vectors to compare the magnitude of their contribution, while the other uses the \emph{KL divergence} to compare the probability distribution similarity of the two hidden states against the final one.
In practice, we compute:
\begin{equation}
    \left\{
    \begin{aligned}
        &{\%}^{(\ell)}_{\textit{norm},\textit{att}} = \frac{\|\gbm{\delta}_\textit{att}^{(\ell)}\|_2}{\|\gbm{\delta}_\textit{att}^{(\ell)}\|_2 + \|\gbm{x}^{(\ell-1)}\|_2} \\
        &{\%}^{(\ell)}_{\textit{norm},\textit{ff}} = \frac{\|\gbm{\delta}_\textit{ff}^{(\ell)}\|_2}{\|\gbm{\delta}_\textit{ff}^{(\ell)}\|_2 + \|{\gbm{x}'}^{(\ell)}\|_2} \label{eq:method_intravisto_norm-contrib}
    \end{aligned}
    \right.
\end{equation}
\begin{equation}
    \left\{
    \begin{aligned}
        &{\%}^{(\ell)}_{\textit{KL},\textit{att}} = \frac{D_{\text{KL}}(\gbm{x}^{(\ell-1)} \parallel {\gbm{x}'}^{(\ell)})}{D_{\text{KL}}(\gbm{\delta}_\textit{att}^{(\ell)} \parallel {\gbm{x}'}^{(\ell)}) + D_{\text{KL}}(\gbm{x}^{(\ell-1)} \parallel {\gbm{x}'}^{(\ell)})} \\
        &{\%}^{(\ell)}_{\textit{KL},\textit{ff}} = \frac{D_{\text{KL}}({\gbm{x}'}^{(\ell)} \parallel \gbm{x}^{(\ell)})}{D_{\text{KL}}(\gbm{\delta}_\textit{ff}^{(\ell)} \parallel \gbm{x}^{(\ell)}) + D_{\text{KL}}({\gbm{x}'}^{(\ell)} \parallel \gbm{x}^{(\ell)})} \label{eq:method_intravisto_kl-contrib}
    \end{aligned}
    \right.
\end{equation}
Where $D_{\text{KL}}$ is the Kullback-Liebler divergence between two distributions, and the notation of hidden states (such as $\gbm{x}^{(\ell)}$, $\gbm{\delta}_\textit{att}^{(\ell)}$, \ldots) references the distinctions made at \Cref{ssec:method_intravisto_decoding}.
It is possible to note that the contributions computed through norms and KL divergence feature opposite terms at the fractional numerator, this is due to their inverse relationship as the KL divergence measures the \emph{dissimilarity} between probability distributions, while the norm can be directly translated into the positive contribution of a hidden state. 

\subsection{Flow}

The second vizualisation introduced in InTraVisTo is a Sankey diagram \todo[orange]{cite} that aims to depict the information flow through the transformer network.
Edges in the diagram indicate the amount of influence that the nodes have on each other and show how the information accumulates from the bottom of the diagram to the top in order to generate the final prediction.
The flow snakes its way through self-attention nodes, which combine information from attended tokens in the level below, feed-forward networks nodes, which introduce information based on detected patterns in the state vector, and aggregation nodes, where updates from the other two types of nodes are added to the residual vector.

In order to calculate the information flow, an attribution algorithm works backwards from the top layers of the network, recursively apportioning the incident flow from the components below based on their relative contributions to the internal state vector above using \Cref{eq:method_intravisto_norm-contrib,eq:method_intravisto_kl-contrib}.
The flow's constantly updating state can be defined in a way that reflects the recursive computations performed by InTraVisTo:
\begin{equation}
    \left\{
    \begin{alignedat}{2}
        &\textit{flow}_{\textit{ffnn}}^{(\ell,j)} &&= {\%}_{\textit{ffnn}}^{(\ell,j)} \cdot \textit{flow}_{x}^{(\ell,j)} \\
        &\textit{flow}_{x'}^{(\ell,j)} &&= \textit{flow}_{\textit{ffnn}}^{(\ell,j)} + (1 - {\%}_{\textit{ffnn}}^{(\ell,j)}) \cdot \textit{flow}_{\textit{x}}^{(\ell,j)} = \textit{flow}_{\textit{x}}^{(\ell,j)} \\
        &\textit{flow}_{\textit{att}}^{(\ell,j)} &&= {\%}_{\textit{att}}^{(\ell,j)} \cdot \textit{flow}_{x'}^{(\ell,j)} = {\%}_{\textit{att}}^{(\ell,j)} \cdot \textit{flow}_{x}^{(\ell,j)} \\
        &\textit{flow}_{x}^{(\ell-1,j)} &&= \sum_{i\in\{j,\ldots,k\}}{\overline{\textit{attend}{\,}}^{(\ell,i)}\bigl[j\bigr]}\cdot\textit{flow}_{\textit{att}}^{(\ell,i)} + ( 1 - {\%}_{\textit{att}}^{(\ell,j)})\cdot \textit{flow}_{\textit{x'}}^{(\ell,j)} \\
            &\quad &&= \biggl(\Bigl(\sum_{i\in\{j,\ldots,k\}}\overline{\textit{attend}{\,}}^{(\ell,i)}\bigl[j\bigr] - 1\Bigr)\cdot{\%}_{\textit{att}}^{(\ell,j)} + 1\biggr) \cdot \textit{flow}_{\textit{x}}^{(\ell,j)}
    \end{alignedat}
    \right.
\end{equation}
Where $\overline{\textit{attend}{\,}}^{(\ell,i)}$ denotes the average attention placed on token $j$ by the attention heads present at position $i$ of layer $\ell$.
This quantity is used to compute all outgoing contributions of a token $j$ to subsequent self-attention nodes, thus it is computed considering tokens between $j$ and $k$ as the attention sources, where $k$ denotes the index of the last token in the generated sentence.

Another type of information shown in the Sankey diagram concerns the computation of differences between residual representations, with the goal of visualizing the flow's `evolution' throughout the model's layers.
To achieve this objective we devised two main approaches that we implemented to explore this \todo{visualization dimension}.
The first approach consists \todo{of} computing the KL divergence between the probability distributions of hidden states belonging to each combination of component outputs and the residual stream state.
In practice, we compute the following quantity for five different state combinations:
\begin{equation}
    \label{eq:method_intravisto_kl-diff}
    \begin{gathered}
        {\textit{kl\_diff}{\,}\strut}_{\gbm{x}_a, \gbm{x}_b}^{(\ell)} = D_{\text{KL}}(P_{\gbm{x}_b} \parallel P_{\gbm{x}_a}) \\
        \text{for} \ (\gbm{x}_a, \gbm{x}_b) \in \Bigl\{
            (\gbm{x}^{(\ell-1)}, {\gbm{x}'}^{(\ell)}), 
            (\gbm{\delta}_\textit{att}^{(\ell)}, {\gbm{x}'}^{(\ell)}), 
            ({\gbm{x}'}^{(\ell)}, \gbm{\delta}_\textit{ffnn}^{(\ell)}), 
            ({\gbm{x}'}^{(\ell)}, \gbm{x}^{(\ell)}), 
            (\gbm{\delta}_\textit{ffnn}^{(\ell)}, \gbm{x}^{(\ell)})
        \Bigr\}
    \end{gathered}
\end{equation}
\todo[cyan]{check KL order}
Where $P_{\gbm{x}} = P(\ \cdot \mid \gbm{x}, d_\textit{dec}, n_\textit{norm})$ as computed in \Cref{eq:method_intravisto_decoding}, $D_{\text{KL}}$ is the Kullback-Liebler divergence between two distributions, and the notation of hidden states (such as $\gbm{x}^{(\ell)}$, $\gbm{\delta}_\textit{att}^{(\ell)}$, \ldots) references the distinctions made at \Cref{ssec:method_intravisto_decoding}.
Whereas, the second approach compares the residual stream states at the input and output location for each layer in the transformer stack.
It does so by calculating the difference between the two hidden states, and performing the decoding operation defined in \Cref{eq:method_intravisto_decoding} as to generate primary and secondary tokens for the resulting quantity.
This is showcased in the following computation:
\begin{equation}
    \label{eq:method_intravisto_state-diff}
    {\textit{state\_diff}{\,}\strut}^{(\ell)} = P(\ \cdot \mid \gbm{x}^{(\ell)} - \gbm{x}^{(\ell - 1)}, d_\textit{dec}, n_\textit{norm})
\end{equation}
It is possible to notice how this last approach could suffer from the same weaknesses that have been identified in iterative decoding as shown in \Cref{alg:method_intravisto_iter-dec}, due to the presence of an operation (difference) which assumes that its operands exist in a shared embedding space with linear properties.
Although this is true, the minimal distance between the hidden states used to compute the difference makes the presented issue have minimal impact on the actual decoding result.

\subsection{Injection}

Lastly, we also include the possibility to perform \emph{injections} in InTraVisTo.
\emph{Injections} are an instance of \emph{activation patching} \todo[orange]{cite} utilized in a context that is not strictly tied to a formal \emph{causal intervention} framework \todo[orange]{cite} .
In fact, the main purpose of injections is to give the user the possibility to explore the model predictions in an interactive way, making it possible to change outputs of components and parts of the residual stream in order to unveil interesting patterns and properties.

Being a type of intervention, it is possible to express an injection utilizing the \emph{do-operator} as defined by \todo[orange]{cite}.
Assuming that we would like to inject an embedding $\gbm{\hat x}$ into the hidden state obtained as a result of the transformation $f_c^{(\ell)}$ of a component $c^{(\ell)}$ during the $i$-th token of the model's forward pass expressed as $f$, we can notate our injection utilizing the do-operator as:
\begin{equation}
    \label{eq:method_intravisto_injection}
    f\Bigl(\gbm{x}_i \, \Big| \, \text{do}\bigl(f_c^{(\ell)}(\gbm{x}_i) = \gbm{\hat x}\bigr)\Bigr)
    \; \text{where}\ \gbm{\hat x} = \textit{encode}(\textit{id}_{\textit{inject}} \,|\, n_\textit{norm}, d_{dec})
\end{equation}
As previously noted for similar approaches, it is important to specify that there might be some problems with the nature of $\gbm{\hat x}$, as it is a representation that is generated from an encoded user-defined textual input ${id}_{\textit{inject}}$, which undergoes an embedding and normalization process utilizing $\gbm{W}_{d_{dec}}$ and $N_{n_\textit{norm}}$ following \Cref{eq:method_intravisto_linear-interp,eq:method_intravisto_quadratic-interp,eq:method_intravisto_normalization}.
This results in the injection of a `clean' embedding that is unexpected by the model, \todo{and thus} may destabilize the generation process.
In most causal intervention scenarios, this problem is not present when performing activation patching using counterfactual examples, due to the fact that patches are generated utilizing resampling or averaging approaches \todo[orange]{cite}.
We choose to avoid these techniques since our main focus is directed towards giving the user complete control over the injected information, which is achieved by directly inserting the wanted embedding vector in the hidden state without letting the model elaborate it to create a plausible representation.

Another noteworthy step in the injection process is the actual encoding of the injected embedding, since the formula used to compute $\gbm{\hat x}$ that has been broadly mentioned in \Cref{eq:method_intravisto_injection} is used considering an injection input that is composed by a single token.
However, InTraVisTo's interface offers the possibility of injecting an embedding representation that contains more than one token, thus we would like to generalize the aforementioned formula to actually handle multiple tokens at once.
\todo[cyan]{check if true} This behavior is modeled by averaging the embedding representations of all tokens that compose the sentence, always under the assumption of a linear embedding space \todo[orange]{cite}. \todo[green]{remarks for sentence embedding?}
Considering a total of $id_1,\ldots,id_t$ tokens to be encoded, this averaging operation can be represented in the following way:
\begin{equation}
    \label{eq:method_intravisto_emb-avg}
    \gbm{\hat x} = N_{n_\textit{norm}}\Bigl(\frac{1}{t} \sum_{i=1}^{t}{{(\gbm{W}_{d_{dec}})}_{\textit{id}_{i},\cdot}}\Bigr)
\end{equation}
The soundness of this approach, including possible variants based on the same or different assumptions will be analyzed more in depth in the next section (\Cref{sec:method_embeddings}).


\section{Embedding Analysis}\label{sec:method_embeddings}

\subsection{Analogies}
\subsection{Delta Analogies}

\section{First Order Prediction}

\subsection{Matrix Comparison}
\subsection{Prediction Comparison}