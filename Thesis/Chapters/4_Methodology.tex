In this section we will explore the theoretical fundaments of our experiments.
To do so, we are going to subdivide our analysis into three main sections, where each one is paired with one of the previously identified research questions.

\section{Transformer Visualization}\label{sec:method_intravisto}

The first section is dedicated to the exploration of the main features pertaining the proposed interactive tool for the exploration of autoregressive transformer architectures, \emph{InTraVisTo} (Inside Transformer Visualization Tool).

\emph{InTraVisTo} is an open-source visualization tool depicting the internal computations performed within a Transformer.
The tool provides visualizations of both the internal state of the LLM, using a heatmap of decoded embedding vectors for all layer/token positions, and the information flow between components of the LLM, using a Sankey diagram to depict paths through which information accumulates to produce next-token predictions.

\subsection{Decoding Internal States}\label{ssec:method_intravisto_decoding}

In this context, with the expression \emph{`decoding process'} we refer to the \emph{vocabulary decoding} process, which consists in the conversion of transformers hidden states, represented by vectors in a high-dimensional space, into human-readable content.
InTraVisTo enables the decoding and inspection of the main four vectors produced by each layer $\ell$ of a transformer:
\begin{itemize}
    \item $\gbm{\delta}_\textit{att}^{(\ell)}$ represents the output of the attention component.
    \item ${\gbm{x}'}^{(\ell)}$ is the \emph{intermediate state}, given by the addition of $\gbm{\delta}_\textit{att}^{(\ell)}$ to the residual stream.
    \item $\gbm{\delta}_\textit{ff}^{(\ell)}$ represents the output of the feedforward network component.
    \item $\gbm{x}^{(\ell)}$ is the \emph{layer output}, which can be seen as the residual stream with the contributions of both $\gbm{\delta}_\textit{ff}^{(\ell)}$ and $\gbm{\delta}_\textit{att}^{(\ell)}$.
\end{itemize}

Decoding the meaning of hidden state vectors at various depths of a transformer stack is essential for providing an intuition as to how the model is working.
We pose our focus on causal models, and assume the state-of-the-art LLM architecture, which involves a continuous \emph{decoration process} where each transformer layer adds the results of its computations to a residual embedding vector from the layer below.
InTraVisTo provides a human-interpretable representation of this internal decoration pipeline by decoding each hidden state with a specific decoder and displaying the most likely token from the model's vocabulary.

\subsubsection{Decoders and Normalization}\label{sssec:method_intravisto_decoding_norm}

As for the decoding process, we compute the probability distribution over the vocabulary space for a hidden state $\gbm{x}$ in the following way:
\begin{equation}
    \label{eq:method_intravisto_decoding}
    P(\ \cdot \mid \gbm{x}, d_\textit{dec}, n_\textit{norm}) = P(\ \cdot \mid \gbm{x}, \gbm{W}_{d_\textit{dec}}, N_{n_\textit{norm}}) = \operatorname{softmax}\Bigl(N_{n_\textit{norm}}(\gbm{x}) \cdot \gbm{W}_{d_\textit{dec}}\Bigr)
\end{equation}
Where $\gbm{W}_{d_\textit{dec}}$ represents the matrix of decoder weights according to the user decoder choice $d_\textit{dec}$, and $N_{n_\textit{norm}}$ is used to identify the normalization operation selected by the user through $n_\textit{norm}$:
\begin{equation}
    N_{n_\textit{norm}}(\gbm{x}) = 
    \label{eq:method_intravisto_normalization}
    \left\{
    \begin{array}{cl}
        \gbm{x} &\ \text{if}\ n_\textit{norm} = \text{`no normalization'} \\
        \mathcal{N}{(\gbm{x})} &\ \text{if}\ n_\textit{norm} = \text{`normalize only'} \\
        \gbm{\gamma}_\ell \cdot \mathcal{N}{(\gbm{x})} + \gbm{\beta}_\ell &\ \text{if}\ n_\textit{norm} = \text{`normalize and scale'}
    \end{array}
    \right.
\end{equation}
Considering $\mathcal{N}{(\gbm{x})}$ the normalization component of the model's final normalization layer, being reliant on $\mu$ and $\sigma$ for LayerNorm implementations as defined in~\cref{eq:background_layernorm,eq:background_layernorm_extra}, and $RMS(\gbm{x})$ for RMSNorm implementations as defined in~\cref{eq:background_rmsnorm}

Two natural choices of decoders to use are the transpose of the \emph{input embedding matrix} $\gbm{W}_\textit{in}^\T$ used by the model to convert tokens to vectors on input, and the \emph{output decoder} $\gbm{W}_\textit{out}$ used upon output within the language modeling head.
Some models, like GPT-2~\cite{radford2019} and Gemma~\cite{mesnard2024,rivi2024} tie these two parameter matrices together during training, while other popular models such as Mistral~\cite{jiang2023} and Llama~\cite{touvron2023,dubey2024} allow these two matrices to differ.
This structural weight difference will be analyzed more in depth in later sections, \todo{however,} in our current scope having different weight matrices for embedding and unembedding ($\gbm{W}_\textit{in}^\T \neq \gbm{W}_\textit{out}$) implies that earlier layers tend to be much more interpretable when decoded with the input embedding ($\gbm{W}_\textit{in}^\T$) while latter layers are more meaningful if the output decoder ($\gbm{W}_\textit{out}$) is used.

Previous work has looked to \emph{train specialized decoders}~\cite{belrose2023a,sakarvadia2023,pal2023} for generating meaningful vocabulary distributions at any point in a model, at the cost of introducing a great deal of additional complexity and potential errors.
InTraVisTo employs a simpler and elegant alternative, by \emph{interpolating} the input and output decoders based on the depth $\ell\in\{0,\ldots,L\}$ of the model layer we wish to decode, we obtain a `hybrid' decoding weight matrix that acts as an equilibrium point calibrated on the current model depth.
We define various alternatives for decoder interpolation mainly focusing on \emph{linear interpolation}, \emph{quadratic interpolation} and \emph{max-probability interpolation}, defined as follows:
\begin{subequations}
    \begin{align}
        \gbm{W}_\textit{linear}^{(\ell)} &=\left(1-\frac{\ell}{L}\right) \cdot \gbm{W}_\textit{in}^\T + \frac{\ell}{L} \cdot \gbm{W}_\textit{out} \label{eq:method_intravisto_linear-interp} \\
        \gbm{W}_\textit{quadratic}^{(\ell)} &=\left(1-\left(\frac{\ell}{L}\right)^2\right) \cdot \gbm{W}_\textit{in}^\T + \left(\frac{\ell}{L}\right)^2 \cdot \gbm{W}_\textit{out} \label{eq:method_intravisto_quadratic-interp} \\
        \gbm{W}_\textit{max\_p} &=\operatornamewithlimits{argmax}_{\gbm{W} \in \{\gbm{W}_\textit{in}^\T, \gbm{W}_\textit{out}\}} \operatornamewithlimits{max}_{v \in \mathcal{V}} P(v \mid \gbm{x}, \gbm{W}, N_{n_\textit{norm}}) \label{eq:method_intravisto_max-p}
    \end{align}
\end{subequations}

Any of the matrices $\gbm{W}_\textit{in}^\T$, $\gbm{W}_\textit{out}$, $\gbm{W}_\textit{linear}$, $\gbm{W}_\textit{quadratic}$ and $\gbm{W}_\textit{max\_p}$ can be used as the decoder matrix $\gbm{W}_{d_\textit{dec}}$ to decode an embedding $\gbm{x}$ into a probability distribution over $\mathcal{V}$ as described in~\cref{eq:method_intravisto_decoding}.
As previously mentioned, this behavior selection is controlled by specifying $d_\textit{dec}$.
It is important to note that the term `quadratic interpolation' can be misleading, as the interpolation between decoding matrices is still linear and \todo{it is the} layer index ratio $\frac{\ell}{L}$ that scales quadratically through the models' layers.

\subsubsection{Secondary Tokens}\label{sssec:method_intravisto_decoding_tokens}
    
As previously illustrated, each hidden representation is decoded by performing a normalization operation first, to then multiply the result for the chosen decoding matrix, and finally obtain a probability distribution over the model's vocabulary by applying a softmax function on the resulting logits.
At this point we utilize the same \emph{sampling policy} of the model in order to extract the `predicted' vocabulary token ID from the computed distribution.
We assume the policy to always be \emph{greedy decoding}, both as a simplifying factor and to reflect the default settings for causal models provided by Hugging Face's transformers library~\cite{wolf2020}.

Nevertheless, our interest in decoding the model's hypothetical intermediate predictions does not only extend to the first token, as important information that cannot be condensed into a single vocabulary token is usually withheld inside the hidden representation~\cite{elhage2022,henighan2023,elhage2023}.
In order to extract this `leftover information', we devise two main approaches that \todo{sublimate} it into \emph{secondary tokens}.
We define \emph{secondary tokens} as additional vocabulary tokens that are the result of a \emph{secondary decoding process}, aimed at obtaining tokens that hold less importance than the token obtained as a result of the \emph{primary decoding process} (as mentioned before) of the same hidden representation.

The first secondary decoding approach is \emph{Top-$k$ probability decoding} and consists of expanding the number of selected tokens from the probability distribution to $k$, thus obtaining $k-1$ secondary tokens ordered by their probability values.
This is a rather simple and immediate technique, which is widely-used in many interpretability applications \todo{and non}~\cite{belrose2023a,pal2023,tufanov2024}.
The principal downside of this approach comes from the fact that the obtained secondary tokens might be overly similar to the primary token, resulting in redundant information.
This is likely caused by the fact that a large part of the hidden representation is used to represent the primary token, often skewing the embedding vector in favor of tokens that are semantically similar~\cite{elhage2022}.

To alleviate this issue we propose a novel way to extract secondary representations from a hidden representation: \emph{iterative decoding}.
The rationale behind the proposed approach is that hidden representations contain an overlap of concepts in an embedding space that is loosely related to both the input and output embedding spaces.
As a consequence, we postulate that hidden representations existing in these intermediate embedding spaces should retain the linear properties that have been ascertained to exist in the input and output embedding spaces of transformer architectures~\cite{mikolov2013,park2023}.
Iterative decoding exploits these linear properties by performing a sequence of subtractions from the main hidden representation, removing the embedding of the most probable representation during each iteration.

\begin{algorithm}
    \caption{Iterative decoding algorithm.}\label{alg:method_intravisto_iter-dec}
    \begin{algorithmic}
        \STATE{$tokens \gets \{\}$}
        \STATE{$norms \gets \{\}$}
        \STATE{$i \gets 0$}
        \WHILE{$i < {rep}_{max}$}
            \STATE{$id \gets \arg \max\{decode(emb)\}$}
            \STATE{${emb}_{real} \gets {(\gbm{W}_{d_{dec}})}_{id,\cdot}$}
            \IF{$\|emb\| \leq {norm}_{min} \vee \bigl( |norms| > 0 \wedge \|emb\| \geq norms\bigl[i-1\bigr] \wedge i \neq 0 \bigr)$}
                \STATE{\textbf{break}}
            \ENDIF{}
            \IF{$id \notin tokens$}
                \STATE{$tokens\bigl[|tokens|-1\bigr] \gets id$}
            \ENDIF{}
            \STATE{$norms\bigl[i\bigr] \gets \|emb\|$}
            \STATE{$emb \gets emb - {emb}_{real}$}
        \ENDWHILE{}
        \RETURN tokens
    \end{algorithmic}
\end{algorithm}

As it is possible to observe in~\cref{alg:method_intravisto_iter-dec}, we perform at most ${rep}_{max}$ iterations obtaining one primary token and between $0$ to ${rep}_{max} - 1$ secondary tokens.
This is due to the fact that a secondary token that is found in two separate iterations is recorded only on the first one, and the presence of a stopping condition that triggers in case the norm of the resulting hidden representation is under a certain threshold ${norm}_{min}$ or is higher than the norm found at the previous iteration.
This last condition is used to avoid situations where the embedding vector of the hidden state `flips' after the computation of the difference with the most probable representation, resulting in a vector that is not informative and can possibly reiterate the effect until ${rep}_{max}$ is reached, thus generating predictions based purely on noise.

One downside of this approach, besides the linearity assumption of the intermediate embedding spaces \todo{which is based upon}, is the fact that there exists a notable shift in representation magnitudes throughout the layers of most state-of-the-art transformer models~\cite{heimersheim2023}.
This typically results in a steady increase in the norms of the embedding vectors, proportional to the layer number.
While this has only a marginal impact on the decoding process, it can significantly disrupt the embedding vector subtraction operation within the iterative decoding approach, as the quantities involved may differ in magnitude.

\subsubsection{Decoding Metrics}\label{sssec:method_intravisto_decoding_metrics}

Other meaningful quantities that are shown through the InTraVisTo visualization are tied to the actual probability distributions obtained through the decoding process.
The first option is \emph{``P(argmax term)''}, which directly translates into the probability of the most probable token output from the chosen decoder. % chktex 36
It gives an immediate idea of how much the model is sure about the token that has been greedily sampled.
On the other hand, a complementary measure is the \emph{entropy} of the probability distribution over the vocabulary space, which the higher it is, the more unsure the model is of the next token.
Entropy for an embedding $\gbm{x}$ is computed in the following way:
\begin{equation*}
    \label{eq:method_intravisto_entropy}
    H_{\gbm{x}} = -\sum{P_{\gbm{x}} \cdot \log{P_{\gbm{x}}}}
\end{equation*}
Where $P_{\gbm{x}} = P(\ \cdot \mid \gbm{x}, d_\textit{dec}, n_\textit{norm})$ as computed in~\cref{eq:method_intravisto_decoding}.

Other showcased metrics are the \emph{attention contribution} and \emph{feedforward contribution}, which measure respectively how much the output of the attention block, or feed forward, contributes in its summation with the residual stream of a transformer block.
The purpose of these metrics is to highlight where the main information of that block is coming from, whether from the attention or feed forward components.
We devised two main approaches to weigh the contribution of each component to the residual stream: one uses the \emph{norms} of hidden state vectors to compare the magnitude of their contribution, while the other uses the \emph{KL divergence} to compare the probability distribution similarity of the two hidden states against the final one.
In practice, we compute:
\begin{equation}
    \left\{
    \begin{aligned}
        &{\%}^{(\ell)}_{\textit{norm},\textit{att}} = \frac{\|\gbm{\delta}_\textit{att}^{(\ell)}\|_2}{\|\gbm{\delta}_\textit{att}^{(\ell)}\|_2 + \|\gbm{x}^{(\ell-1)}\|_2} \\
        &{\%}^{(\ell)}_{\textit{norm},\textit{ff}} = \frac{\|\gbm{\delta}_\textit{ff}^{(\ell)}\|_2}{\|\gbm{\delta}_\textit{ff}^{(\ell)}\|_2 + \|{\gbm{x}'}^{(\ell)}\|_2} \label{eq:method_intravisto_norm-contrib}
    \end{aligned}
    \right.
\end{equation}
\begin{equation}
    \left\{
    \begin{aligned}
        &{\%}^{(\ell)}_{\textit{KL},\textit{att}} = \frac{D_{\text{KL}}(\gbm{x}^{(\ell-1)} \parallel {\gbm{x}'}^{(\ell)})}{D_{\text{KL}}(\gbm{\delta}_\textit{att}^{(\ell)} \parallel {\gbm{x}'}^{(\ell)}) + D_{\text{KL}}(\gbm{x}^{(\ell-1)} \parallel {\gbm{x}'}^{(\ell)})} \\
        &{\%}^{(\ell)}_{\textit{KL},\textit{ff}} = \frac{D_{\text{KL}}({\gbm{x}'}^{(\ell)} \parallel \gbm{x}^{(\ell)})}{D_{\text{KL}}(\gbm{\delta}_\textit{ff}^{(\ell)} \parallel \gbm{x}^{(\ell)}) + D_{\text{KL}}({\gbm{x}'}^{(\ell)} \parallel \gbm{x}^{(\ell)})} \label{eq:method_intravisto_kl-contrib}
    \end{aligned}
    \right.
\end{equation}
Where $D_{\text{KL}}$ is the Kullback-Liebler divergence between two distributions, and the notation of hidden states (such as $\gbm{x}^{(\ell)}$, $\gbm{\delta}_\textit{att}^{(\ell)}$, \ldots) references the distinctions made at~\cref{ssec:method_intravisto_decoding}.
It is possible to note that the contributions computed through norms and KL divergence feature opposite terms at the fractional numerator, this is due to their inverse relationship as the KL divergence measures the \emph{dissimilarity} between probability distributions, while the norm can be directly translated into the positive contribution of a hidden state. 

\subsection{Flow}\label{ssec:method_intravisto_flow}

The second visualization introduced in InTraVisTo is a \emph{Sankey diagram} that aims to depict the information flow through the transformer network.
Edges in the diagram indicate the amount of influence that the nodes have on each other and show how the information accumulates from the bottom of the diagram to the top in order to generate the final prediction.
The flow snakes its way through self-attention nodes, which combine information from attended tokens in the level below, feed-forward networks nodes, which introduce information based on detected patterns in the state vector, and aggregation nodes, where updates from the other two types of nodes are added to the residual vector.
The proposed Sankey diagram, technically qualifies as an \emph{Alluvial diagram} due to the fact that nodes are grouped into `steps' (in our case corresponding to layers), which provide intermediate subdivisions of the flow mass, and help to \todo{focalize attention} on the changes in flow composition throughout the model's layers.

In order to calculate the information flow, an attribution algorithm works backwards from the top layers of the network, recursively apportioning the incident flow from the components below based on their relative contributions to the internal state vector above using~\cref{eq:method_intravisto_norm-contrib,eq:method_intravisto_kl-contrib}.
The flow's constantly updating state can be defined in a way that reflects the recursive computations performed by InTraVisTo:
\begin{equation}
    \label{eq:method_intravisto_flows}
    \left\{
    \begin{alignedat}{2}
        &\textit{flow}_{\textit{ffnn}}^{(\ell,j)} &&= {\%}_{\textit{ffnn}}^{(\ell,j)} \cdot \textit{flow}_{x}^{(\ell,j)} \\
        &\textit{flow}_{x'}^{(\ell,j)} &&= \textit{flow}_{\textit{ffnn}}^{(\ell,j)} + (1 - {\%}_{\textit{ffnn}}^{(\ell,j)}) \cdot \textit{flow}_{\textit{x}}^{(\ell,j)} = \textit{flow}_{\textit{x}}^{(\ell,j)} \\
        &\textit{flow}_{\textit{att}}^{(\ell,j)} &&= {\%}_{\textit{att}}^{(\ell,j)} \cdot \textit{flow}_{x'}^{(\ell,j)} = {\%}_{\textit{att}}^{(\ell,j)} \cdot \textit{flow}_{x}^{(\ell,j)} \\
        &\textit{flow}_{x}^{(\ell-1,j)} &&= \sum_{i\in\{j,\ldots,k\}}{\overline{\textit{attend}{\,}}^{(\ell,i)}\bigl[j\bigr]}\cdot\textit{flow}_{\textit{att}}^{(\ell,i)} + ( 1 - {\%}_{\textit{att}}^{(\ell,j)})\cdot \textit{flow}_{\textit{x'}}^{(\ell,j)} \\
            &\quad &&= \biggl(\Bigl(\sum_{i\in\{j,\ldots,k\}}\overline{\textit{attend}{\,}}^{(\ell,i)}\bigl[j\bigr] - 1\Bigr)\cdot{\%}_{\textit{att}}^{(\ell,j)} + 1\biggr) \cdot \textit{flow}_{\textit{x}}^{(\ell,j)}
    \end{alignedat}
    \right.
\end{equation}
Where $\overline{\textit{attend}{\,}}^{(\ell,i)}$ denotes the average attention placed on token $j$ by the attention heads present at position $i$ of layer $\ell$.
This quantity is used to compute all outgoing contributions of a token $j$ to subsequent self-attention nodes, thus it is computed considering tokens between $j$ and $k$ as the attention sources, where $k$ denotes the index of the last token in the generated sentence.

Another type of information shown in the Sankey diagram concerns the computation of differences between residual representations, with the goal of visualizing the flow's `evolution' throughout the model's layers.
To achieve this objective we devised two main approaches that we implemented to explore this \todo{visualization dimension}.
The first approach consists \todo{of} computing the KL divergence between the probability distributions of hidden states belonging to each combination of component outputs and the residual stream state.
In practice, we compute the following quantity for five different state combinations:
\begin{equation}
    \label{eq:method_intravisto_kl-diff}
    \begin{gathered}
        {\textit{kl\_diff}{\,}\strut}_{\gbm{x}_a, \gbm{x}_b}^{(\ell)} = D_{\text{KL}}(P_{\gbm{x}_b} \parallel P_{\gbm{x}_a}) \\
        \text{for} \ (\gbm{x}_a, \gbm{x}_b) \in \Bigl\{
            (\gbm{x}^{(\ell-1)}, {\gbm{x}'}^{(\ell)}), 
            (\gbm{\delta}_\textit{att}^{(\ell)}, {\gbm{x}'}^{(\ell)}), 
            ({\gbm{x}'}^{(\ell)}, \gbm{\delta}_\textit{ffnn}^{(\ell)}), 
            ({\gbm{x}'}^{(\ell)}, \gbm{x}^{(\ell)}), 
            (\gbm{\delta}_\textit{ffnn}^{(\ell)}, \gbm{x}^{(\ell)})
        \Bigr\}
    \end{gathered}
\end{equation}
Where $P_{\gbm{x}} = P(\ \cdot \mid \gbm{x}, d_\textit{dec}, n_\textit{norm})$ as computed in~\cref{eq:method_intravisto_decoding}, $D_{\text{KL}}$ is the Kullback-Liebler divergence between two distributions, and the notation of hidden states (such as $\gbm{x}^{(\ell)}$, $\gbm{\delta}_\textit{att}^{(\ell)}$, \ldots) references the distinctions made at~\cref{ssec:method_intravisto_decoding}.
Whereas, the second approach compares the residual stream states at the input and output location for each layer in the transformer stack.
It does so by calculating the difference between the two hidden states, and performing the decoding operation defined in~\cref{eq:method_intravisto_decoding} as to generate primary and secondary tokens for the resulting quantity.
This is showcased in the following computation:
\begin{equation}
    \label{eq:method_intravisto_state-diff}
    {\textit{state\_diff}{\,}\strut}^{(\ell)} = P(\ \cdot \mid \gbm{x}^{(\ell)} - \gbm{x}^{(\ell - 1)}, d_\textit{dec}, n_\textit{norm})
\end{equation}
It is possible to notice how this last approach could suffer from the same weaknesses that have been identified in iterative decoding as shown in~\cref{alg:method_intravisto_iter-dec}, due to the presence of an operation (difference) which assumes that its operands exist in a shared embedding space with linear properties.
Although this is true, the minimal distance between the hidden states used to compute the difference makes the presented issue have minimal impact on the actual decoding result.

\subsection{Injection}\label{ssec:method_intravisto_injection}

Lastly, we also include the possibility to perform \emph{injections} in InTraVisTo.
\emph{Injections} are an instance of \emph{activation patching}~\cite{olsson2022,meng2022,hanna2023,conmy2023,wang2023,mohebbi2023,zhang2024} utilized in a context that is not strictly tied to a formal \emph{causal intervention} framework~\cite{geiger2021,mcgrath2023}.
In fact, the main purpose of injections is to give the user the possibility to explore the model predictions in an interactive way, making it possible to change outputs of components and parts of the residual stream in order to unveil interesting patterns and properties.

Being a type of intervention, it is possible to express an injection utilizing the \emph{do-operator} as defined by~\citet{pearl2009}.
Assuming that we would like to inject an embedding $\gbm{\hat x}$ into the hidden state obtained as a result of the transformation $f_c^{(\ell)}$ of a component $c^{(\ell)}$ during the $i$-th token of the model's forward pass expressed as $f$, we can notate our injection utilizing the do-operator as:
\begin{equation}
    \label{eq:method_intravisto_inj_complete}
    f\Bigl(\gbm{x}_i \, \Big| \, \text{do}\bigl(f_c^{(\ell)}(\gbm{x}_i) = \gbm{\hat x}\bigr)\Bigr)
    \;\; \text{where} \;\; \gbm{\hat x} = \textit{encode}(\textit{id}_{\textit{inject}} \,|\, n_\textit{norm}, d_{dec})
\end{equation}
As previously noted for similar approaches, it is important to specify that there might be some problems with the nature of $\gbm{\hat x}$, as it is a representation that is generated from an encoded user-defined textual input ${id}_{\textit{inject}}$, which undergoes an embedding and normalization process utilizing $\gbm{W}_{d_{dec}}$ and $N_{n_\textit{norm}}$ following~\cref{eq:method_intravisto_linear-interp,eq:method_intravisto_quadratic-interp,eq:method_intravisto_normalization}.
This results in the injection of a `clean' embedding that is unexpected by the model, \todo{and thus} may destabilize the generation process.
In most causal intervention scenarios, this problem is not present when performing activation patching using counterfactual examples, due to the fact that patches are generated utilizing resampling or averaging approaches~\cite{hanna2023,conmy2023,wang2023}.
We choose to avoid these techniques since our main focus is directed towards giving the user complete control over the injected information, which is achieved by directly inserting the wanted embedding vector in the hidden state without letting the model elaborate it to create a plausible representation.
We \todo{refer to} this approach as \emph{complete replacement injection}.

Given the shortcomings of completely replacing an internal representation belonging to the model, we also devise a novel alternative to perform injections in a less destructive \todo{way}.
This idea stems from an interpretation of internal states as linear combinations of token embeddings, akin to the approach taken for the iterative decoding algorithm illustrated in~\cref{alg:method_intravisto_iter-dec}.
In practice, we directly modify the hidden state by computing a term consisting of the difference between the embedding representations of our injection $\gbm{\hat x}$ and the most likely token $\gbm{e}_{\gbm{h}}^{\textit{max}}$ obtained by decoding the internal state following~\cref{eq:method_intravisto_decoding}.
The additional term is weighted by a singleton scaling factor, computed as the dot product between the original hidden state and the embedding of the most likely decoded token $\gbm{e}_{\gbm{h}}^{\textit{max}}$.
We \todo{refer to} this approach as \emph{main component replacement injection}, and we \todo{it as}:
\begin{equation}
    \label{eq:method_intravisto_inj_main}
    \begin{aligned}
    \gbm{h'} = \gbm{h} + (\gbm{h} \cdot \gbm{e}_{\gbm{h}}^{\textit{max}})(\gbm{\hat x} - \gbm{e}_{\gbm{h}}^{\textit{max}})
    \;\; \text{where} \;\; &\gbm{e}_{\gbm{h}}^{\textit{max}} = {(\gbm{W}_{d_{dec}})}_{v_{\gbm{h}}^{\textit{max}},\cdot} \; , \\
    &v_{\gbm{h}}^{\textit{max}} = \operatornamewithlimits{argmax}_{v \in \mathcal{V}} P(v \mid \gbm{h}, d_\textit{dec}, N_{n_\textit{norm}})
    \end{aligned}
\end{equation}
In this case, referencing~\cref{eq:method_intravisto_inj_complete}, $\gbm{h}$ corresponds to the target hidden state $f_c^{(\ell)}(\gbm{x}_i)$, which gets \todo{modified} into $\gbm{h'}$ by $\gbm{\hat x}$.

Another noteworthy step in the injection process is the actual encoding of the injected embedding, since the formula used to compute $\gbm{\hat x}$ that has been broadly mentioned in~\cref{eq:method_intravisto_inj_complete,eq:method_intravisto_inj_main} is used considering an injection input that is composed by a single token.
However, InTraVisTo's interface offers the possibility of injecting an embedding representation that contains more than one token, thus we would like to generalize the aforementioned formula to actually handle multiple tokens at once.
This behavior is modeled by averaging the embedding representations of all tokens that compose the sentence, always under the assumption of a linear embedding space~\cite{mikolov2013,park2023}.
Considering a total of $id_1,\ldots,id_t$ tokens to be encoded, this averaging operation can be represented in the following way:
\begin{equation}
    \label{eq:method_intravisto_emb-avg}
    \gbm{\hat x} = N_{n_\textit{norm}}\Bigl(\frac{1}{t} \sum_{i=1}^{t}{{(\gbm{W}_{d_{dec}})}_{\textit{id}_{i},\cdot}}\Bigr)
\end{equation}
The soundness of this approach, including possible variants based on the same or different assumptions is \todo{tackled} in the following section (\cref{sec:method_embeddings}).


\section{Embedding Analysis}\label{sec:method_embeddings}

This second section is geared towards understanding the extent to which text embeddings still retain some degree of semantic factuality in LLMs (Large Language Models).
Early transformer models have been noted for their text embedding representations possessing interesting spatial properties deeply correlated with the semantic properties of the embedded words~\cite{allen2019,kalinowski2020}.
However, with the emergence of newer models that are larger and more intricate than ever before, our objective is to assess whether the semantic properties inherently tied with the geometry of embeddings still hold true.

\subsection{Word Encoding}

The first step for performing experiments on embedding representations, is defining a set of operations that make the encoding and comparison of embedding vectors possible.
In the following section we will illustrate the theoretical basis of the preliminary steps that were used to set up the actual experiments.

\subsubsection{Distance Metrics}

In order to compare and find `close' embeddings, we need to formalize a similarity measure between two embeddings. 
To this end, we define two main distance metrics that will be used to compare embedding vectors in their space:
\begin{equation}
    d_{\textit{dist}}(\gbm{x}, \gbm{y}) = 
    \label{eq:method_embeddings_distance}
    \left\{
    \begin{array}{cl}
        \sqrt{\sum_{i=1}^{D}{{(x_i - y_i)}^2}} &\ \text{if}\ \textit{dist} = \text{`euclidean'} \\
        1 - \frac{\sum_{i=1}^{D}{x_i y_i}}{\sqrt{\sum_{i=1}^{D}{x_i^2}} \cdot \sqrt{\sum_{i=1}^{D}{y_i^2}}} &\ \text{if}\ \textit{dist} = \text{`cosine'}
    \end{array}
    \right.
\end{equation}
The \emph{cosine similarity/distance} metric is the most immediate and widely used approach to compare embeddings since it utilizes the angle between two vectors to determine their similarity, its inherent normalization means that the magnitude of the vectors is not taken into consideration and only their directionality is compared.
On the other hand, the \emph{euclidean distance} is a less common distance metric that we decided to include to lay out an alternate interpretation option, which can provide a different geometric perspective over the embedding space.
Unfortunately, in order to make the euclidean distance an effective metric we need to normalize it separately.

\subsubsection{Normalization}

On the topic of normalization, we also include the possibility of \emph{`pre-normalizing'} the embedding space before performing the experiments.
This practice should help improve the comparability of embedding vectors, enhancing the results of linear operations in the embedding space.
Although it is also possible for it to have detrimental effects in some specific scenarios, mainly due to the intrinsic nonlinearity of the normalization operation.
In practice, the `pre-normalization' step includes the normalization of all embedding vectors using the euclidean norm as follows:
\begin{equation}
    \label{eq:method_embeddings_normalization}
    \gbm{W}_{\textit{emb}} = (\bar w_{ij})_{V \times D}
    \ \ \text{where}\ \bar w_{ij} = \frac{w_{ij}}{{\| w_{ij} \|}_2}
\end{equation}

\subsubsection{Multi-token Words}

Handling multi-token words is a vital aspect of our analysis, as we may encounter words that are split into multiple sub-word tokens by the model's tokenization process.
As we will see later in~\cref{ssec:method_embeddings_analogies}, our main experiments for this section will concern the evaluation of analogies and similarities between word groups.
As anticipated in~\cref{ssec:method_intravisto_injection}, the main choice of computing multi-token falls towards the usage of linear operations from the supposed preservation of linear features in the embedding space~\cite{mikolov2013,park2023}.
Therefore, we devised three main strategies to address multi-token words when they are fed into the embedding as inputs of analogies, along with two additional strategies to handle them as valid outputs of analogies.

Strategies applicable to both cases involve considering only the first token of a multi-token word (this causes results to be slightly worse on average, but the impact seems to be negligible in most cases).
For multi-token words as inputs, other approaches include calculating the average as already done in~\cref{eq:method_intravisto_emb-avg} or the sum over all the embedded tokens that form the multi-token word.
Conversely, for output multi-token words, we subdivide them into their token components, to then consider all tokens as targets for assessing the top-$k$ accuracy of an analogy result in the embedding space.
Formalizing the previously identified alternatives we obtain two $\textit{encode}$ functions controlled by $\textit{enc\_strat}$ for a series of $t$ tokens obtained from a word $w$:
\begin{equation}
    \label{eq:method_embeddings_multitok-in}
    {\textit{encode}}_{\textit{enc\_strat}}^{in}(w) = 
    \left\{
    \begin{array}{cl}
        \gbm{e}_1^w &\ \text{if}\ \textit{enc\_strat} = \text{`first\_only'} \\
        \frac{1}{t}\sum_{i=1}^{t}{\gbm{e}_i^w} &\ \text{if}\ \textit{enc\_strat} = \text{`average'} \\
        \sum_{i=1}^{t}{\gbm{e}_i^w} &\ \text{if}\ \textit{enc\_strat} = \text{`sum'}
    \end{array}
    \right.
\end{equation}
\begin{equation}
    \label{eq:method_embeddings_multitok-out}
    {\textit{encode}}_{\textit{enc\_strat}}^{out}(w) = 
    \left\{
    \begin{array}{cl}
        \{ tok_1^w \} &\ \text{if}\ \textit{enc\_strat} = \text{`first\_only'} \\
        \{ tok_1^w, \ldots, tok_t^w \} &\ \text{if}\ \textit{enc\_strat} = \text{`subdivide'}
    \end{array}
    \right.
\end{equation}
\vspace{0.25em}
\begin{equation*}
    \text{where}\  tok_1^w,\ldots,tok_t^w = \textit{tokenize}(w)
    ,\ \ \gbm{e}_1^w, \ldots, \gbm{e}_{t}^w = \Bigl\{ {{(\gbm{W}_{emb})}_{\textit{tok}_{i},\cdot}} \ |\ \forall tok_i \in \textit{tokenize}(w) \Bigr\}
\end{equation*}
One important distinction between the $\textit{encode}$ functions for input and output is the type of \todo{data} returned.
When performing input encoding we are interested in obtaining a single aggregated embedded representation of the target word, whereas, when encoding a word to be checked against the outputs of our experiment we need a set of token identifiers in the vocabulary space.
The first preliminary transformations performed on $w$ is tokenization, modeled as the $\textit{tokenize}$ function, and utilizing the model's tokenizer to obtain a set of token identifiers $tok_1^w,\ldots,tok_t^w$ from $w$.
After tokenization, tokens get transformed into embedding vectors $\gbm{e}_1^w, \ldots, \gbm{e}_{t}^w$ by selecting the corresponding row from the embedding matrix $\gbm{W}_{emb}$.
The role of the embedding matrix $\gbm{W}_{emb}$ can be fulfilled by either the input and output embedding matrices, as already seen previously.

Another approach that will be used to completely set aside issues tied with multi-token words consists in reducing the dataset to exclusively consider analogies composed of single-token words, at the cost of less overall valid samples.
This topic is discussed more in depth in the experimental setup section dedicated to the related set of experiments (\cref{sssec:exp_emb_exp1_expset}).

\subsection{Word Analogies}\label{ssec:method_embeddings_analogies}

We wish to establish if geometric relationships can still be modeled inside the input or output embedding space of recent LLM architectures.
The first step \todo{towards the direction of this approach} was the direct experimentation on \emph{word analogy tasks}, as already illustrated in~\citet{mikolov2013}.
Solving word analogies showcases the ability of an embedding space to model the semantics of words in a relatively consistent way, implying the existence of embedding dimensions with associated meanings (even if overlapping or under superposition as mentioned by~\citet{elhage2022,henighan2023}), which is not necessarily an expected feature of language models on a large scale.

\todo{In their purest form}, word analogies consist of $4$ words that satisfy a mutual analogy relationship of the type:
\begin{center}
    \emph{``The dog barks as the cat meows''} $\Longleftrightarrow$ $\textit{`dog'}\ :\ \textit{`bark'}\ =\ \textit{`cat'}\ :\ \textit{`meow'}$
\end{center}
As previously mentioned, in the context of transformer models, these types of analogies are commonly used to test whether semantic relationships between words are encoded as linear transformations in the embedding space of models.
There exist \todo{many} types of analogies, encoding different paradigms across various semantic fields and morphological connections.
For example~\citet{drozd2016} identify $4$ main relationship types between words in their \emph{BATS (Bigger Analogy Test Set)}: 
\begin{itemize}
    \item \textbf{Inflectional morphology}: $\textit{`user'}\!:\!\textit{`users'}$, $\textit{`hot'}\!:\!\textit{`hotter'}$, $\textit{`enjoy'}\!:\!\textit{`enjoying'}$, \dots
    \item \textbf{Derivational morphology}: $\textit{`able'}\!:\!\textit{`unable'}$, $\textit{`apply'}\!:\!\textit{`reapply'}$, $\textit{`nice'}\!:\!\textit{`nicely'}$, \dots
    \item \textbf{Encyclopedic semantics}: $\textit{`madrid'}\!:\!\textit{`spain'}$, $\textit{`dante'}\!:\!\textit{`poet'}$, $\textit{`cabbage'}\!:\!\textit{`green'}$, \dots
    \item \textbf{Lexicographic semantics}: $\textit{`armchair'}\!:\!\textit{`chair'}$, $\textit{`nap'}\!:\!\textit{`sleep'}$, $\textit{`after'}\!:\!\textit{`before'}$, \dots
\end{itemize}
By combining two pairs of words from the same category we obtain word analogies in their full form, and they can be resolved by performing appropriate geometric transformation on the vectors that represent words.

\subsubsection{Analogy Computation}

As far as the actual word analogy computation is concerned, given 4 words $\{w_1, w_2, w_3, w_4\}$ set up as an analogy of the type `$w_1 : w_2 = w_3 : w_4$', we can formalize it as follows:
\begin{equation}
    \label{eq:method_embeddings_analogy}
    \begin{gathered}
        closest = \operatornamewithlimits{argmin}_{\forall v \in \mathcal{V}} d_{\textit{dist}}\bigl(\tilde w, \textit{encode}^{\textit{emb}}(v)\bigr) \\
        \tilde w = \textit{analogy}(w_1, w_2, w_3, w_4)
    \end{gathered}
\end{equation}
Where $d_{\textit{dist}}$ is the chosen distance metric as illustrated in~\cref{eq:method_embeddings_distance}, and $\textit{encode}^{\textit{emb}}$ refers to a simple encoding of a single token $v \in \mathcal{V}$ using the embedding matrix $\gbm{W}_{\textit{emb}}$.
On the other hand, \emph{`closest'} indicates the returned value, which is the actual token identifier of the closest embedding (of a word present in vocabulary $\mathcal{V}$) to the computed result of the word analogy.
Furthermore, the $\textit{analogy}$ function expresses the combination of words $\{w_1, w_2, w_3, w_4\}$ that composes the logic relationship of the word analogy.
In case we need a more robust evaluation support for comparing models, there also exists the possibility of gathering the top-$k$ closest embeddings to the obtained result instead of only representing the first one.
Additionally, due to the fact that it is possible to define multiple analogy test cases from a single combination of words, the definitions of the actual \emph{analogy function} implementations slightly falls out of the scope of this section, while keeping in mind that they will be discussed more in depth inside the experimental setup section of the corresponding experiment (\cref{sssec:exp_emb_exp1_expset}).
Thus, for simplicity, we can assume to \todo{follow} the \todo{renowed} `$king - man + woman \approx queen$' example from~\citet{mikolov2013}, obtaining:
\begin{equation}
    \label{eq:method_embeddings_analogy-function}
    \begin{aligned}
        \tilde w &= \textit{analogy}(w_1, w_2, w_3, w_4) \\
        &= {\textit{encode}}_{\textit{enc\_strat}}^{in}(w_1) - {\textit{encode}}_{\textit{enc\_strat}}^{in}(w_2) + {\textit{encode}}_{\textit{enc\_strat}}^{in}(w_4) \\
        &\color{gray} \approx {\textit{encode}}_{\textit{enc\_strat}}^{out}(w_3) 
    \end{aligned}
\end{equation}
Where the ${\textit{encode}}_{\textit{enc\_strat}}$ function follows the definitions laid out in~\cref{eq:method_embeddings_multitok-in,eq:method_embeddings_multitok-out}.

Another perspective over the classic word analogies resolution strategy consists in the \emph{offset analogies} task.
It holds a similar premise and the set of input analogies is shared between tasks, however, we perform a further subdivision of the dataset, assigning each word analogy into a bigger group based on the actual relationship that is modeled by the analogy (such as `gender', `capital of', `royalty', \todo{etc.}).
Subsequently, we use multiple analogies from the same group to obtain the relationship term as a vector (offset term), and we observe if it is possible to shift the representation from an element of the analogy to the other utilizing the computed offset.
This techniques shares many similarities with the \emph{3CosAvg} approach suggested by \citet{drozd2016}, however, we offer a more specific computation for the offset term identified as $\gbm{\Delta}$.
\begin{equation}
    \label{eq:method_embeddings_delta-analogy-function}
    \begin{aligned}
        \tilde{w}^i &= \textit{offset\_analogy}(\gbm{\Delta}_{\textit{batch}}^{(w_3, w_4)}, w_3^i, w_4^i) \\
        &= {\textit{encode}}_{\textit{enc\_strat}}^{in}(w_3^i) + \gbm{\Delta}_{\textit{batch}}^{(w_3, w_4)} \\
        &\color{gray} \approx {\textit{encode}}_{\textit{enc\_strat}}^{out}(w_4^i) \\
        \gbm{\Delta}_{\textit{batch}}^{(w_a, w_b)} &= \frac{1}{B_{\textit{batch}}}\sum_{j=1}^{B_{\textit{batch}}}{\Bigl({\textit{encode}}_{\textit{enc\_strat}}^{in}(w_b^j) - {\textit{encode}}_{\textit{enc\_strat}}^{in}(w_a^j)\Bigr)} \\
        \text{where}&\ \textit{batch} = \Bigl\{ \{w_1^i, w_2^i, w_3^i, w_4^i\} \ |\ \forall i \in \{1,\ldots, B_{\textit{batch}}\} \Bigr\}
    \end{aligned}
\end{equation}
As it is possible to observe in~\cref{eq:method_embeddings_delta-analogy-function}, we are computing the analogy term considering only \todo{half} of the word available in the analogy; in the cited case $w_3$ and $w_4$.
This is \todo{done} since, as explained in the first paragraphs of~\cref{ssec:method_embeddings_analogies}, word analogies are effectively composed by two pairs of words, each representing a semantic or morphological relationship.
Thus, considering that in the offset analogies scenario we are looking to extract a vector that represents the common relationship modeled by a subgroup of analogies, we only need a single occurrence for each \todo{single} pair of words.

\subsubsection{Analogy Evaluation}\label{subsubsec:method_embeddings_evaluation}

One of the key aspects of the proposed analogy approaches is the actual evaluation process of a set of model's embeddings on a given analogy task.
To assess the models' performance in this particular scenarios we considered two main metrics: the top-$k$ accuracy across all analogies, and the \emph{rankscore}.

The top-$k$ accuracy is a well established scoring metric that considers the number of times that a correct analogy solution appears inside the first $k$ results, averaging over all $n$ analogy samples present in the dataset.
In our case, it can be written using the following notation:
\begin{equation}
    \label{eq:method_embeddings_topk-accuracy}
    \textit{acc}_k = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}\Bigl( {\left\{ {\textit{result}\,}^i_j \ |\ \forall j = 1,\ldots,k \right\}} \cap {\textit{sol}\,}^i \neq \emptyset \Bigr)
\end{equation}
where ${\textit{result}\,}^i_j$ represents the $j$-th result of the $i$-th analogy, ${\textit{sol}\,}^i$ indicates the set of solution for the $i$-th analogy obtained through the use of the ${\textit{encode}}^{out}$ function as illustrated in~\cref{eq:method_embeddings_multitok-out}, and $n$ is the dimension of the considered dataset.

%\todo[purple!30]{Mean Reciprocal Rank / Rankscore evaluation of analogies}
%Alternatively, we devised an additional measure which fulfills a similar purpose to the top-$k$ accuracy by evaluating a set of embeddings on an analogy task.
%The chosen metric is a slight variation of the Mean Reciprocal Rank statistic measure, commonly referred to as $MRR = \frac{1}{n}\sum_{i=1}{n}{\frac{1}{rank_i}}$ \todo[orange]{cite?}.
%We considered this approach in order to fully utilize the rank information given by the \todo{comparison analysis}, rather than just limiting the computation of our metric to check for the presence of matches between the results and the solutions, as it is done in the top-$k$ accuracy case.
%Rankscore:
%\begin{equation}
%    \label{eq:method_embeddings_rankscore}
%    \textit{rankscore}_k = 1 - \frac{\sum_{i=1}^{n}{rank_i}}{n \cdot k}
%\end{equation}
%\todo[cyan]{Understand if rankscore makes sense, eventually adapt it to MRR or remove MRR as a reference}


\section{First Order Prediction}\label{sec:method_fom}

This third section explores the theoretical background of experiments analyzing the relationship between output embeddings (represented by output decoder weights) and input embeddings in the model's initial embedding layer.
In particular, we are interested in observing if there exists any particular function that is being performed by the output decoder against the input embeddings, such as inversion or even an attempt at predicting the most likely next token.

\subsection{Markov Models}

The hypothesis regarding output embeddings carrying out a prediction of the most likely subsequent token based on the input is inspired by architectural deconstructions of decoder-only transformer models, revealing residual links directly connecting the embedding layer to the decoder~\cite{vaswani2017}.
On this topic, \citet{elhage2021} claim that zero layer transformers model bigram statistics and that the bigram table can be accessed directly from the weights, although their experimental results only confirm their hypotheses for a restricted set of models.
Since the task of decoder-only models is to predict the next word given the previous context~\cite{radford2019}, by stripping from the architecture all the transformer blocks while keeping only a direct link from the input embedding layer to the output decoder layer, yields a \emph{First Order Model (FOM)} at best capable of predicting the next token given the previous one, akin to the behavior of first-order Markov models~\cite{markov2006}.
Such Markov models leverage bigram probability estimates retrieved from a corpus of text in order to generate the next word, provided with the previous one.
Given words $w_i$ and $w_j$, the bigram probability can be expressed as:
\begin{equation}
    \label{eq:method_fom_markov-prob}
    P(w_j|w_i) = P_{ij} = \frac{P(w_i, w_j)}{P(w_i)}
\end{equation}

The state-based stochastic transitions of a Markov model can be entirely described \todo{by} a transition matrix $\gbm{Q} \in \mathbb {R}_{+}^{|S| \times |S|}$ given the state space $S$.
The transition matrix is defined as:
\begin{equation*}
    \gbm{Q} = \left(P_{ij}\right)_{i,j} \;\; \forall i,j \in 1,\dots,|S|
\end{equation*}
Where $P_{ij}$ follows the definition specified in~\cref{eq:method_fom_markov-prob} and, representing the transition probability of moving to state $j$ from state $i$, must respect $\sum_{j=1}^{|S|}{P_{ij}} = 1 \;\; \forall j \in 1,\dots,|S|$.

\subsection{Matrix Comparison}\label{ssec:method_fom_matrix}

As a preliminary way to explore various relationships between the input and output embeddings, we decide to perform an analysis of their matrices.
To do so, we compute the `transition matrix' for our First Order Model (FOM) by combining and transposing the input and output weight matrices as follows:
\begin{equation}
    \label{eq:method_fom_fom-matrix}
    \gbm{Q}_{\textit{FOM}} = \operatorname{softmax}(\gbm{Q}_{\textit{FOM}}^{log}) = \operatorname{softmax}(\gbm{W}_{in} \cdot \gbm{W}_{out}^\T)
\end{equation}
Where $\gbm{W}_{in}$ and $\gbm{W}_{out}$ are, respectively, the input and output embedding weight matrices.
One important detail is the presence of the \emph{softmax} operator, which is applied to each row of the resulting logit transition matrix $\gbm{Q}_{\textit{FOM}}^{log}$ in order to obtain a transition matrix representing actual probabilities over the vocabulary space.
This operation is necessary \todo{since} the direct product between embedding matrices returns series of logit distributions over the vocabulary, which do not benefit from the \todo{properties of} probability distributions.
However, not all experiments require the computation of the actual transition matrix $\gbm{Q}_{\textit{FOM}}$ as experiments that consider top-$k$ aggregations and other greedy approximations of next-token predictions can also be performed with $\gbm{Q}_{\textit{FOM}}^{log}$ since the ordering of elements is not \todo{modified} by the softmax operator.
To avoid confusion, the \todo{aforementioned} distinction is \todo{made} only for practical experiments present in~\cref{sec:exp_fom}, whereas the \todo{theoretical groundwork} \todo{present} in this section \todo{only acknowledges} the probability matrix $\gbm{Q}_{\textit{FOM}}$ as the FOM transition matrix.

\todo{For reasons examined in depth} in~\cref{sec:exp_fom,sssec:exp_fom_exp1_expset}, we also define a FOM transition matrix with the addition of RMSNorm (\todo{from here} simply RMS normalization).
RMS normalization is a normalization technique defined in~\cref{eq:background_rmsnorm}, \todo{which} is commonly found in multiple places inside recent transformer architectures as highlighted in~\cref{ssec:background_transf_current}.
In our case, we split the RMS computation between input and output embedding matrices by performing the actual RMS normalization portion of the computation in the input and the weight product on the output.
\begin{equation}
    \label{eq:method_fom_fom-matrix-rms}
    \begin{gathered}
        \gbm{Q}_{\textit{FOM}}^{RMS} = \operatorname{softmax}(\gbm{W}_{in,RMS} \cdot \gbm{W}_{out,RMS}^\T) \\
        \text{where}\ \left\{
            \begin{array}{cl}
                \gbm{W}_{in,RMS} &= \left(\frac{\gbm{w}_i^{in}}{RMS(\gbm{w}_i^{in})}\right)_{i,\cdot} = \left(\frac{\gbm{w}_i^{in}}{\sqrt{\frac{1}{D}\sum_{j=1}^{D}{(w_{ij}^{in})^2}}}\right)_{i,\cdot} \\
                \gbm{W}_{out,RMS} &= \left(\gbm{\gamma}^\T \odot \gbm{w}_j^{out}\right)_{\cdot,j}
            \end{array}
        \right.
    \end{gathered}
\end{equation}
In particular, $\gbm{w}_i^{in}$ refers to row vectors extracted from $\gbm{W}_{in}$, $w_{ij}^{in}$ refers to elements extracted from $\gbm{W}_{in}$ and $\gbm{w}_j^{out}$ refers to column vectors extracted from $\gbm{W}_{out}$, therefore:
\begin{equation*}
    \gbm{w}_i^{in} = \left(\gbm{W}_{in}\right)_{i,\cdot},\ \ w_{ij}^{in} = \left(\gbm{W}_{in}\right)_{i,j},\ \ \gbm{w}_j^{out} = \left(\gbm{W}_{out}\right)_{\cdot,j},
\end{equation*}
Additionally, the multiplicative weights $\gbm{\gamma}$ associated with RMS normalization in~\cref{eq:method_fom_fom-matrix-rms} \todo{are the} weights of the RMSNorm layer positioned at the end of the stack of transformer layers, before the unembedding layer in recent decoder architecture, as seen in~\cref{fig:background_llama-arch}.

At this point, we are able to use the newly defined matrix to compare it against \todo{others} in order to quantify its resemblance against other known transition matrices.
Our metric of choice for direct matrix comparison is the Frobenius norm, which is a relatively simple way to estimate how two matrices differ from each other.
Due to its inherent simplicity, the Frobenius norm can be considered a naïve option for evaluating matrix distance in a \todo{scenario composed of} transition matrices.
Indeed, the results obtained in~\cref{sssec:exp_fom_exp1_results} \todo{expose} the Frobenius norm as an unreliable comparison metric with respect to other measures, \todo{but} still capable of providing some amount of valuable insight to the experiments formalized in~\cref{ssec:exp_fom_exp1,ssec:exp_fom_exp2}. 

To explore the potential inverse relationship, we compute the distance between the FOM transition matrix $\gbm{Q}_{\textit{FOM}}$ and the identity matrix of equivalent dimensions $\gbm{I}_V$.
This follows the idea that a Markov model where every state leads into itself is represented by an identity matrix, thus if our input/output embedding pair is attempting to converge towards being equal (as it is enforced by weight tying) we should observe the FOM transition matrix degenerating into an identity one.
We can model the distance between the FOM matrix and the identity matrix in the following way:

\begin{equation}
    \label{eq:method_fom_fom-i-comp}
    d_{\textit{FOM},I} = \| \gbm{Q}_{\textit{FOM}} - \gbm{I}_V \|_F
\end{equation}

Following the same intuition, it is also possible to compare the FOM transition matrix $\gbm{Q}_{\textit{FOM}}$ with the transition matrix of a bigram Markov model $\gbm{Q}_{\textit{markov}}$ trained on a \todo{statistically relevant dataset}:

\begin{equation}
    \label{eq:method_fom_fom-markov-comp}
    d_{\textit{FOM},\textit{markov}} = \| \gbm{Q}_{\textit{FOM}} - \gbm{Q}_{\textit{markov}} \|_F
\end{equation}

\subsection{Prediction Comparison}\label{ssec:method_fom_pred}

Additionally, we wanted to approach the previous ideas from a more practical point of view.
To this end we recorded the top-$k$ answers elicited using the FOM matrix $\gbm{Q}_{\textit{FOM}}$ across all tokens inside the model's vocabulary, using each token as the known word and obtaining the FOM's most probable next tokens.
Once we have the set of generated next tokens for each token inside the vocabulary we can compare them to our actual hypothesized reference tokens, obtained applying the identity function in the `inversion' hypothesis case, and in the `Markov' hypothesis case by querying the Markov model in the same way that we did with the FOM\@.
As already done before in~\cref{subsubsec:method_embeddings_evaluation} using~\cref{eq:method_embeddings_topk-accuracy}, we employ the top-$k$ accuracy metric to establish the similarity between the predictions of the FOM against the two baselines defined by the identity function and the Markov model.
The top-$k$ accuracy is computed by taking the $k$ tokens corresponding to the $k$ highest values between logits, and observing if the input token is present among them.

Generalizing, we can formalize the average top-$k$ accuracy between two models represented by their transition matrices $\gbm{Q}_1$ and $\gbm{Q}_2$ according to the respective $k_1$ and $k_2$ variables in the following way:
\begin{equation}
    \label{eq:method_fom_topk}
    \begin{aligned}
    \textit{accuracy}_{k_1,k_2}(\gbm{Q}_1, \gbm{Q}_2) = \frac{1}{V} \sum_{v \in \mathcal{V}}{\textit{tok\_overlap}_{k_1,k_2}(\gbm{Q}_1, \gbm{Q}_2, v)} \\
    \textit{tok\_overlap}_{k_1,k_2}(\gbm{Q}_1, \gbm{Q}_2, v) = \left\{
        \begin{array}{cl}
            1 &\ \text{if}\ \operatorname{top-k}(\gbm{Q}_1, k_1, v) \cap \operatorname{top-k}(\gbm{Q}_2, k_2, v) \neq \emptyset \\
            0 &\ \text{otherwise}
        \end{array}
        \right.
    \end{aligned}
\end{equation}
Where, as stated before, $\operatorname{top-k}(\gbm{Q}, k, v)$ represents the set of $k$ vocabulary tokens corresponding to the $k$ indices with highest probability obtained from indexing $Q$ with $v$.
In addition, when comparing $\gbm{Q}_1 = \gbm{Q}_{FOM}$ with the identity matrix we have that $\gbm{Q}_2 = \gbm{I}_V$ and $k_2 = 1$.

On the other hand, we also expand on the prediction-based comparisons by introducing two specific set metrics to provide a weighted perspective on the distributions of transition matrices.
We choose to employ the overlap coefficient (which is equal to the Dice-S\o{}rensen index when considering sets with equal cardinality) and the Jaccard index to evaluate the predictions obtained from pairs of models averaged over the target vocabulary.
We refer \todo{to them} as overlap metrics and we formalize them in the following way:
\begin{equation}
    \label{eq:method_fom_overlap}
    \begin{aligned}
        \operatorname{overlap}_k(\gbm{Q}_1, \gbm{Q}_2) &= \frac{1}{V}\sum_{v \in \mathcal{V}}{\frac{|\operatorname{top-k}(\gbm{Q}_1, k, v) \cap \operatorname{top-k}(\gbm{Q}_2, k, v)|}{\operatornamewithlimits{min}\bigl\{|\operatorname{top-k}(\gbm{Q}_1, k, v)|, |\operatorname{top-k}(\gbm{Q}_2, k, v)|\bigr\}}} \\
        \operatorname{J}_k(\gbm{Q}_1, \gbm{Q}_2) &= \frac{1}{V}\sum_{v \in \mathcal{V}}{\frac{|\operatorname{top-k}(\gbm{Q}_1, k, v) \cap \operatorname{top-k}(\gbm{Q}_2, k, v)|}{|\operatorname{top-k}(\gbm{Q}_1, k, v) \cup \operatorname{top-k}(\gbm{Q}_2, k, v)|}} \\
    \end{aligned}
\end{equation}

\subsection{Probabilistic Comparison}\label{ssec:method_fom_prob}

In order to obtain a wider perspective on the probability distributions included inside \todo{the} transition matrices, we evaluate our models through the lenses of metrics that take into account the entirety of the information included in said probability distributions.
These metrics can be utilized, either directly or indirectly, to measure the similarity or dissimilarity between two probability distributions.

\todo{The first chosen metric} is the perplexity, which quantifies how well a certain discrete probability distribution is able to predict a sample; that is, the likelihood of the sample being drawn from the distribution itself.
In other words, it is commonly referred to as the `surprise' of the distribution in observing a certain sample: high values mean that the sample is unlikely to align with the distribution (greater surprise), whereas low values imply that the sample is likely to be drawn from the distribution (low surprise).
\todo{Given} the \todo{generic} formulation for perplexity for a probability distribution $p$ over a sequence of words $w_1,\dots,w_N$ defined as:
\begin{equation}
    \label{eq:method_fom_perplexity-base}
    PP(p) = \exp{\Bigl(-\frac{1}{N}\sum_{i=1}^{N}{\log{p(w_i)}}\Bigr)}
\end{equation}
In practice, we compute our version of perplexity considering the transition matrix $\gbm{Q}$ as:
\begin{equation}
    \label{eq:method_fom_perplexity}
    PP(\gbm{Q}, S) = e^{-\frac{1}{T}\sum_{t=1}^{T}{\ln{\left((\gbm{Q})_{S(t+1),S(t)}\right)}}}
\end{equation}
Where $S$ \todo{indicates} a sentence composed of a total of $T$ tokens identified as $S(1),\dots,S(T)$ and $(\gbm{Q})_{i,j}$ represents the indexing operation done on the transition matrix $\gbm{Q}$, thus obtaining the transition probability from state $i$ to state $j$.

Interestingly, we highlight the fact that concept of perplexity is strictly related to \todo{that of} cross-entropy.
Cross-entropy is generally computed between two probability distributions and measures the information quantity needed to represent a sample as if it was drawn from a model distribution rather than the true one.
\todo{In fact}, by simply looking at~\cref{eq:method_fom_perplexity-base} we can clearly observe that it directly \todo{includes} the formulation for an estimation of cross-entropy $H$ over a distribution $p$, \todo{that being}:
\begin{equation*}
    H(p) = - \frac{1}{N}\sum_{i=1}^{N}{\log{p(w_i)}}
\end{equation*}
One crucial fact to \todo{keep in mind} is that the logarithm's base doesn't affect the \todo{ground truth} of the result, only its unit of measure.
In most NLP scenarios, cross-entropy is measured in \emph{bits}, thus $2$ is used as a base for the logarithm.
However, when computing the perplexity measure, the base choice must be consistent between logarithm and exponentiation \todo{such as in the case} of~\cref{eq:method_fom_perplexity}, where we utilize $e$ for both \todo{operations}.

On the other hand, we also consider the Kullback-Leibler divergence (KL divergence) as a way to \todo{gauge at} the direct dissimilarity between the probability distributions encoded inside transition matrices.
By definition, the KL divergence cannot be classified as a metric due to its inherent \todo{asymmetricity} since it is possible to identify a reference distribution and a model distribution.
The KL divergence effectively measures the divergence of the model distribution $Q$ from the reference distribution $P$ over a common domain $X$, and it is \todo{generally} expressed as:
\begin{equation*}
    D_{KL}(P||Q) = \sum_{x \in X}{P(x)\ln{\frac{P(x)}{Q(x)}}}
\end{equation*}
In \todo{our case}, we compute the mean KL divergence over all tokens present in the vocabulary using the following formula:
\begin{equation}
    \label{eq:method_fom_kldiv}
    \begin{gathered}
    {\overline {D_{KL}}}(\gbm{Q}_{ref} || \gbm{Q}_{model}) = \frac{1}{V}\sum_{v \in \mathcal{V}}{D_{KL}^v(\gbm{Q}_{ref} || \gbm{Q}_{model})} \\
    \text{where}\ D_{KL}^v(\gbm{Q}_{ref} || \gbm{Q}_{model}) = \sum_{w \in \mathcal{V}}{\bigl((\gbm{Q}_{ref})_{v,w}\bigr)\ln{\frac{(\gbm{Q}_{ref})_{v,w}}{(\gbm{Q}_{model})_{v,\cdot}}}}
    \end{gathered}
\end{equation}
Where, as mentioned before for~\cref{eq:method_fom_perplexity}, $(\gbm{Q})_{i,j}$ represents the indexing operation on the transition matrix $\gbm{Q}$.
