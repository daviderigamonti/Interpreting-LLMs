\todo[gray]{V: Consider "Aim" (thesis goals) / "Scope" (field review) / "Outline" (chapter list with short descriptions) structure for this chapter}

With the current escalation in popularity and pervasiveness of machine learning solutions applied to the realm of natural languages, there exists an increasingly relevant topic that is still being the subject of extensive studies by experts in this field: interpretabilty.
Interpretabilty in itself is far from being a novel concept, as from the early advent of machine learning there has always been the necessity of understanding the inner process by which an opaque model performs its inner computations, possibly even going as far as giving an algorithmic interpretation to it. 

With the introduction of deep learning, the need for interpretabilty spiked as models got increasingly more complex.
The entire paradigm of deep learning is centered around the removal of human control over the feature space, letting models figure out `what needs to be learned' to solve the problem at hand.
In particular, we are interested in a specific class of machine learning models that are currently considered state-of-the art in the Natural Language Processing (NLP): Large Language Models (or LLMs).

This works intends to provide novel perspectives over the internal structure of Large Language Models, this goal is achieved by performing exploratory analyses, through the creation of tools and conducting of experiments aimed at confirming and possibly improving existing hypotheses and observations in the interpretability field.
Particular focus will be placed on the observation and interpretation of the models' hidden states, which are their internal representations, conveying condensed information between various internal components and acting as the `internal language' of the Language Model.

In this introductory section we will analyze the background and context of both NLP and machine learning techniques that will be relevant for this work.
This serves the purpose of providing a common knowledge base as a starting point, briefly covering all the pertinent subjects.

\section{Machine Learning}

Machine Learning is commonly considered a subfield of Artificial Intelligence, and is characterized by the idea of training a machine to learn from past experience, without providing an explicit algorithm to execute.
The term `past experience' is often used loosely in this scenario: it is not necessarily tied with information directly perceived by an agent within a specific environment.
Instead, it commonly denotes the entirety of accumulated data, often referred to as the dataset, which may have been collected heterogeneously and asynchronously, and that is fed to the model in order to learn.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{background_ml-para.pdf}
    \caption{\todo[red]{placeholder image: Machine Learning Paradigms.}}
    \label{fig:background_ml-paradigm}
\end{figure}

There exist three main paradigms of learning for machine learning:
\begin{itemize}
    \item \textbf{Supervised learning}: in supervised learning, the data that we feed to the model is labeled, meaning that each data point is associated to a specific label representing the ground truth.
The model receives a sample from the dataset as input, eliciting an output that fulfills the role of the model's prediction.
This prediction is then compared against the label corresponding to the input data point.
If there is a discrepancy between the two, the model's internals are algorithmically adjusted to minimize future suboptimal predictions.
Most machine learning techniques belong to this category, and it also will be our main focus for this work.
    \item \textbf{Unsupervised learning}: unsupervised learning detaches the labeled component from the dataset, and removes direct feedback from the model's training process.
In this scenario we let the model autonomously find patterns inside data.
This process usually implies some kind of summarization or condensation of information, however the common factor is given by the search of hidden structure inside data.
Clustering techniques are a fairly common and well-known instance of unsupervised learning. 
    \item \textbf{Reinforcement learning}: reinforcement learning mostly concerns the decision-making process of an agent placed inside a dynamic environment.
The inputs of a reinforcement learning algorithm are represented by the environment state and rewards, while the agent's actions are treated as output.
In this paradigm we are trying to discover the optimal policy to navigate the environment, balancing both the exploration of new states, and the exploitation of the current maximally rewarding actions.
\end{itemize}

\subsection{Neural Networks}

Neural networks are one of the most popular machine learning techniques.
They are heavily inspired by the structure of the human brain, which is generally considered to be constituted by multiple neurons connected by synapses.
Classical neural networks usually have a layer-based architecture, where a layer consists of an array of neurons connected to neurons belonging to both the previous and next layers.
A neural network may have any number of intermediate layers, an input layer (which collects information at its inputs) and an output layer (which aggregates the outputs of its neurons into the final network prediction).

\begin{figure}[t!]
    \centering
    \subfloat[Neural Network Architecture.\label{fig:background_nn-arch}]{
        \includegraphics[width=0.8\textwidth]{background_nn-arch.pdf}
    }
    \quad
    \subfloat[Neuron.\label{fig:background_neuron}]{
        \includegraphics[width=0.4\textwidth]{background_neuron.pdf}
    }
    \caption{\todo[red]{placeholder image: Neural Network Architecture + Neuron.}}
    \label{fig:background_nn-arch_neuron}
\end{figure}

Neurons are generally mono-directional and perform a linear combination of their inputs, each one scaled by the weight associated to its synapse.
Before transmitting their output, they also apply an activation function which in most cases is a `S-shaped' nonlinear function.
This is mainly chosen in order to prevent the network from just degenerating into a single linear transformation due to the linearity properties.
Thus, nonlinearities directly affect the expressive power of neural networks and are a vital component to guarantee their success, even on complex problems that require elaborate decision boundaries.

The central process used by neural networks to learn new information is backpropagation.
It works by following the inverse flow of information and updating the network's parameters (represented by the synapses' weights) according to the gradient value accumulated inside each neuron.
The gradient is said to be `accumulated inside neurons' for computational reasons, however it is always computed starting from the loss function at the output of the network and with respect to the parameters of the network.

\subsection{Deep Learning}

Deep Learning is a further specialization of machine learning.
The `deep' adjective refers to the high number of hidden layers that compose the intermediate section of deep neural networks.

The most important property obtained through the dimensional scale-up is the hierarchical feature extraction capability, which consists in obtaining rich condensed representations that follow a semantic hierarchy from the latent spaces defined by the network's layer projections.

\begin{figure}[t!]
    \centering
    \subfloat[Classic Machine Learning.\label{fig:background_fex}]{
        \raisebox{2.65em}{\includegraphics[width=0.46\textwidth]{background_fex.pdf}}
    }
    \quad
    \subfloat[Deep Learning.\label{fig:background_deep-fex}]{
        \includegraphics[width=0.46\textwidth]{background_deep-fex.pdf}
    }
    \caption{\todo[red]{placeholder image: Deep Learning Hierarchical Feature Extraction.}}
    \label{fig:background_fex_deep-fex}
\end{figure}

This is not a property that is exclusive to deep architectures, even classic neural networks present the same hierarchical feature patterns.
However, the sheer scale of deep neural networks makes the exploitation of these condensed representations a viable option to build an artificial feature base that has many upsides with respect to human-defined features.
For instance, we may require expert knowledge in order to craft an appropriate dataset containing meaningful features, while this would be done automatically in a deep learning scenario.

The main drawback of this approach comes from the fact that learned features are not directly human-understandable, as they result from mathematical optimization.
Consequently, the process of explaining a model's prediction is not as straightforward as with classical machine learning techniques.

The latent feature space concept and its properties are of particular relevance for this work, as most of the experiments that will be performed are aimed at attempting to understand them in the context of state-of-the-art models in the Natural Language Processing field of study.

Another type of deep architecture relevant for this work is the Recurrent Neural Network (RNN) architecture: one of the first promising approaches to text generation which featured a structure much more similar to classic neural networks, but with the addition of recurrent connections.
Recurrent connections enable RNN models to establish a distributed hidden state that acts as a memory, much like a flip-flop gate array in electronics.
In practice, each RNN module is fed the inputs at the current time instant and its own outputs at the previous time instant, making it suitable to process sequences and other time-based data.

\begin{figure}[t!]
    \centering
    \subfloat[Recurrent Neural Networks Architecture.\label{fig:background_rnn}]{
        \includegraphics[width=0.95\textwidth]{background_rnn.pdf}
    }
    \quad
    \subfloat[LSTMs.\label{fig:background_lstm}]{
        \includegraphics[width=0.9\textwidth]{background_lstm.pdf}
    }
    \caption{\todo[red]{placeholder image: Recurrent Neural Networks Architectures + LSTMs.}}
    \label{fig:background_rnn_lstm}
\end{figure}

The most popular architecture for text generation before the current state-of-the-art models was the Long Short-Term Memory (LSTM).
LSTMs are a direct improvement on the RNN, solving the vanishing gradient problem, which was the main failure point of classic RNNs.
The \todo{overwhelming} vanishing gradient in RNNs is a direct consequence of the co-occurrence between activation functions without a prominent zero-valued region and the Backpropagation Through Time process (being the backpropagation equivalent of a network using recurrent connections), resulting in drastically small gradient values which can limit the model's context window and make the training process slow and unreliable.
LSTMs solve this by applying specialized gating functions that let the model decide what information to remember, forget and pass along into what is called the Constant Error Carousel.

\section{Natural Language Processing}

Natural Language Processing (NLP) is a subfield of computer science that stems off the branches of artificial intelligence, information engineering and computational linguistics.
Its main purpose is to interface computers to languages that are commonly used between humans for the purpose of communication.
This entails a great variety of many different subtasks, from text classification and machine translation to text generation itself.
The main focus of most of the state-of-the art models is text generation, even though it is possible to repurpose them for many other tasks due to their modular nature.

\subsection{Evolution of NLP}

Before the modern artificial intelligence era, the NLP field was mainly focused on the study of rule-based systems which had their roots in formal logic.
Consequently, the main sought after tasks were related to translation and parsing, which however, never resulted in any revolutionary breakthroughs.
During the first AI winter that took place \todo{during} the 70s and 80s, the focus of NLP shifted towards statistical and data-driven approaches, opening the NLP field to major influences from other branches of mathematics, especially probability and statistics.

Exactly in these years the use of Markov Models began to rise in popularity, despite being theorized decades prior \todo[orange]{cite shannon}.
\todo[green]{explain markov models}

\todo[green]{explain document-oriented representation approaches, tying into embeddings}


\subsection{Word Embeddings}

One of the most relevant and groundbreaking developments in the field of NLP was the introduction of word embeddings.
Word embeddings came as a straightforward application of deep learning concepts and approaches to the world of NLP.
Their introduction not only determined an improvement in most text-based applications, but also motivated a shift in text representation techniques, incentivizing a word-centric approach rather than focusing on documents and defining words as frequencies inside them.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{background_embeddings_placeholder.pdf}
    \caption{\todo[red]{placeholder image: Word Embedding Space Visualization.}}
    \label{fig:background_embeddings}
\end{figure}

The main appeal of word embeddings is the dimensionality reduction effect, since the original vocabulary representation (often referred as Bag of Words) suffers from the curse of dimensionality to a great extent.
The `curse of dimensionality' is a common way to refer to feature spaces that present numerous dimensions in conjunction with high data sparsity.
In a feature space affected by the curse of dimensionality, as the vocabulary space is, we can observe the fact that some common similarity measures and distances between data points (such as the Euclidean distance) almost lose meaning due to the majority of dimensions suppressing comparisons along the few dimensions that actually matter.
Word embeddings are not able to completely solve the curse of dimensionality as they still are relatively high-dimensional.
However, the improvement of adopting a dense feature space, with the choice of a normalized similarity function (such as cosine similarity) was still substantial enough to revolutionize the NLP field.

In fact, another enticing property is their ability to capture the semantic depth and context information inside the representation of each word.
Word embeddings are generally created by extrapolating the intermediate latent representations of a neural network solving a certain task, obtaining a continuous vector space for words to reside.
In practice, word embeddings are mappings from the vocabulary space to the embedding space which features a reduced number of dense dimensions devoid of any human-understandable meaning, characteristic of deep learning methodologies.

The concept of using distributed word representations was anticipated by~\citet{bengio2000} with the Neural Network Language Model (NNLM) in the early 2000s.
The first real successful implementation of word embeddings was Word2Vec by~\citet{mikolov2013}, greatly inspired by Bengio's previous work.
The main purpose of Word2Vec is to create word embeddings, it is composed by a single hidden layer and is trained discriminatively by using both positive and negative examples.

\blockquote[J.R. Firth]{You Shall Know a Word by the Company It Keeps}

\begin{figure}[t!]
    \centering
    \subfloat[CBoW.\label{fig:background_word2vec-cbow}]{
        \includegraphics[width=\textwidth]{background_word2vec-cbow.pdf}
    }
    \quad
    \subfloat[Skip-gram.\label{fig:background_word2vec-skipg}]{
        \includegraphics[width=0.9\textwidth]{background_word2vec-skipg.pdf}
    }
    \caption{\todo[red]{placeholder image: Skip-gram and CBoW Word2Vec.}}
    \label{fig:background_word2vec-cbow_word2vec-skipg}
\end{figure}

Mikolov's original work introduces two primary training variants for Word2Vec:
\begin{itemize}
    \item \textbf{Continuous Bag of Words (CBoW)}: predict a target word given its context.
The context is given as a symmetric window containing all terms occurring around the target term, modeling a many-to-1 prediction.
    \item \textbf{Skip-gram}: predicts surrounding words given a target word.
Single context words are predicted one at a time, modeling a 1-to-1 prediction.
\end{itemize}
The results are generally comparable between the two techniques.
The key aspect is that the full context of each word is integrated into the word representation itself.

\subsubsection*{Word Embeddings Semantic Properties}

One particular property that emerges from the majority of latent representations is their flexibility to arithmetic manipulations.
It is not uncommon to observe these properties showcased in many other deep learning applications, such as computer vision.
However, in the word embeddings scenario, these arithmetic properties have been reported to be much more prominent and \todo{reflecting} of the actual word semantics.

In particular, the semantic meaningfulness of certain arithmetic and geometric transformations applied to word embeddings was noticed by Mikolov himself in the Word2Vec paper~\cite{mikolov2013}.
This went from finding similarities in embeddings of words that manifested syntactic regularities, to complete double-word analogies located in common semantic fields.
One of the most famous examples presented in~\cite{mikolov2013} is: \texttt{emb(`King') - emb(`Man') + emb(`Woman') $\simeq$ emb(`Queen')}, which models both the relationships of `gender' and `royalty'.
Other \todo{relevant} analogies include country-capital, family relation, and verb tenses.
However, any kind of semantic relationship could be potentially modeled by word embeddings, given enough support in the training dataset.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{background_emb-arithmetic_placeholder.pdf}
    \caption{\todo[red]{placeholder image: Embedding Arithmetic https://developers.google.com/machine-learning/crash-course/embeddings/embedding-space?hl=it.}}
    \label{fig:background_emb-arithmetic}
\end{figure}

While these properties have some direct use cases principally involving clustering and similarity comparisons, their study has been proven to be insightful from the explainability standpoint.
Possibly understanding the semantic associations that exist \todo{at the base} of text models can provide an educated perspective on how they interpret their inputs, but also underline human and language biases.
Additionally, this topic is of particular relevance due to the focus on interpretability of this work.

\section{Transformer Architectures}

Transformers are the current state-of-the art model architecture for most text applications and not only.
Although they were initially conceived for textual sequential inputs, in recent years saw discrete success in a wide variety of other applications (in particular image processing), albeit occasionally with some slight variations.
This architecture has proven to be flexible, efficient and parallelizable, in contrast with previous popular choices for sequence learning problems such as RNNs and LSTMs.

The first transformer architecture was introduced by~\citet{vaswani2017} in the seminal 2017 paper ``Attention Is All You Need''.
This architecture is of the encoder-decoder type, following the typical paradigm of sequence-to-sequence (seq2seq) learning.
This structure implies the existence of two separate modules (an encoder and a decoder), which work in tandem to elaborate inputs and provide an output.
Both input and output consist of sequential data.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{background_trans-enc-dec_placeholder.pdf}
    \caption{\todo[red]{placeholder image: Attention is All You Need Transformer Architecture https://arxiv.org/abs/1706.03762.}}
    \label{fig:background_trans-enc-dec}
\end{figure}

The purpose of the encoder module is to process the inputs and create an intermediate representation that is read and processed by the decoder, which outputs the next token.
The encoder and the decoder have a fairly similar structure, both featuring multiple basic blocks that can be sequentially connected to each other (operation commonly referred to as `stacking').

\subsection{Structure}\label{ssec:background_transf_structure}

Having analyzed the overarching structure of a transformer model, we are now going to explore each module in depth.
To this end, we are going to subdivide the architecture in three main parts:
\begin{itemize}
    \item Starting with the \textbf{preprocessing step}, which embodies the preliminary encoding operations performed at the start and a single time throughout each execution.
    \item Then, the \textbf{module architecture} that concerns the internals of single basic transformer modules, being replicated multiple times in the transformer stack.
    \item Finally, the \textbf{postprocessing step}, which covers the step that conclude the generation process of the transformer architecture.
\end{itemize}

All of the mentioned parts are vital for the \todo{correct functioning} of the transformer, albeit slight modifications will be introduced throughout the years in order to optimize the performance of the basic architecture shown in the current section.
These additional improvements will see \todo{further discussion} in a dedicated section (\Cref{ssec:background_transf_current}).

\subsubsection*{Preprocessing}

The first step to process inputs is tokenization, as it is fundamental to reduce the possibly infinite vocabulary to a finite set of tokens that can be embedded and understood by the model.
This is normally done by a tokenizer, which performs some preprocessing on the input sentence to obtain a sequence of tokens that will be fed to the model.
Most transformer architectures use different variants of sub-word tokenization algorithms, meaning that each word may correspond to a combination of multiple tokens.
This type of tokenization is usually obtained as the result of a training process where common character sequences get progressively aggregated until they reach a certain frequency threshold, at this point a compact and efficient representation of the vocabulary that is balanced between granularity and size is achieved.
Sub-word tokenization improves the ability of the model to deal with rare and unknown words, greatly limiting the use of unknown tokens (special tokens used in place of certain words that cannot be correctly tokenized and encoded) and potentially facilitating the creation of multilingual models.

The tokenization and embedding steps are perhaps the most critical processes due to the fact that they shape both inputs and outputs of the transformer, framing how it interfaces itself with the environment.
Additionally, the transformer architecture is word position agnostic, meaning that embedded tokens do not directly contain any additional information that \todo{tells} the model their position inside the input sentence.
This is commonly solved by implementing \textbf{positional encoding}: an additive mask that gets applied to the input tokens right after the embedding step.
The original ``Attention Is All You Need''~\cite{vaswani2017} paper suggests the use of fixed positional encodings based on the \emph{sine} and \emph{cosine} functions, although alternatives exist.

\subsubsection*{Basic Block Structure}

Transformer blocks \todo{compose} large part of the transformer architecture and contain \textbf{attention blocks}, \textbf{feed-forward blocks}, \textbf{residual connections} and \textbf{normalization blocks}.

\textbf{Attention blocks} are \todo{powered} by the attention mechanism, the \todo{principal mechanic} responsible for the outstanding results achieved by transformer models.
The concept of attention is not novel and did already see some applications in seq2seq models~\cite{bahdanau2015,kim2017,cho2015}, determining substantial improvements in most LSTM architectures.
Attention in encoder-decoder models is implemented by allowing the decoder to \todo{look} at the internal states generated by the encoder, thus eliminating the information bottleneck represented by the single point of connection between the two.
The most important part of attention is its scoring function, which allows the decoder to `focus' on certain tokens by modeling a weight distribution on the attended hidden states using an attention function.
There exist different attention function implementations such as \emph{simple dot product}, \emph{Luong}~\cite{luong2015} (multiplicative) and \emph{Bahdanau}~\cite{bahdanau2015} (additive).
The original transformer architecture mentioned in ``Attention Is All You Need''~\cite{vaswani2017} uses \emph{scaled dot product attention}, which is implemented with the following formula:
\begin{equation}
    \label{eq:background_attention}
    Attention(Q,K,V) = softmax\left(\frac{QK^\T}{\sqrt{d_K}}\right)V
\end{equation}
Where $Q$, $K$ and $V$ represent the \emph{query}, \emph{key} and \emph{value} components of attention which are obtained by multiplying the hidden state vectors to their respective projection matrices $W_Q$, $W_K$ and $W_V$, which are composed of learnable model parameters.

\begin{figure}[!htp]
    \centering
    \subfloat[Encoder-Decoder.\label{fig:background_enc-dec-attention}]{
        \includegraphics[width=\textwidth]{background_enc-dec-attention.pdf}
    }
    \quad
    \subfloat[Self.\label{fig:background_self-attention}]{
        \includegraphics[width=\textwidth]{background_self-attention.pdf}
    }
    \caption{\todo[red]{placeholder image: Attention Blocks (Encoder-Decoder / Self).}}
    \label{fig:background_enc-dec-attention_self-attention}
\end{figure}

The transformer architecture implements three different types of attention:
\begin{itemize}
    \item An \textbf{encoder-decoder attention} (or \textbf{cross-attention}), which is found inside the decoder block, and attends to the encoder blocks' hidden states using its current state as the \emph{query}.
    \item The \textbf{self-attention}, which is used by both the encoder block to process the hidden states at its input.
It works in following the same principles as cross-attention however, it attends to representations of the same nature, meaning that \emph{query}, \emph{key} and \emph{value} are extrapolated by the same set of hidden states.
In practice, the model is able to manage long-range dependencies and possibly capture patterns or associations between representations in a completely parallelizable way.
    \item Lastly, \textbf{masked self-attention} is a specific implementation of self-attention that can be found in the decoder block, due to its need of autoregressive modeling.
In fact, the decoder is not allowed to use information about future words, since it is capable of returning a single output token per stack execution.
Masked self-attention differs from self-attention in scenarios where the full output is available (e.g.\ training), and it works by adding a mask with the effect of nullifying the contribution of future tokens in the attention computation.
The masked self-attention formula can be summarized as follows:
\begin{equation}
    \label{eq:background_masked-attention}
    \begin{gathered}
        MaskedAttention(Q,K,V) = softmax\left(\frac{QK^\T + M}{\sqrt{d_K}}\right)V \\
        M_{ij} = \begin{cases}
            0 & \text{if}\ i \ge j, \\
            -\infty & \text{otherwise}.
        \end{cases}
    \end{gathered}
\end{equation}
\todo[cyan]{check M definition equation}
\end{itemize}

In reality, transformers use a more refined attention implementation, called \textbf{multi-head attention} which can be applied in all the previously identified scenarios.
Multi-head attention is a novelty introduced in the ``Attention Is All You Need''~\cite{vaswani2017} paper, and it consists in replicating the attention structure multiple times, thus obtaining multiple representation spaces for each attention layer.
This operation influences the dimension of attention vectors, which needs to be divided by $h$, where $h$ is the chosen number of separate attention heads for multi-head attention.
The attention output for multi-head attention is obtained as the concatenation between the attention outputs of all $h$ heads as follows:
\begin{equation}
    \label{eq:background_multihead-attention}
    \begin{aligned}
    MultiHead(Q,K,V)    &= head_1\ \oplus\ \ldots\ \oplus\ head_h \\
                        &= Attention_1(Q,K,V)\ \oplus\ \ldots\ \oplus\ Attention_h(Q,K,V)
    \end{aligned}
\end{equation}
In practice, multi-head attention enables the model to simultaneously focus on information present at different positions, avoiding excessive attention on single tokens.

\todo[green]{attention dimensional analysis}

The second main \todo{block} contained in the transformer block is the \textbf{feed-forward block}.
As the name suggests, it consists of a simple feed-forward network and is normally situated as the last step of the transformer block, using the outputs of the previous attention layers as inputs.
The intermediate representations are processed by the feed-forward layer flow in a completely independent manner, therefore the execution of this block is parallelizable by nature.
Feed-forward blocks' principal contribution to the transformer architecture consists in the introduction of nonlinearities, since attention is still a predominantly linear process.
The feed-forward network acts as a `grouping' mechanism, where information gathered in previous steps through attention is aggregated, processed and reformulated, adding depth to the computation.

\textbf{Layer normalization} and \textbf{residual connections} are techniques that are commonly found in most deep learning models and are generally used to improve gradient flow, training stability and overall generalization.
In particular:
\begin{itemize}
    \item \textbf{Layer normalization}~\cite{ba2016} is applied after each attention and feed-forward block.
It consists of two steps: a batch-independent normalization step performed with parameters ($\mu$, $\sigma$) which model a normal distribution over the feature space of the vector representations, and a linear transformation using a bias and a scale factor.
The bias and scale factor are usually referred to as $\beta$ and $\gamma$ respectively, and are trainable layer-level parameters which perform a shift and rescale operation on the normalized vector.
If $v_i$ is a vector representation, after layer normalization at layer $\ell$ we obtain:
\begin{subequations}
    \begin{gather}
        \bar v_{i,\ell} = \gamma_\ell \cdot \left( \frac{v_i - \mu}{\sigma} \right) + \beta_\ell, \label{eq:background_layernorm} \\
        \text{where}\ \mu = \frac{1}{n}\sum_{i=1}^{n}{v_i},\ \sigma = \sqrt{\frac{1}{n}\sum_{i=1}^{n}{{(v_i - \mu)}^2}} \label{eq:background_layernorm_extra}
    \end{gather}
\end{subequations}
\todo[cyan]{Check equation pedices}
    \item \textbf{Residual connections}~\cite{he2016} are a strategy employed in deep architectures to streamline gradient propagation and avoid vanishing gradients.
Additionally, it helps the transformer to preserve local information as attention and feed-forward blocks are added to the base residual flow, limiting their freedom in the effective output space.
In practice, residual connections sum the identity function of the input of a block to the output of that block, in the transformer's case the residual summation happens before layer normalization.
\end{itemize}

\subsubsection*{Postprocessing}

Of particular importance is the last step in the transformer generative process.
After the inputs have been embedded and have been passed through all the decoder's transformer blocks, iteratively refining them into new vector representations, the decoder has to output a new token.
Given the fact that a transformer model generates a single token at a time, we would presumably obtain a single hidden representation at the end of the decoder stack with a size compatible to the embedding space.
However, in order to output a vocabulary token we need the vector to lay in the vocabulary space.
To achieve this goal, the transformer architecture features a last linear layer outside of the stack with the purpose of bringing the last hidden representation from the embedding space to the vocabulary space, modeling an `unembedding' operation.
In some specific model architectures, this last weight matrix is forced to be equal to the transpose of the initial embedding matrix for the sake of retaining a consistent representation between the first and last layers; other architectures opt to learn a different embedding representation space for the outputs of the model.
\todo[green]{this is known as "weight tying"...}
After the `unembedding' operation, the resulting vector of floating point numbers (commonly referred to as logits) will be laying in the vocabulary space.
However, in order to get an actual probability distribution over the vocabulary for the next token of the input sequence, a last softmax application is needed.

\subsection{Current Decoder Architectures}\label{ssec:background_transf_current}

What was previously described is generally considered the first transformer architecture that was originally mentioned in the ``Attention Is All You Need''~\cite{vaswani2017} paper in 2017.
However, with time, a large number of variations and improvements on the base architecture has struck the NLP field.
We are going to be mainly concerned with the Llama 2~\cite{touvron2023} and Llama 3~\cite{dubey2024} (from \todo{here} just Llama) models, developed by \href{https://ai.meta.com/}{Meta AI} in 2023 and 2024 respectively; other minor models may also be taken into consideration, although without particular focus on their architecture.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{background_llama-arch_placeholder.pdf}
    \caption{\todo[red]{placeholder image: Decoder-only Architecture Llama https://github.com/hkproj/pytorch-llama-notes.}}
    \label{fig:background_llama-arch}
\end{figure}

The first main novelty in the new state-of-the-art models is the complete removal of the encoder from the architecture.
These models are often called \textbf{`decoder-only'}, \textbf{`autoregressive'}, \textbf{`causal'} \todo[green]{differentiate between causal/autoregressive?}or \textbf{`GPT-like'} from the Generative Pre-trained Transformer (GPT) architecture~\cite{radford2018,radford2019,brown2020,openai2023} that popularized the use of transformer models without the decoder, although the first documented use of a `decoder-only' architecture can be ascribed to this 2018 paper by~\citet{liu2018}.
The major improvement of autoregressive architectures consists in the massive reduction in parameters, which enables more flexibility during training, besides the upsides tied to scalability and efficiency.
In addition, removing the encoder also negates possible information redundancy between the two components, empirically improving the optimization procedure~\citet{liu2018}.
The decoder remains mostly unchanged from this modification, with the exception of the cross-attention block since it can no longer fetch the representations generated by the encoder.
Cross-attention is consequently removed, \todo{leaving} a single masked self-attention block \todo{inside} the decoder architecture.

Another important change in the transformer architecture is the \textbf{Rotary Position Embeddings (RoPE)} mechanism, proposed in 2021 by~\citet{su2024}.
This technique completely replaces the positional encoding strategy that was previously employed in transformer models (absolute positional encoding) by guaranteeing better flexibility for long sequences and introducing the decay of inter-token dependency with increasing relative token distance.
One of the main flaws of absolute positional encoding is its inability to model dependence between tokens, as each token positional encoding is independent of the others and, in particular, from the distance between tokens.
RoPE is able to model both absolute and relative token positional information by encoding them as a geometric rotation inside the embedding space.
This rotation operation, which has a multiplicative nature (opposed to the additive one of absolute positional encoding), has the additional advantages of avoiding any change in vector norm for the subjected hidden representation.
This property is vital in determining RoPE's capability of being directly applied inside the query and key matrices in linear self-attention, improving overall efficiency.

The original transformer implementation establishes the presence of layer normalization right after each attention and feed-forward blocks, after residual addition.
Extensive research has been performed on possible alternatives and a notable shift from models implementing classic normalization (post-norm) to models implementing pre-normalization emerged.
The \textbf{pre-normalization (pre-norm)}~\cite{baevski2019,xiong2020} approach consists in applying layer normalization directly inside the residual block, right before the attention or feed-forward blocks.
Additionally, a further layer normalization operation is performed before prediction, right after the last transformer block.
The main benefits of this technique are tied to training efficiency and convergence speed~\cite{xiong2020} as it was shown to improve overall gradient stability, even at initialization time.
This improvement allowed the removal of the `learning rate warm-up stage': a technique that was commonly used in post-norm transformer architectures to avoid diverging in training~\cite{popel2018} and involved a gradual increase of the learning rate in the first training epochs.

Another variation on the transformer normalization mechanism was achieved with the introduction of \textbf{root mean square layer normalization (RMSNorm)}~\cite{zhang2019}.
The fundamental improvement introduced by RMSNorm is a substantial reduction in the amount of computation performed with respect to classical layer normalization and an overall gain in efficiency, this is especially true for deep architectures where the computational overhead of layer normalization is meaningful.
Implementation wise, RMSNorm only focuses on the rescaling aspect rather than including both centering and rescaling as layer normalization does.
The central quantity used to perform the rescaling computation is the Root Mean Square (RMS) statistic and only the $\gamma$ parameter is retained from the set of learnable parameters due to the fact that the re-centering operation performed by $\beta$ was deemed to be mostly irrelevant for both layer normalization and RMSNorm.
If $v_i$ is a vector representation, after RMSNorm at layer $\ell$ we obtain:
\begin{equation}
    \label{eq:background_rmsnorm}
    \begin{aligned}
        \bar v_{i,\ell} = \gamma_\ell \cdot \frac{v_i}{RMS(v)}, &&
        \text{where}\ RMS(v) = \sqrt{\frac{1}{n}\sum_{i=1}^{n}{v_i^2}}
    \end{aligned}
\end{equation}
One peculiar side effect of RMSNorm is the fact it forces the summed inputs into a $\sqrt{n}$-scaled unit sphere which benefits the stability of layer activations and output distribution, positively influencing the interpretability of the hidden space.

The last main change featured in most recent autoregressive transformer architectures concerns the structure of the feed-forward block.
The base transformer implementation establishes the feed-forward layer as the combination of two linear transformations separated by a non-linear activation function, typically a choice between \emph{ReLU} and \emph{GELU}.
However, newer architectures have begun to incorporate Gated Linear Units (GLUs)~\cite{dauphin2017} implemented using a \emph{Swish} (also called \emph{SiLU}) activation function, this particular architectural combination for the feed-forward block is commonly referred to as \textbf{SwiGLU}~\cite{shazeer2020}.
\emph{Swish}~\cite{shazeer2020} is an activation function similar to \emph{ReLU}, characterized by a non-monotonic depression in its zero-region, and is implemented in the following way: $Swish(x) = x \cdot \sigma(x)$ where $\sigma(x)$ represents the logistic sigmoid function.
\emph{Swish} was shown to offer marginal improvements over \emph{ReLU} and other similar variations, based on its smoothness and possibility of returning small negative values for inputs close to zero, implying the efficient convergence of non-zero gradients and consequently minimizing the problem of dead neurons (caused by the nullification of gradient).

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\textwidth]{background_ffnn-glu.pdf}
    \caption{\todo[red]{placeholder image: GLU Feedforward Architecture.}}
    \label{fig:background_ffnn-glu}
\end{figure}

On the other hand, GLUs determine an entire revision of the feed-forward architecture, as they introduce a gating mechanism where one linear transformation is modulated by the output of another, which allows the model to dynamically control the flow of information.
In the case of Llama models~\cite{touvron2023,dubey2024}, the gating mechanism happens through an element-wise multiplication where $W_{in} \in \mathbb{R}^{d_{model} \times d_{hid}}$ represents the input transformation matrix with its respective bias $b_{in}$, $V \in \mathbb{R}^{d_{model} \times d_{hid}}$ is the gate transformation matrix with its respective bias $b_{V}$, $W_{out} \in \mathbb{R}^{d_{hid} \times d_{model}}$ performs the out-projection to the model's hidden representation dimensionality with its respective bias $b_{out}$ and $f$ represents the activation/gating function (in the SwiGLU case $f(x) = Swish(x)$):
\begin{equation}
    \label{eq:background_ffnn}
    FFN(x) = GLU(x) \cdot W_{out} + b_{out} 
    = \Bigl( f(xV + b_V) \odot xW_{in} + b_{in} \Bigr) \cdot W_{out} + b_{out}
\end{equation}
\todo[cyan]{check dimensionality}
For Llama models~\cite{touvron2023,dubey2024}, all biases of the transformation matrices referring to the SwiGLU implementation are set to zero ($b_{in} = b_V = b_{out} = \vec 0$).
The benefits of SwiGLU are largely tied to providing a better downstream performance on fine-tuning tasks and pre-training objectives, despite proof of SwiGLU actually having an impactful effect on transformer models being purely empirical.
However, the reasons for its effectiveness may potentially be associated with its capability to model complex functions through the gating mechanism~\cite{shazeer2020,shibuya2023}.

\subsection{The Large Language Model Paradigm}

With the advent of Large Language Models (LLMs), a complete shift in training paradigms was observed throughout the NLP field since, due to their massive scale, the need for applying ad-hoc Deep Learning training techniques \todo{arose}.
In particular, we can point out two main approaches commonly using for streamlining, improving and optimizing the training process: \textbf{pre-training} and \textbf{fine-tuning}.

\textbf{pre-training} is often performed in an unsupervised or \emph{`self-supervised'} manner, meaning that either the training dataset is used without labels or labels can be directly inferred from it.
Pre-training fulfills the function of providing the model with general language patterns, without specific orientation or direction.
For this reason, large unbiased corpuses of text are needed to perform a successful pre-training, often resulting in lengthy and expensive training runs.
There are two principal methods to perform pre-training: \emph{Masked Language Modeling (MLM)} and \emph{Next Sentence Prediction (NSP)}.
\emph{MLM} is popular between encoder architectures such as the BERT family, and consists of trying to infer certain `masked' tokens given the surrounding ones by leveraging the provided context.
Whereas, \emph{NSP} sees more use in the pre-training of GPT-like and decoder-based models by giving the model a pair of sentences and asking it to determine if the second sentence can logically follow the first one.

On the other hand, \textbf{fine-tuning} is usually faster and performed after pre-training, having the goal of specializing the model to carry out a specific task.
By freezing part of the model architecture (usually the first layers), fine-tuning can be performed on a subset of the model's parameters resulting in a faster and less disruptive training, as one of the main concerns correlated with fine-tuning is the model's possibility of forgetting what was learned during pre-training.