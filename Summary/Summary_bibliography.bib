@inproceedings{drozd2016,
  author    = {Aleksandr Drozd and
               Anna Gladkova and
               Satoshi Matsuoka},
  editor    = {Nicoletta Calzolari and
               Yuji Matsumoto and
               Rashmi Prasad},
  title     = {Word Embeddings, Analogies, and Machine Learning: Beyond king - man
               + woman = queen},
  booktitle = {{COLING} 2016, 26th International Conference on Computational Linguistics,
               Proceedings of the Conference: Technical Papers, December 11-16, 2016,
               Osaka, Japan},
  pages     = {3519--3530},
  publisher = {{ACL}},
  year      = {2016},
  url       = {https://aclanthology.org/C16-1332/},
  timestamp = {Tue, 27 Dec 2022 18:26:56 +0100},
  biburl    = {https://dblp.org/rec/conf/coling/DrozdGM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{elhage2021,
  title   = {A Mathematical Framework for Transformer Circuits},
  author  = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year    = {2021},
  journal = {Transformer Circuits Thread},
  note    = {\url{https://transformer-circuits.pub/2021/framework/index.html}},
}

@article{elhage2022,
  author     = {Nelson Elhage and
                Tristan Hume and
                Catherine Olsson and
                Nicholas Schiefer and
                Tom Henighan and
                Shauna Kravec and
                Zac Hatfield{-}Dodds and
                Robert Lasenby and
                Dawn Drain and
                Carol Chen and
                Roger Grosse and
                Sam McCandlish and
                Jared Kaplan and
                Dario Amodei and
                Martin Wattenberg and
                Christopher Olah},
  title      = {Toy Models of Superposition},
  journal    = {CoRR},
  volume     = {abs/2209.10652},
  year       = {2022},
  url        = {https://doi.org/10.48550/arXiv.2209.10652},
  doi        = {10.48550/ARXIV.2209.10652},
  eprinttype = {arXiv},
  eprint     = {2209.10652},
  timestamp  = {Wed, 28 Sep 2022 15:17:28 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2209-10652.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
}

@inproceedings{mikolov2013,
  author    = {Tom{\'{a}}s Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}

@online{nostalgebraist2020,
  author    = {nostalgebraist},
  title     = {interpreting GPT: the logit lens},
  publisher = {AI Alignment Forum},
  year      = {2020},
  url       = {https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
  urldate   = {2024-07-17},
}

@online{olah2022,
  author    = {Chris Olah},
  title     = {Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases},
  publisher = {Transformer Circuits Thread},
  year      = {2022},
  url       = {https://transformer-circuits.pub/2022/mech-interp-essay},
  urldate   = {2024-07-17},
}

@article{park2023,
  author     = {Kiho Park and
                Yo Joong Choe and
                Victor Veitch},
  title      = {The Linear Representation Hypothesis and the Geometry of Large Language
                Models},
  journal    = {CoRR},
  volume     = {abs/2311.03658},
  year       = {2023},
  url        = {https://doi.org/10.48550/arXiv.2311.03658},
  doi        = {10.48550/ARXIV.2311.03658},
  eprinttype = {arXiv},
  eprint     = {2311.03658},
  timestamp  = {Tue, 14 Nov 2023 14:47:55 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2311-03658.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
}

@article{tufanov2024,
  author     = {Igor Tufanov and
                Karen Hambardzumyan and
                Javier Ferrando and
                Elena Voita},
  title      = {{LM} Transparency Tool: Interactive Tool for Analyzing Transformer
                Language Models},
  journal    = {CoRR},
  volume     = {abs/2404.07004},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2404.07004},
  doi        = {10.48550/ARXIV.2404.07004},
  eprinttype = {arXiv},
  eprint     = {2404.07004},
  timestamp  = {Thu, 16 May 2024 13:49:33 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2404-07004.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
}
