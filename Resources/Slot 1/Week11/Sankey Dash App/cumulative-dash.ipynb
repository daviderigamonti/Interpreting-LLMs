{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, AutoConfig, StoppingCriteriaList, StoppingCriteria\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from experiment_utils import load_model\n",
    "from itertools import cycle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import dataclasses\n",
    "import torch\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # \"mistralai/Mistral-7B-v0.1\" \n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daviderigamonti/Thesis/nlp-thesis/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/daviderigamonti/Thesis/nlp-thesis/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97f91aec3cb4645b8e34a7d4de16a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_default_device(device)\n",
    "model, tokenizer, device, model_config = load_model(model_id=model_id, quantization=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.eos_token_id and not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### https://github.com/oobabooga/text-generation-webui/blob/2cf711f35ec8453d8af818be631cb60447e759e2/modules/callbacks.py#L12\n",
    "class _SentinelTokenStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, sentinel_token_ids: list, starting_idx: int):\n",
    "        StoppingCriteria.__init__(self)\n",
    "        self.sentinel_token_ids = sentinel_token_ids\n",
    "        self.starting_idx = starting_idx\n",
    "        self.shortest = min([x.shape[-1] for x in sentinel_token_ids])\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, _scores: torch.FloatTensor) -> bool:\n",
    "        for sample in input_ids:\n",
    "            trimmed_sample = sample[self.starting_idx:]\n",
    "            trimmed_len = trimmed_sample.shape[-1]\n",
    "            if trimmed_len < self.shortest:\n",
    "                continue\n",
    "\n",
    "            for sentinel in self.sentinel_token_ids:\n",
    "                sentinel_len = sentinel.shape[-1]\n",
    "                if trimmed_len < sentinel_len:\n",
    "                    continue\n",
    "\n",
    "                window = trimmed_sample[-sentinel_len:]\n",
    "                if torch.all(torch.eq(sentinel, window)):\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "####\n",
    "\n",
    "def generate_stopping_criteria(stopgen_tokens, input_len=0):\n",
    "    return StoppingCriteriaList([\n",
    "        _SentinelTokenStoppingCriteria(\n",
    "            sentinel_token_ids = stopgen_tokens,\n",
    "            starting_idx=input_len\n",
    "        )\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_id in [\"microsoft/phi-1_5\"]:\n",
    "    stopgen_tokens = [\n",
    "        torch.tensor([198, 198]),  # \\n\\n\n",
    "        torch.tensor([628])        # \\n\\n\n",
    "    ]\n",
    "    prompt_structure = \"Question: {prompt}\\n\\nAnswer:\"\n",
    "    exclude_token_offset = 3\n",
    "    fix_characters = [(\"Ġ\", \"␣\"), (\"Ċ\", \"\\n\")]\n",
    "elif model_id in [\"meta-llama/Llama-2-7b-hf\", \"mistralai/Mistral-7B-v0.1\"]:\n",
    "    stopgen_tokens = [\n",
    "        torch.tensor([1]),  # <s>\n",
    "        torch.tensor([2])   # </s>\n",
    "    ]\n",
    "    prompt_structure = \"{prompt}\"\n",
    "    exclude_token_offset = None\n",
    "    fix_characters = [(\"<0x0A>\", \"\\n\")]\n",
    "\n",
    "fix_characters += [(\"\\n\", \"\\\\n\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_masked_attentions(attentions, max_len):\n",
    "    \"\"\"\n",
    "    Attention in generative models are masked, we want to plot a heatmap so we must pad all attentions to the same size with 0.0 values\n",
    "    \"\"\"\n",
    "    array_attentions = [np.array(att.float()) for att in attentions] # TODO: optimize\n",
    "    new_attentions = [np.concatenate([att, np.zeros([max_len - len(att)])]) for att in array_attentions]\n",
    "    return np.array(new_attentions)\n",
    "\n",
    "def compute_complete_padded_attentions(generated_output, layer, head):\n",
    "    single_layer_attentions = []\n",
    "    # Prompt tokens\n",
    "    for single_layer_single_head in torch.squeeze(torch.select(generated_output.attentions[0][layer], 1, head)):\n",
    "        single_layer_attentions.append(single_layer_single_head)\n",
    "    # Response tokens\n",
    "    for attentions_per_token in generated_output.attentions[1:]:\n",
    "        # Take single layer\n",
    "        single_layer = attentions_per_token[layer]\n",
    "        # Take only one head\n",
    "        single_layer_single_head = torch.select(single_layer, 1, head)\n",
    "        single_layer_attentions.append(single_layer_single_head)\n",
    "    # Squeeze dimensions to one a one-dimensional tensor\n",
    "    pure_attentions = [s.squeeze() for s in single_layer_attentions]\n",
    "    max_seq_len  = len(pure_attentions[-1])\n",
    "    # Print last attention heatmap\n",
    "    padded_attentions = pad_masked_attentions(pure_attentions, max_seq_len)\n",
    "    return padded_attentions\n",
    "\n",
    "def compute_batch_complete_padded_attentions(generated_output, heads):\n",
    "    multi_layer_head_attentions = []\n",
    "    for head in heads:\n",
    "        multi_layer_attentions = []\n",
    "        for layer in range(0, len(generated_output.attentions[0])):\n",
    "            # Prompt tokens\n",
    "            prompt_att = [\n",
    "                torch.squeeze(single_head)\n",
    "                for single_head in torch.squeeze(torch.select(generated_output.attentions[0][layer], 1, head))\n",
    "            ]\n",
    "            # Response tokens\n",
    "            response_att = [\n",
    "                torch.squeeze(torch.select(single_layer[layer], 1, head))\n",
    "                for single_layer in generated_output.attentions[1:]\n",
    "            ]\n",
    "            # Pad and merge attentions\n",
    "            multi_layer_attentions.append(pad_masked_attentions( \n",
    "                [att_token for att_token in prompt_att + response_att],\n",
    "                len(response_att[-1])\n",
    "            ))\n",
    "        multi_layer_head_attentions.append(multi_layer_attentions)\n",
    "    return multi_layer_head_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ids_from_embedding(token_emb, weights, bias, tot_layers=-1, layer_n=-1):\n",
    "    # Interpolated embeddings\n",
    "    if type(weights) == dict:\n",
    "        logits = {k:torch.matmul(token_emb, weight) + bias for k, weight in weights.items()}\n",
    "        logits = ((tot_layers - layer_n) * (logits[\"input\"]) + layer_n * (logits[\"output\"])) / tot_layers\n",
    "    # Single embeddings\n",
    "    else:\n",
    "        logits = torch.matmul(token_emb, weights) + bias\n",
    "    return torch.argmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multirep(model, hidden_states, weights, bias, reverse_weights, max_rep=5, tot_layers=-1, layer_n=-1):\n",
    "    pred_ids = []\n",
    "    #pred_norms = []\n",
    "    for n, hs in enumerate(hidden_states):\n",
    "        tokens = []\n",
    "        norms = []\n",
    "        token_emb = hs.squeeze()\n",
    "        for i in range(0, max_rep):\n",
    "            # Compute token and embedding norm\n",
    "            token_id = compute_ids_from_embedding(token_emb, weights, bias, tot_layers=tot_layers, layer_n=n)\n",
    "            norm = torch.norm(token_emb) \n",
    "            # Stop prematurely if norm is too small or if norm is bigger than previous one\n",
    "            if norm <= 0.01 or (len(norms) > 0 and norm >= norms[-1]):\n",
    "                break\n",
    "            # Do not add repreated tokens\n",
    "            if token_id not in tokens:\n",
    "                tokens.append(token_id)\n",
    "            norms.append(norm)\n",
    "            # Compute next embedding by subtracting the closest embedding to the current embedding\n",
    "            closest_emb = reverse_weights[token_id]\n",
    "            token_emb = token_emb - closest_emb\n",
    "        pred_ids.append(tokens)\n",
    "        #pred_norms.append(norms)\n",
    "    return pred_ids#, pred_norms\n",
    "\n",
    "def test_multirep(model, input, embedding, token=1):\n",
    "    if embedding == 'output':\n",
    "        weights = model.lm_head.weight.T\n",
    "    elif embedding == 'input':\n",
    "        weights = model.model.embed_tokens.weight.T\n",
    "\n",
    "    bias = model.lm_head.bias\n",
    "    if bias:\n",
    "        reverse_weights = torch.add(weights.T, bias.unsqueeze(dim=1))\n",
    "    else:\n",
    "        bias = 0\n",
    "        reverse_weights = weights.T \n",
    "    inputs = tokenizer(\"Hi, how are you\", return_tensors=\"pt\")\n",
    "    gen_config = GenerationConfig(\n",
    "        pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else None,\n",
    "        output_attentions=True, output_hidden_states=True, return_dict_in_generate=True\n",
    "    )\n",
    "    gen_output = model.generate(inputs.input_ids, generation_config=gen_config, max_new_tokens=5)\n",
    "    print(tokenizer.decode(gen_output.sequences.squeeze()))\n",
    "    a,aa = compute_multirep(model, gen_output.hidden_states[1], weights, bias, reverse_weights)\n",
    "    return [[(tokenizer.decode(c), cc.detach().numpy().tolist()) for c,cc in zip(b,bb)] for b,bb in zip(a,aa)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From hidden states to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_lm_head(hidden_states, weights, bias, tot_layers=-1):\n",
    "    \"\"\"\n",
    "    Function which takes as input the hidden states of the model and returns the prediction of the next token.\n",
    "    Uses the language modeling head of input\n",
    "    \"\"\"\n",
    "    pred_ids = []\n",
    "    for n, token_layer in enumerate(hidden_states):\n",
    "        token_id = compute_ids_from_embedding(token_layer, weights, bias, tot_layers=tot_layers, layer_n=n)\n",
    "        pred_ids.append(token_id)\n",
    "    return pred_ids\n",
    "    \n",
    "def embed_hidden_states(model, hidden_states, embedding=\"output\", include_prompt=False, include_end=True, multirep=True, max_rep=10):\n",
    "    end_idx = len(hidden_states) if include_end else len(hidden_states) - 1\n",
    "    tot_layers = model.config.num_hidden_layers\n",
    "\n",
    "    if embedding == 'output':\n",
    "        weights = model.lm_head.weight.T\n",
    "        reverse_weights = model.lm_head.weight\n",
    "    elif embedding == 'input':\n",
    "        weights = model.model.embed_tokens.weight.T\n",
    "        reverse_weights = model.model.embed_tokens.weight\n",
    "    elif embedding == 'interpolate':\n",
    "        weights = {\"input\": model.model.embed_tokens.weight.T, \"output\": model.lm_head.weight.T}\n",
    "        reverse_weights = {\"input\": model.model.embed_tokens.weight, \"output\": model.lm_head.weight}\n",
    "    else:\n",
    "        raise ValueError(\"Embedding not valid\")\n",
    "\n",
    "    bias = 0\n",
    "    if model.lm_head.bias:\n",
    "        raise ValueError(\"Bias not supported\") \n",
    "\n",
    "    predictions = []\n",
    "    # Prompt tokens\n",
    "    if include_prompt:\n",
    "        for token_states in torch.stack(hidden_states[0]).swapaxes(0, 2):\n",
    "            if multirep:\n",
    "                pred_ids = compute_multirep(model, token_states.swapaxes(0, 1), weights, bias, reverse_weights, max_rep=max_rep, tot_layers=tot_layers)\n",
    "            else:\n",
    "                pred_ids = [_apply_lm_head(token_states.swapaxes(0, 1), weights, bias, tot_layers=tot_layers)]\n",
    "            predictions.append([[int(id) for id in idd] for idd in pred_ids])\n",
    "    # Response tokens\n",
    "    for token_states in hidden_states[1:end_idx]:\n",
    "        if multirep:\n",
    "            pred_ids = compute_multirep(model, token_states, weights, bias, reverse_weights, max_rep=max_rep, tot_layers=tot_layers)\n",
    "        else:\n",
    "            pred_ids = [_apply_lm_head(token_states, weights, bias, tot_layers=tot_layers)]\n",
    "        predictions.append([[int(id) for id in idd] for idd in pred_ids])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataframe_characters(df, replacements, multirep=False, columns=False):\n",
    "    for old, new in replacements:\n",
    "        df = df.map(lambda x: [i.replace(old, new) for i in x] if multirep else x.replace(old, new))\n",
    "    if columns:\n",
    "        for old, new in replacements:\n",
    "            df.columns = df.columns.str.replace(old, new)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_debug_info(raw_debug_vector, n_layers):\n",
    "    new_vector = None\n",
    "    layer_vector = None\n",
    "    for iter_tokens in raw_debug_vector:\n",
    "        i, iter_tokens = iter_tokens\n",
    "        n_layer = i % n_layers\n",
    "        layer_vector = torch.cat([layer_vector, iter_tokens], dim=0) if layer_vector != None else iter_tokens\n",
    "        if n_layer == n_layers - 1:\n",
    "            new_vector = torch.cat([new_vector, layer_vector], dim=1) if new_vector != None else layer_vector\n",
    "            layer_vector = None\n",
    "    new_vector = new_vector.permute([1, 0, 2])\n",
    "    return new_vector\n",
    "\n",
    "def extrapolate_debug_vectors(model):\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    debug_vectors = {\n",
    "        \"input_residual_embedding\": extrapolate_debug_info(model.model.input_residual_embedding, n_layers),\n",
    "        \"attention_plus_residual_embedding\": extrapolate_debug_info(model.model.attention_plus_residual_embedding, n_layers),\n",
    "        \"post_attention_embedding\": extrapolate_debug_info(model.model.post_attention_embedding, n_layers),\n",
    "        \"post_FF_embedding\": extrapolate_debug_info(model.model.post_FF_embedding, n_layers),\n",
    "    }\n",
    "    return debug_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate(model, tokenizer, prompt, max_extra_length, config, min_stop_length, stopping_tokens):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = len(inputs.input_ids.squeeze().tolist())\n",
    "    max_len = input_len + max_extra_length\n",
    "    \n",
    "    gen_config = config\n",
    "    stopping_criteria = generate_stopping_criteria(stopping_tokens, input_len + min_stop_length)\n",
    "    \n",
    "    generated_output = model.generate(inputs.input_ids, generation_config=gen_config, max_length=max_len, stopping_criteria=stopping_criteria)\n",
    "    outputs = generated_output.sequences.squeeze()\n",
    "    text_output = tokenizer.decode(generated_output.sequences.squeeze()[input_len:])\n",
    "    \n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(generated_output.sequences[0])\n",
    "    input_tokens = all_tokens[0:input_len]\n",
    "    generated_tokens = all_tokens[input_len:]\n",
    "    \n",
    "    return text_output, generated_output, {\"in\": input_tokens, \"gen\": generated_tokens}\n",
    "\n",
    "def create_hidden_states_df(model, tokenizer, generated_output, gen_tokens, embedding, include_prompt, fix_characters, multirep=False):\n",
    "    predictions = embed_hidden_states(model, generated_output.hidden_states, embedding, include_prompt=include_prompt, multirep=multirep, max_rep=5)\n",
    "    rows = [[tokenizer.convert_ids_to_tokens(pred) for pred in pred_list] for pred_list in predictions]\n",
    "    rows = rows if multirep else np.squeeze(rows)\n",
    "    if embedding == \"input\":\n",
    "        cols = gen_tokens[\"in\"] + gen_tokens[\"gen\"][:-1]\n",
    "    else:\n",
    "        cols = gen_tokens[\"in\"][1:] + gen_tokens[\"gen\"]\n",
    "    df = pd.DataFrame(rows).T.sort_index(ascending=False).rename(columns={n: col for n, col in enumerate(cols)})\n",
    "    df = fix_dataframe_characters(df, fix_characters, multirep=multirep, columns=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sankey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SankeyParameters:\n",
    "    # DATA\n",
    "    row_index: int = 0 # Row of starting token (where 0 corresponds to the top row, and n_layers - 1 corresponds to the bottom row)\n",
    "    token_index: int = 9 # Position index of starting token (where 0 is first token of the input sequence)\n",
    "    rowlimit: int = 5 # Limit number of layers to visualize\n",
    "    multirep: bool = False # Accomodate for each token having multiple labels\n",
    "    show_0: bool = False\n",
    "    # COLORS\n",
    "    colormap: list[str, ...] = field( default_factory = lambda : [\"#FF6692\"] ) # Colors -- colormap = cycle(px.colors.qualitative.Plotly)\n",
    "    #colormap = cycle(px.colors.qualitative.Plotly)\n",
    "    color_change_count_threshold: int = 3 # Number of virtual rows that should have the same color associated to them\n",
    "    color_brightness_range: tuple[float, float] = (-0.5, 0.2) # Brightness range for tokens color gradient\n",
    "    node_opacity: float = 0.7 # Opacity of nodes\n",
    "    link_opacity: float = 0.4 # Opacity of links\n",
    "    non_residual_link_color: tuple[int, int, int] = (100, 100, 100) # Default color for non-resiudal links\n",
    "    default_node_color: tuple[int, int, int] = (220, 220, 220) # Default color for nodes\n",
    "    color_nodes: bool = False # If set to true, color nodes based on the colormap, otherwise all nodes will have their default color\n",
    "    extra_brightness_map: dict[str, float] = field( default_factory = lambda : {\"Node\": -0.5, \"FFNN\": 0.15, \"Attention\": -0.15, \"Intermediate\": -0.3} )\n",
    "    # LAYOUT\n",
    "    print_indexes: bool = False\n",
    "    rescale_factor: int = 3\n",
    "    fixed_offsets: dict[str, float] = field( default_factory = lambda : {\"Node\": 0, \"FFNN\": 0.02, \"Attention\": 0.02, \"Intermediate\": 0} )\n",
    "    column_pad: float = 0.05\n",
    "    sankey_zero: float = 0.000000000000001 # Correction to avoid feeding nodes with a coordinate value of 0, which causes problems with Plotly Sankey Diagrams\n",
    "    size: int = 1200 # Size of square canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_sankey_traces(\n",
    "    df, linkinfo,             # Dataframe and link info to access labels and node hidden information\n",
    "    row, indexes, el_indexes, # Dataframe is indexed by index and row, while el_index references the index for sankey visualization\n",
    "    bases,                    # Base attention value of parents\n",
    "    labels,                   # Current set of labels for sankey visualization\n",
    "    elmap,                    # Reference for duplicate nodes as a dictionary indexed with (row, index) and containing a dictionary composed of\n",
    "                              #  an id and a base\n",
    "    rowlimit,                 # Depth limit\n",
    "):\n",
    "    new_labels = []\n",
    "    new_indexes = []\n",
    "    new_elmap = elmap.copy() # TODO: copy necessary?\n",
    "\n",
    "    under = []\n",
    "    over = []\n",
    "    val = []\n",
    "    types = []\n",
    "    # Calculate current value of node by weighting its attention value for the parent's weight\n",
    "    for index, el_index, base in zip(indexes, el_indexes, bases):\n",
    "        res_w = linkinfo[\"residuals\"][index][-(row + 1)].item()\n",
    "        attn_w = linkinfo[\"attn_states\"][index][-(row + 1)].item()\n",
    "        mlp_w = linkinfo[\"ffnn_states\"][index][-(row + 1)].item()\n",
    "        # Create MLP / Attention / Intermediate nodes\n",
    "        mlp_index = len(new_elmap.keys())\n",
    "        new_labels.append([\"FFNN\"])\n",
    "        new_elmap[(round(row + 1 - 0.8, 2), round(index - 0.5, 2))] = {\"id\": mlp_index, \"base\": base * mlp_w, \"base_pow\": pow(base * mlp_w, 0.5), \"type\": \"FFNN\"}\n",
    "        attn_index = len(new_elmap.keys())\n",
    "        new_labels.append([\"Attention\"])\n",
    "        new_elmap[(round(row + 1 - 0.45, 2), round(index - 0.5, 2))] = {\"id\": attn_index, \"base\": base * attn_w, \"base_pow\": pow(base * attn_w, 0.5), \"type\": \"Attention\"}\n",
    "        hid_index = len(new_elmap.keys())\n",
    "        new_labels.append([\"-\"])\n",
    "        new_elmap[(round(row + 1 - 0.65, 2), index)] = {\"id\": hid_index, \"base\": base, \"base_pow\": pow(base, 0.5), \"type\": \"Intermediate\"}\n",
    "        # Iterate over all elements of the next row\n",
    "        for i, label in enumerate(df.iloc[row+1].tolist()):\n",
    "            v = base * attn_w * linkinfo[\"attentions\"][row][index][i].item()\n",
    "            if v > 0:\n",
    "                over.append(attn_index)\n",
    "                # If node is already present store its information\n",
    "                if (row+1, i) in new_elmap:\n",
    "                    under.append(new_elmap[(row+1, i)][\"id\"])\n",
    "                    new_elmap[(row+1, i)][\"base\"] += v\n",
    "                    new_elmap[(row+1, i)][\"base_pow\"] += pow(v, 0.5)\n",
    "                # If the node is new create a new entry in the element map with a new sankey index \n",
    "                else:\n",
    "                    new_index = len(new_elmap.keys())\n",
    "                    new_labels.append(label)\n",
    "                    new_indexes.append(i)\n",
    "                    under.append(new_index)\n",
    "                    new_elmap[(row+1, i)] = {\"id\": new_index, \"base\": v, \"base_pow\": pow(v, 0.5), \"type\": \"Node\"}\n",
    "                val.append(v)\n",
    "                types.append(\"attention\")\n",
    "        # MLP State\n",
    "        over.append(el_index)\n",
    "        under.append(mlp_index)\n",
    "        val.append(base * mlp_w)\n",
    "        types.append(\"mlp\")\n",
    "        over.append(mlp_index)\n",
    "        under.append(hid_index)\n",
    "        val.append(base * mlp_w)\n",
    "        types.append(\"mlp\")\n",
    "        # Attention State\n",
    "        over.append(hid_index)\n",
    "        under.append(attn_index)\n",
    "        val.append(base * attn_w)\n",
    "        types.append(\"att\")\n",
    "        # Residuals\n",
    "        over.append(hid_index)\n",
    "        under.append(new_elmap[(row+1, index)][\"id\"])\n",
    "        val.append(base * (res_w + mlp_w))\n",
    "        types.append(\"residual\")\n",
    "        new_elmap[(row+1, index)][\"base\"] += base * (res_w + mlp_w)\n",
    "        new_elmap[(row+1, index)][\"base_pow\"] = pow(base * (res_w + mlp_w), 0.5)\n",
    "        over.append(el_index)\n",
    "        under.append(hid_index)\n",
    "        val.append(base * (attn_w + res_w))\n",
    "        types.append(\"residual\")\n",
    "        \n",
    "    # If depth limit is reached, stop recurring\n",
    "    if row < rowlimit:\n",
    "        # Call itself on all the new nodes\n",
    "        nex_under, nex_over, nex_val, nex_types, nex_labels, new_elmap = cumulative_sankey_traces(\n",
    "            df, linkinfo,\n",
    "            row+1, new_indexes, [new_elmap[(row+1, i)][\"id\"] for i in new_indexes],\n",
    "            [new_elmap[(row+1, i)][\"base\"] for i in new_indexes],\n",
    "            new_labels,\n",
    "            new_elmap,\n",
    "            rowlimit\n",
    "        )\n",
    "        # Update elements map, sankey trace lists and sankey labels list with children's results\n",
    "        new_labels += nex_labels\n",
    "        under += nex_under\n",
    "        over += nex_over\n",
    "        val += nex_val\n",
    "        types += nex_types\n",
    "    # Only executed at topmost level\n",
    "    if len(el_indexes) == 1 and el_indexes[0] == 0:\n",
    "        # Complete sankey labels list with starting label\n",
    "        new_labels = labels + new_labels\n",
    "    return under, over, val, types, new_labels, new_elmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescales values of a list inside a given range, if invert is set to True, the range is flipped\n",
    "def rescale_list(l, range_min=0, range_max=1, old_min=None, old_max=None, invert=False):\n",
    "    if old_max == None:\n",
    "        old_max = max(l)\n",
    "    if old_min == None:\n",
    "        old_min = min(l)\n",
    "    old_range = old_max - old_min\n",
    "    new_range = range_max - range_min\n",
    "\n",
    "    invert_k = 0\n",
    "    invert_a = 1\n",
    "    if invert:\n",
    "        invert_k = old_max\n",
    "        invert_a = -1\n",
    "\n",
    "    return [ range_min + (((invert_k + (invert_a * (el - old_min))) * new_range ) / old_range) for el in l ]\n",
    "\n",
    "# Given a list and a list of indexes that have been previously sorted, restore the original order of the list\n",
    "def restore_list_order(l, indexes):\n",
    "    return [l[indexes.index(i)] for i in range(0, len(indexes))]\n",
    "\n",
    "# Return a list of RGBA color strings given a list of RGBA colors tuples\n",
    "def build_rgba_from_tuples(l, opacity=1.0):\n",
    "    return [f\"rgba{tuple(el) + (opacity,)}\" if len(el) == 3 else f\"rgba{el}\" for el in l]\n",
    "\n",
    "def change_color_brightness(rgb_color, brightness):\n",
    "    delta_color = tuple([int((channel) * brightness) for channel in rgb_color])\n",
    "    return tuple([sum(channel) for channel in zip(rgb_color, delta_color)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sankey_linkinfo(generated_output, debug_vectors, att_head):\n",
    "    # TODO: fix range\n",
    "    attentions = compute_batch_complete_padded_attentions(generated_output, range(0, 32))[att_head] #TODO: fix range\n",
    "    res_contrib = debug_vectors[\"input_residual_embedding\"]\n",
    "    attn_contrib = debug_vectors[\"post_attention_embedding\"]\n",
    "    ffnn_contrib = debug_vectors[\"post_FF_embedding\"]\n",
    "    res_percent = []\n",
    "    attn_percent = []\n",
    "    ffnn_percent = []\n",
    "    for res_token, att_token, ffnn_token in zip(res_contrib, attn_contrib, ffnn_contrib):\n",
    "        res_tokenlayer_list = []\n",
    "        att_tokenlayer_list = []\n",
    "        ffnn_tokenlayer_list = []\n",
    "        for res_tokenlayer, att_tokenlayer, ffnn_tokenlayer in zip(res_token, att_token, ffnn_token):\n",
    "            #den = res_tokenlayer.norm() + att_tokenlayer.norm() + ffnn_tokenlayer.norm()\n",
    "            res_tokenlayer_list.append(res_tokenlayer.norm() / (res_tokenlayer.norm() + att_tokenlayer.norm()))\n",
    "            att_tokenlayer_list.append(att_tokenlayer.norm() / (res_tokenlayer.norm() + att_tokenlayer.norm()))\n",
    "            ffnn_tokenlayer_list.append(ffnn_tokenlayer.norm() / ((res_tokenlayer + att_tokenlayer).norm() + ffnn_tokenlayer.norm()))\n",
    "        res_percent.append(res_tokenlayer_list)\n",
    "        attn_percent.append(att_tokenlayer_list)\n",
    "        ffnn_percent.append(ffnn_tokenlayer_list)\n",
    "    linkinfo = {\"attentions\": attentions, \"residuals\": res_percent, \"attn_states\": attn_percent, \"ffnn_states\": ffnn_percent} # Aggregated intermediate weights information\n",
    "    return linkinfo\n",
    "\n",
    "def generate_sankey(df, linkinfo, sankey_parameters: SankeyParameters):\n",
    "    row_index = sankey_parameters.row_index\n",
    "    token_index = sankey_parameters.token_index\n",
    "    token_label = df.iloc[row_index].iloc[token_index]\n",
    "    # Generate diagram data\n",
    "    under, over, values, types, labels, elmap = cumulative_sankey_traces(\n",
    "        df, linkinfo, \n",
    "        row_index, [token_index], [0], \n",
    "        [1.0], \n",
    "        [token_label], \n",
    "        {(row_index, token_index): {\"id\": 0, \"base\": 1.0, \"base_pow\": 1, \"type\": \"Node\"}},\n",
    "        sankey_parameters.rowlimit\n",
    "    )\n",
    "    return (under, over, values, types, labels, elmap)\n",
    "\n",
    "def format_sankey(un, ov, vl, types, lab, elmap, sankey_parameters: SankeyParameters):\n",
    "    # Handle multiple labels for tokens with multiple representations\n",
    "    nodes_extra = []\n",
    "    if sankey_parameters.multirep:\n",
    "        nodes_extra = [{\"text\": l} for l in lab]\n",
    "        lab = [l[0] for l in lab]\n",
    "    else:\n",
    "        lab = [np.squeeze(l).item() for l in lab]\n",
    "        nodes_extra = [{\"text\": l} for l in lab]\n",
    "\n",
    "    # Generate numbered labels\n",
    "    lab = [f\"{k[1]} {lab[v['id']]}\" if sankey_parameters.print_indexes and el[\"type\"] in [\"Node\"] else lab[el['id']] for k,el in elmap.items()]\n",
    "\n",
    "    # Add non-rescaled info to links and nodes extra information\n",
    "    for k, el in elmap.items():\n",
    "        nodes_extra[el[\"id\"]] = nodes_extra[el[\"id\"]] | {\"v\": el[\"base\"]}\n",
    "    links_extra = [{\"v\": v, \"type\": t} for v, t in zip(vl, types)]\n",
    "\n",
    "    # Rescale node and link values by a rescale factor to fit into graph\n",
    "    rescale_factor = sankey_parameters.rescale_factor\n",
    "    rescaled_elmap = {k: el | {\"base\": el[\"base\"] / rescale_factor } for k,el in elmap.items()}\n",
    "    rescaled_vl = [el / rescale_factor for el in vl]\n",
    "\n",
    "    # Create reverse mapping obtaining lists indexed by the node id and containing virtual coordinates and node values\n",
    "    revmap = [next(k for k,v in rescaled_elmap.items() if v[\"id\"] == i) for i in range(len(rescaled_elmap.keys()))]\n",
    "    revmap_values = [next(v for k,v in rescaled_elmap.items() if v[\"id\"] == i) for i in range(len(rescaled_elmap.keys()))]\n",
    "    revmap_x = [key[0] for key in revmap]\n",
    "    revmap_y = [key[1] for key in revmap]\n",
    "    # Sort reverse-mapped lists to perform transformations on them with more ease, while keeping an index list to reverse the sorting\n",
    "    revmap_indexes = [i for i in range(0,len(revmap))]\n",
    "    revmap_x_sort, revmap_y_sort, revmap_values_sort, revmap_indexes = zip(*sorted(zip(revmap_x, revmap_y, revmap_values, revmap_indexes), key=lambda x: x[0]))\n",
    "\n",
    "    # Build colors\n",
    "    node_colors = []\n",
    "    node_colors_ref = []\n",
    "    link_colors = []\n",
    "    colormap = cycle(sankey_parameters.colormap)\n",
    "    current_color = next(colormap)\n",
    "    old_x = -1\n",
    "    change_count = sankey_parameters.color_change_count_threshold\n",
    "    color_brightness_range = sankey_parameters.color_brightness_range\n",
    "    # Node colors\n",
    "    for x, y, v in zip(revmap_x_sort, rescale_list(revmap_y_sort, range_min=color_brightness_range[0], range_max=color_brightness_range[1]), revmap_values_sort):\n",
    "        # Color switching\n",
    "        if x != old_x:\n",
    "            if change_count > sankey_parameters.color_change_count_threshold:\n",
    "                current_color = next(colormap)\n",
    "                change_count = 0\n",
    "            change_count += 1\n",
    "        color_ref = change_color_brightness(px.colors.hex_to_rgb(current_color), y)\n",
    "        node_colors_ref.append(color_ref)\n",
    "        actual_color = sankey_parameters.default_node_color\n",
    "        if sankey_parameters.color_nodes:\n",
    "            actual_color = px.colors.hex_to_rgb(current_color)\n",
    "        color = change_color_brightness(actual_color, y + sankey_parameters.extra_brightness_map[v[\"type\"]])\n",
    "        node_colors.append(color)\n",
    "        old_x = x\n",
    "    node_colors = restore_list_order(node_colors, revmap_indexes)\n",
    "    node_colors_ref = restore_list_order(node_colors_ref, revmap_indexes)\n",
    "    # Link colors\n",
    "    link_colors = [node_colors_ref[el] if typ in [\"residual\"] else sankey_parameters.non_residual_link_color for typ, el in zip(types, un)]\n",
    "    # Convert colors and add opacities\n",
    "    node_colors = build_rgba_from_tuples(node_colors, sankey_parameters.node_opacity)\n",
    "    link_colors = build_rgba_from_tuples(link_colors, sankey_parameters.link_opacity)\n",
    "\n",
    "    # Generate columns based on maximum node width for each column to fit nodes into\n",
    "    col_pad = sankey_parameters.column_pad\n",
    "    columns_width = [max([v[\"base\"] if y == y_index else 0 for (y, v) in zip(revmap_y_sort, revmap_values_sort)]) for y_index in range(0, 10)] # TODO use actual range\n",
    "    s = sum(columns_width) + col_pad * len(columns_width)\n",
    "    columns_width = [w/s + col_pad for w in columns_width]\n",
    "    columns_ys = []\n",
    "    tot_w = 0\n",
    "    for w in columns_width:\n",
    "        columns_ys.append(tot_w)\n",
    "        tot_w += w\n",
    "\n",
    "    # Adjust coordinates \n",
    "    revmap_x = rescale_list(revmap_x, range_min=sankey_parameters.sankey_zero, range_max=1, invert=False)\n",
    "    revmap_y = [ columns_ys[math.ceil(y)] + v[\"base\"] / 2 - sankey_parameters.fixed_offsets[v[\"type\"]] for y, v in zip(revmap_y, revmap_values) ]\n",
    "\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        orientation = \"v\",\n",
    "        arrangement=\"fixed\",\n",
    "        valueformat=\".5r\",\n",
    "        node=dict(\n",
    "            customdata=nodes_extra,\n",
    "            hovertemplate='%{customdata.text}<extra>%{customdata.v}</extra>',\n",
    "            align=\"left\",\n",
    "            label=lab,\n",
    "            color=node_colors,\n",
    "            x=revmap_x,\n",
    "            y=revmap_y,\n",
    "            pad=800,\n",
    "        ),\n",
    "        link=dict(\n",
    "            customdata=links_extra,\n",
    "            hovertemplate='%{customdata.type} from %{source.label} to %{target.label} <extra>%{customdata.v}</extra>',\n",
    "            source=ov,\n",
    "            target=un,\n",
    "            value=rescaled_vl,\n",
    "            color=link_colors\n",
    "        )\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        font_size=12, font_family=\"Verdana\", font_color=\"black\",\n",
    "        width=sankey_parameters.size, height=sankey_parameters.size,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"Beauty is in the eye of the\"\n",
    "#gen_config = GenerationConfig(\n",
    "#    pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else None,\n",
    "#    output_attentions=True, output_hidden_states=True, return_dict_in_generate=True\n",
    "#)\n",
    "#text_output, generated_output, gen_tokens = model_generate(\n",
    "#    model, tokenizer, prompt, \n",
    "#    max_extra_length=10, \n",
    "#    config=gen_config, \n",
    "#    min_stop_length=1, stopping_tokens=stopgen_tokens\n",
    "#)   \n",
    "#dfs = {}\n",
    "#for emb in [\"input\", \"output\", \"interpolate\"]:\n",
    "#    dfs[emb] = create_hidden_states_df(\n",
    "#        model, tokenizer, generated_output, gen_tokens, emb, \n",
    "#        include_prompt=True, fix_characters=fix_characters,\n",
    "#        multirep=False,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import diskcache\n",
    "import uuid\n",
    "\n",
    "from dash import dcc, html, ctx, Patch, DiskcacheManager\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "\n",
    "import dash_daq as daq\n",
    "import plotly.colors as pc\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = diskcache.Cache(\"./cache\")\n",
    "background_callback_manager = DiskcacheManager(cache)\n",
    "app = dash.Dash(\"Cumulative Sankey Diagram Demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"red\": [\"#FF6692\"], \"plotly\": px.colors.qualitative.Plotly}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_tab = html.Div([\n",
    "    html.Div([\n",
    "        dcc.Textarea(id='model_input', placeholder ='Insert prompt...', style={'width': '100%', 'height': \"50%\"}),\n",
    "        dcc.Loading(id=\"model_loading\", type=\"dot\", color=\"#873ba1\", children =\n",
    "                dcc.Textarea(id='model_output', readOnly=True, style={'width': '100%', 'height': \"50%\"})\n",
    "            )\n",
    "    ], style={\"float\": \"left\", \"width\": \"70%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "    html.Div([\n",
    "        html.P(children=\"# of attention heads to load\", style={\"margin\": \"0\"}),\n",
    "        dcc.Dropdown([{\"value\": i, \"label\": i+1 if i >= 0 else \"Average\"} for i in range(-1, model_config.num_attention_heads)], id='model_generate_heads', value=-1, clearable=False),\n",
    "        dcc.Input(id=\"min_stop_tokens\", type='number', value=1, min=0, max=1024),\n",
    "        html.Label(\"Min # tokens for stopping criteria\"),\n",
    "        dcc.Input(id=\"max_new_tokens\", type='number', value=10, min=0, max=1024),\n",
    "        html.Label(\"Max # of generated tokens\"),\n",
    "        dcc.Checklist([\"Compute Multiple Token Representations\"], id=\"multirep_tokens\", inline=True),\n",
    "        html.Button('Generate', id='model_generate', style={\"width\": \"100%\", \"height\": \"20px\"}),\n",
    "    ], style={\"float\": \"right\", \"height\": \"100%\", \"width\": \"20%\", \"padding\": 2}),\n",
    "], style={\"height\": \"240px\"})\n",
    "\n",
    "vis_data_tab = html.Div([\n",
    "    html.P(\"Attention head selector\"),\n",
    "    dcc.Slider(id='attention_heads', marks={}, step=1, value=-1, ),\n",
    "    html.Div([\n",
    "        html.P(\"Embeddings selector\"),\n",
    "        dcc.RadioItems([\n",
    "            {\"label\": \"Input\", \"value\": \"input\"},\n",
    "            {\"label\": \"Output\", \"value\": \"output\"},\n",
    "            {\"label\": \"Interpolate\", \"value\": \"interpolate\"}\n",
    "        ], id='embeddings', value='interpolate', inline=True),\n",
    "    ], style={\"float\": \"left\", \"width\": \"50%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "    html.Div([\n",
    "        dcc.Input(id=\"row_limit\", type='number', value=5, min=1, max=31), # TODO: max layers\n",
    "        html.Label(\"# layers to visualize\"),\n",
    "        dcc.Input(id=\"row_index\", type='number', value=31, min=0, max=31), #  TODO: max/default layers\n",
    "        html.Label(\"Index of starting layer\"),\n",
    "        dcc.Input(id=\"token_index\", type='number', value=0, min=0, max=1024), #  TODO: max index\n",
    "        html.Label(\"Index of starting token\"),\n",
    "        dcc.Checklist([\"Show first token\"], id=\"show_0\", value=\"Show first token\", inline=True),\n",
    "    ], style={\"float\": \"right\", \"width\": \"20%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "], style={\"height\": \"240px\"})\n",
    "\n",
    "vis_color_tab = html.Div([\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.P(\"Color mapping\"),\n",
    "            dcc.Dropdown([\n",
    "                {\"value\": \"red\", \"label\": \"Red\"},\n",
    "                {\"value\": \"plotly\", \"label\": \"Plotly Palette\"}\n",
    "            ], id='colormap', value=\"plotly\", clearable=False),\n",
    "            dcc.Input(id=\"node_opacity\", type='number', value=0.7, min=0, max=1, step=0.05), \n",
    "            html.Label(\"Nodes color opacity\"),\n",
    "            dcc.Input(id=\"link_opacity\", type='number', value=0.4, min=0, max=1, step=0.05),\n",
    "            html.Label(\"Links color opacity\"),\n",
    "            dcc.Checklist([{\"label\": \"Color nodes\", \"value\": \"color\"}], id=\"color_nodes\", value=[\"color\"], inline=True),\n",
    "        ], style={\"float\": \"left\", \"width\": \"34%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "        html.Div([\n",
    "            html.P(\"Color brightness range\"),\n",
    "            dcc.RangeSlider(-1.0, 1.0, id='color_brightness_range', value=[-0.5, 0.2], allowCross=False, tooltip={\"placement\": \"top\", \"always_visible\": True}),\n",
    "            dcc.Input(id=\"extra_bright_node\", type='number', value=-0.5, min=-1, max=1, step=0.05), \n",
    "            html.Label(\"Extra brightness for base nodes\"), html.Br(),\n",
    "            dcc.Input(id=\"extra_bright_ffnn\", type='number', value=0.15, min=-1, max=1, step=0.05),\n",
    "            html.Label(\"Extra brightness for FFNN nodes\"), html.Br(),\n",
    "            dcc.Input(id=\"extra_bright_att\", type='number', value=-0.15, min=-1, max=1, step=0.05),\n",
    "            html.Label(\"Extra brightness for attention nodes\"), html.Br(),\n",
    "            dcc.Input(id=\"extra_bright_int\", type='number', value=-0.3, min=-1, max=1, step=0.05),\n",
    "            html.Label(\"Extra brightness for intermediate nodes\"),\n",
    "        ], style={\"float\": \"right\", \"width\": \"65%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "    ], style={\"float\": \"left\", \"width\": \"49%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            daq.ColorPicker(id=\"color_mapping_color\", label=\"Color for color-mapping\", size=160, value={\"hex\":\"#FF6692\"}, labelPosition=\"right\"),\n",
    "        ], style={\"float\": \"right\", \"width\": \"30%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "        html.Div([\n",
    "            daq.ColorPicker(id=\"non_residual_link_color\", label=\"Color for non-residual links\", size=160, value=dict(rgb=dict(r=100, g=100, b=100, a=0)), labelPosition=\"right\"),\n",
    "        ], style={\"float\": \"right\", \"width\": \"30%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "        html.Div([\n",
    "            daq.ColorPicker(id=\"default_node_color\", label=\"Default color for nodes\", size=160, value=dict(rgb=dict(r=220, g=220, b=220, a=0)), labelPosition=\"right\"),\n",
    "        ], style={\"float\": \"right\", \"width\": \"30%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "    ], style={\"float\": \"right\", \"width\": \"50%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "], style={\"height\": \"240px\"})\n",
    "\n",
    "vis_layout_tab = html.Div([], style={\"height\": \"240px\"})\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Tabs(children=[\n",
    "        dcc.Tab(children=[generation_tab], label=\"Generation\"),\n",
    "        dcc.Tab(children=[vis_data_tab], label=\"Visualization (data)\"),\n",
    "        dcc.Tab(children=[vis_color_tab], label=\"Visualization (colors)\"),\n",
    "        dcc.Tab(children=[vis_layout_tab], label=\"Visualization (layout)\"),\n",
    "    ],),\n",
    "    html.Div([\n",
    "        dcc.Graph(id='sankeyplot'),\n",
    "    ]),\n",
    "    dcc.Store(id=\"run_config\"),\n",
    "    dcc.Store(id=\"current_run_config\"),\n",
    "    dcc.Store(id=\"vis_config\"),\n",
    "    dcc.Store(id=\"notify\"),\n",
    "    dcc.Store(id=\"graph_id\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_dict_to_tuple(d, a=False):\n",
    "    tup = (d[\"r\"], d[\"g\"], d[\"b\"])\n",
    "    tup += (d[\"a\"], ) if a else ()\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output('run_config', 'data'),\n",
    "    [\n",
    "        Input('model_generate_heads', 'value'),\n",
    "        Input('min_stop_tokens', 'value'),\n",
    "        Input('max_new_tokens', 'value'),\n",
    "        Input('multirep_tokens', 'value'),\n",
    "    ]\n",
    ")\n",
    "def update_run_config(gen_heads, min_stop_tokens, max_new_tok, multirep):\n",
    "    return {\n",
    "        \"gen_heads\": gen_heads,\n",
    "        \"min_stop_tokens\": min_stop_tokens,\n",
    "        \"max_new_tok\": max_new_tok,\n",
    "        \"multirep\": len(multirep) if multirep else None,\n",
    "    }\n",
    "\n",
    "@app.callback(\n",
    "    [\n",
    "        Output('vis_config', 'data'),\n",
    "        Output(\"notify\", \"data\"),\n",
    "    ],\n",
    "    [\n",
    "        Input('attention_heads', 'value'), Input('embeddings', 'value'),  Input('row_index', 'value'), Input('token_index', 'value'), Input('row_limit', 'value'),\n",
    "            Input('show_0', 'value'),\n",
    "        Input('colormap', 'value'), Input('color_mapping_color', 'value'), Input('color_brightness_range', 'value'), Input('node_opacity', 'value'),\n",
    "            Input('link_opacity', 'value'), Input('non_residual_link_color', 'value'), Input('default_node_color', 'value'), Input('color_nodes', 'value'),\n",
    "            Input('extra_bright_node', 'value'), Input('extra_bright_ffnn', 'value'), Input('extra_bright_att', 'value'), Input('extra_bright_int', 'value'),\n",
    "        \n",
    "        Input('run_config', 'data')\n",
    "    ],\n",
    "    prevent_initial_call=True,\n",
    ")\n",
    "def update_vis_config(\n",
    "    att_head, emb, row_index, token_index, row_limit, show_0,\n",
    "    colormap, color_mapping_color, color_brightness_range, node_opacity,\n",
    "        link_opacity, non_residual_link_color, default_node_color, color_nodes,\n",
    "        extra_bright_node, extra_bright_ffnn, extra_bright_att, extra_bright_int,\n",
    "    run_config\n",
    "):  \n",
    "    vis_colormap = colors[colormap] if colormap in colors else [color_mapping_color[\"hex\"]]\n",
    "    nrlc = rgb_dict_to_tuple(non_residual_link_color[\"rgb\"])\n",
    "    dnc = rgb_dict_to_tuple(default_node_color[\"rgb\"])\n",
    "    print(show_0)\n",
    "    return {\n",
    "        \"head\": att_head,\n",
    "        \"embedding\": emb,\n",
    "        \"sankey_parameters\": dataclasses.asdict(SankeyParameters(\n",
    "            row_index=row_index, token_index=token_index, rowlimit=row_limit, multirep=run_config[\"multirep\"], show_0=show_0,\n",
    "            colormap=vis_colormap, color_brightness_range=color_brightness_range, node_opacity=node_opacity,\n",
    "                link_opacity=link_opacity, non_residual_link_color=nrlc, default_node_color=dnc, color_nodes=color_nodes,\n",
    "                extra_brightness_map={\"Node\":extra_bright_node, \"FFNN\":extra_bright_ffnn, \"Attention\":extra_bright_att, \"Intermediate\":extra_bright_int},\n",
    "        )),\n",
    "    }, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache.memoize()\n",
    "def model_output(prompt, session, run_config):\n",
    "    prompt = prompt_structure.format(prompt=prompt)\n",
    "    gen_config = GenerationConfig(\n",
    "        pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else None,\n",
    "        output_attentions=True, output_hidden_states=True, return_dict_in_generate=True\n",
    "    )\n",
    "    text_output, generated_output, gen_tokens = model_generate(\n",
    "            model, tokenizer, prompt, \n",
    "            max_extra_length=run_config[\"max_new_tok\"], \n",
    "            config=gen_config, \n",
    "            min_stop_length=run_config[\"min_stop_tokens\"], stopping_tokens=stopgen_tokens\n",
    "    )\n",
    "    debug_vectors = extrapolate_debug_vectors(model)\n",
    "\n",
    "    dfs = {}\n",
    "    linkinfos = {}\n",
    "    for head in range(-1, run_config[\"gen_heads\"] + 1):\n",
    "        linkinfos[head] = generate_sankey_linkinfo(generated_output, debug_vectors, head)\n",
    "    for emb in [\"input\", \"output\", \"interpolate\"]:\n",
    "        dfs[emb] = create_hidden_states_df(\n",
    "            model, tokenizer, generated_output, gen_tokens, emb, \n",
    "            include_prompt=True, fix_characters=fix_characters,\n",
    "            multirep=run_config[\"multirep\"],\n",
    "        )\n",
    "    return text_output, dfs, linkinfos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback to generate output\n",
    "@app.callback(\n",
    "    [\n",
    "        Output('model_output', 'value'),\n",
    "        Output('attention_heads', 'marks'),\n",
    "        Output('attention_heads', 'value'),\n",
    "        Output('graph_id', 'data'),\n",
    "        Output('current_run_config', 'data'),\n",
    "        Output(\"notify\", \"data\", allow_duplicate=True),\n",
    "    ],\n",
    "    Input('model_generate', 'n_clicks'),\n",
    "    [\n",
    "        State('model_input', 'value'),\n",
    "        State('run_config', 'data'),\n",
    "    ],\n",
    "    running=[(Output(\"model_generate\", \"disabled\"), True, False)],\n",
    "    prevent_initial_call=True,\n",
    "    background=True,\n",
    "    manager=background_callback_manager\n",
    ")\n",
    "def update_model_generation(click_data, prompt, run_config):\n",
    "    if ctx.triggered_prop_ids:\n",
    "        graph_id = str(uuid.uuid4())\n",
    "        slider_marks = {i: f\"Head {i}\" for i in range(0, run_config[\"gen_heads\"] + 1)}\n",
    "        slider_marks.update({-1: \"AVG\"})\n",
    "        text_output, _, _ = model_output(prompt, graph_id, run_config)\n",
    "        return text_output, slider_marks, -1, graph_id, run_config, True\n",
    "    raise PreventUpdate\n",
    "\n",
    "@app.callback(\n",
    "    Output('sankeyplot', 'figure'),\n",
    "    [\n",
    "        Input('notify', 'data'),\n",
    "    ],[\n",
    "        State('model_input', 'value'),\n",
    "        State('graph_id', 'data'),\n",
    "        State('current_run_config', 'data'),\n",
    "        State('vis_config', 'data'),\n",
    "    ]\n",
    ")\n",
    "def update_sankey_plot(notify, prompt, graph_id, run_config, vis_config):\n",
    "    if ctx.triggered_prop_ids and run_config and vis_config:\n",
    "        _, dfs, linkinfos = model_output(prompt, graph_id, run_config)\n",
    "        sankey_parameters = SankeyParameters(**vis_config[\"sankey_parameters\"])\n",
    "        cur_linkinfo = linkinfos[vis_config[\"head\"]]\n",
    "        cur_linkinfo[\"attentions\"] = cur_linkinfo[\"attentions\"] if sankey_parameters.show_0 else [np.array([[0 if i == 0 or j == 0 else e2 for j,e2 in enumerate(e1)] for i,e1 in enumerate(row)]) for row in cur_linkinfo[\"attentions\"]] # Attentions\n",
    "        sankey_info = generate_sankey(\n",
    "            dfs[vis_config[\"embedding\"]],\n",
    "            linkinfos[vis_config[\"head\"]],\n",
    "            sankey_parameters\n",
    "        )\n",
    "        fig = format_sankey(*sankey_info, sankey_parameters)\n",
    "        return fig\n",
    "    raise PreventUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "Show first token\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n",
      "['S', 'h', 'o', 'w', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'o', 'k', 'e', 'n']\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=\"True\", jupyter_mode=\"_none\", port=8050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
