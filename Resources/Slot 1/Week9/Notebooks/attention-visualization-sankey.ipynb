{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ngrok -q\n",
    "#!pip install dash -q\n",
    "#!pip install \"dash[diskcache]\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, AutoConfig, StoppingCriteriaList, StoppingCriteria\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import itertools\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/oobabooga/text-generation-webui/blob/2cf711f35ec8453d8af818be631cb60447e759e2/modules/callbacks.py#L12\n",
    "class _SentinelTokenStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, sentinel_token_ids: list, starting_idx: int):\n",
    "        StoppingCriteria.__init__(self)\n",
    "        self.sentinel_token_ids = sentinel_token_ids\n",
    "        self.starting_idx = starting_idx\n",
    "        self.shortest = min([x.shape[-1] for x in sentinel_token_ids])\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, _scores: torch.FloatTensor) -> bool:\n",
    "        for sample in input_ids:\n",
    "            trimmed_sample = sample[self.starting_idx:]\n",
    "            trimmed_len = trimmed_sample.shape[-1]\n",
    "            if trimmed_len < self.shortest:\n",
    "                continue\n",
    "\n",
    "            for sentinel in self.sentinel_token_ids:\n",
    "                sentinel_len = sentinel.shape[-1]\n",
    "                if trimmed_len < sentinel_len:\n",
    "                    continue\n",
    "\n",
    "                window = trimmed_sample[-sentinel_len:]\n",
    "                if torch.all(torch.eq(sentinel, window)):\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "####\n",
    "\n",
    "def generate_stopping_criteria(stopgen_tokens, input_len=0):\n",
    "    return StoppingCriteriaList([\n",
    "        _SentinelTokenStoppingCriteria(\n",
    "            sentinel_token_ids = stopgen_tokens,\n",
    "            starting_idx=input_len\n",
    "        )\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"microsoft/phi-1_5\"\n",
    "#model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b55a9b1cad54d889eff365ff9f8b6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_key = \"\"\n",
    "if model_id in [\"meta-llama/Llama-2-7b-hf\"]:\n",
    "    hf_key = input(\"Hugging Face Key: \")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n",
    "model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n",
    "del hf_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_id in [\"microsoft/phi-1_5\"]:\n",
    "    stopgen_tokens = [\n",
    "        torch.tensor([198, 198]),  # \\n\\n\n",
    "        torch.tensor([628])        # \\n\\n\n",
    "    ]\n",
    "    prompt_structure = \"Question: {prompt}\\n\\nAnswer:\"\n",
    "    exclude_token_offset = 3\n",
    "    fix_characters = [(\"Ġ\", \"␣\"), (\"Ċ\", \"\\n\")]\n",
    "elif model_id in [\"meta-llama/Llama-2-7b-hf\", \"mistralai/Mistral-7B-v0.1\"]:\n",
    "    stopgen_tokens = [\n",
    "        torch.tensor([1]),  # <s>\n",
    "        torch.tensor([2])   # </s>\n",
    "    ]\n",
    "    prompt_structure = \"{prompt}\"\n",
    "    exclude_token_offset = None\n",
    "    fix_characters = [(\"<0x0A>\", \"\\n\")]\n",
    "\n",
    "fix_characters += [(\"\\n\", \"\\\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.eos_token_id and not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALTERATION ### Divided computation for attentions\n",
    "### ALTERATION ###  Added function to compute attentions also for prompt\n",
    "\n",
    "def pad_masked_attentions(attentions, max_len):\n",
    "    \"\"\"\n",
    "    Attention in generative models are masked, we want to plot a heatmap so we must pad all attentions to the same size with 0.0 values\n",
    "    \"\"\"\n",
    "    array_attentions = [np.array(att) for att in attentions]\n",
    "    new_attentions = [np.concatenate([att, np.zeros([max_len - len(att)])]) for att in array_attentions]\n",
    "    return np.array(new_attentions)\n",
    "\n",
    "def compute_complete_padded_attentions(generated_output, layer, head):\n",
    "    single_layer_attentions = []\n",
    "    # Prompt tokens\n",
    "    for single_layer_single_head in torch.squeeze(torch.select(generated_output.attentions[0][layer], 1, head)):\n",
    "        single_layer_attentions.append(single_layer_single_head)\n",
    "    # Response tokens\n",
    "    for attentions_per_token in generated_output.attentions[1:]:\n",
    "        # Take single layer\n",
    "        single_layer = attentions_per_token[layer]\n",
    "        # Take only one head\n",
    "        single_layer_single_head = torch.select(single_layer, 1, head)\n",
    "        single_layer_attentions.append(single_layer_single_head)\n",
    "    # Squeeze dimensions to one a one-dimensional tensor\n",
    "    pure_attentions = [s.squeeze() for s in single_layer_attentions]\n",
    "    max_seq_len  = len(pure_attentions[-1])\n",
    "    # Print last attention heatmap\n",
    "    padded_attentions = pad_masked_attentions(pure_attentions, max_seq_len)\n",
    "    return padded_attentions\n",
    "\n",
    "def compute_batch_complete_padded_attentions(generated_output, heads):\n",
    "    multi_layer_head_attentions = []\n",
    "    for head in heads:\n",
    "        multi_layer_attentions = []\n",
    "        for layer in range(0, len(generated_output.attentions[0])):\n",
    "            # Prompt tokens\n",
    "            prompt_att = [\n",
    "                torch.squeeze(single_head)\n",
    "                for single_head in torch.squeeze(torch.select(generated_output.attentions[0][layer], 1, head))\n",
    "            ]\n",
    "            # Response tokens\n",
    "            response_att = [\n",
    "                torch.squeeze(torch.select(single_layer[layer], 1, head))\n",
    "                for single_layer in generated_output.attentions[1:]\n",
    "            ]\n",
    "            # Pad and merge attentions\n",
    "            multi_layer_attentions.append(pad_masked_attentions( \n",
    "                [att_token for att_token in prompt_att + response_att],\n",
    "                len(response_att[-1])\n",
    "            ))\n",
    "        multi_layer_head_attentions.append(multi_layer_attentions)\n",
    "    return multi_layer_head_attentions\n",
    "\n",
    "def plot_attentions(generated_output, layer, head, generated_tokens, past_tokens):\n",
    "    # Plot \n",
    "    data = compute_padded_attentions(generated_output, layer, head)\n",
    "    fig, ax = plt.subplots(figsize = (12,5))\n",
    "    im = ax.imshow(data)\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_yticks(np.arange(len(generated_tokens)), labels=generated_tokens)\n",
    "    ax.set_xticks(np.arange(len(past_tokens)), labels=past_tokens, fontsize=8)\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Heatmap of attention layers: layer {layer} head {head}\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALTERATION ### Adapted functions to work outside model\n",
    "### ALTERATION ### Added option to include hidden states for prompt and ending token in embed_hidden_states\n",
    "def _apply_lm_head(model, hidden_states, weights, bias):\n",
    "    \"\"\"\n",
    "    Function which takes as input the hidden states of the model and returns the prediction of the next token.\n",
    "    Uses the language modeling head of input\n",
    "    \"\"\"\n",
    "    pred_ids = []\n",
    "    for token_layer in hidden_states:\n",
    "        output = torch.matmul(token_layer.to(weights.device), weights) + bias\n",
    "        token_id = output.argmax(dim=-1)\n",
    "        pred_ids.append(token_id)\n",
    "    return pred_ids\n",
    "    \n",
    "def embed_hidden_states(model, hidden_states, embedding=\"output\", include_prompt=False, include_end=True, multirep=True, max_rep=10):\n",
    "    if embedding not in ['input', 'output']:\n",
    "        raise ValueError(\"Embedding not valid\")\n",
    "\n",
    "    end_idx = len(hidden_states) if include_end else len(hidden_states) - 1\n",
    "\n",
    "    if embedding == 'output':\n",
    "        weights = model.lm_head.weight.T\n",
    "    elif embedding == 'input':\n",
    "        weights = model.model.embed_tokens.weight.T\n",
    "\n",
    "    bias = model.lm_head.bias\n",
    "    if bias:\n",
    "        reverse_weights = torch.add(weights.T, bias.unsqueeze(dim=1))\n",
    "    else:\n",
    "        bias = 0\n",
    "        reverse_weights = weights.T \n",
    "\n",
    "    predictions = []\n",
    "    # Prompt tokens\n",
    "    if include_prompt:\n",
    "        for token_states in torch.stack(hidden_states[0]).swapaxes(0, 2):\n",
    "            if multirep:\n",
    "                pred_ids = compute_multirep(model, token_states.swapaxes(0, 1), weights, bias, reverse_weights, max_rep=max_rep)\n",
    "            else:\n",
    "                pred_ids = [_apply_lm_head(model, token_states.swapaxes(0, 1), weights, bias)]\n",
    "            predictions.append([[int(id) for id in idd] for idd in pred_ids])\n",
    "    # Response tokens\n",
    "    for token_states in hidden_states[1:end_idx]:\n",
    "        if multirep:\n",
    "            pred_ids = compute_multirep(model, token_states, weights, bias, reverse_weights, max_rep=max_rep)\n",
    "        else:\n",
    "            pred_ids = [_apply_lm_head(model, token_states, weights, bias)]\n",
    "        predictions.append([[int(id) for id in idd] for idd in pred_ids])\n",
    "    return predictions\n",
    "\n",
    "def compute_multirep(model, hidden_states, weights, bias, reverse_weights, max_rep=5):\n",
    "    pred_ids = []\n",
    "    #pred_norms = []\n",
    "    for i, hs in enumerate(hidden_states):\n",
    "        tokens = []\n",
    "        norms = []\n",
    "        token_emb = hs.squeeze()\n",
    "        for i in range(0, max_rep):\n",
    "            # Compute token and embedding norm\n",
    "            logits = torch.matmul(token_emb, weights) + bias\n",
    "            token_id = torch.argmax(logits)\n",
    "            norm = torch.norm(token_emb) \n",
    "            # Stop prematurely if norm is too small or if norm is bigger than previous one\n",
    "            if norm <= 0.01 or (len(norms) > 0 and norm >= norms[-1]):\n",
    "                break\n",
    "            # Do not add repreated tokens\n",
    "            if token_id not in tokens:\n",
    "                tokens.append(token_id)\n",
    "            norms.append(norm)\n",
    "            # Compute next embedding by subtracting the closest embedding to the current embedding\n",
    "            closest_emb = reverse_weights[token_id]\n",
    "            token_emb = token_emb - closest_emb\n",
    "        pred_ids.append(tokens)\n",
    "        #pred_norms.append(norms)\n",
    "    return pred_ids#, pred_norms\n",
    "\n",
    "def test_multirep(model, input, embedding, token=1):\n",
    "    if embedding == 'output':\n",
    "        weights = model.lm_head.weight.T\n",
    "    elif embedding == 'input':\n",
    "        weights = model.model.embed_tokens.weight.T\n",
    "\n",
    "    bias = model.lm_head.bias\n",
    "    if bias:\n",
    "        reverse_weights = torch.add(weights.T, bias.unsqueeze(dim=1))\n",
    "    else:\n",
    "        bias = 0\n",
    "        reverse_weights = weights.T \n",
    "    inputs = tokenizer(\"Hi, how are you\", return_tensors=\"pt\")\n",
    "    gen_config = GenerationConfig(\n",
    "        pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else None,\n",
    "        output_attentions=True, output_hidden_states=True, return_dict_in_generate=True\n",
    "    )\n",
    "    gen_output = model.generate(inputs.input_ids, generation_config=gen_config, max_new_tokens=5)\n",
    "    print(tokenizer.decode(gen_output.sequences.squeeze()))\n",
    "    a,aa = compute_multirep(model, gen_output.hidden_states[1], weights, bias, reverse_weights)\n",
    "    return [[(tokenizer.decode(c), cc.detach().numpy().tolist()) for c,cc in zip(b,bb)] for b,bb in zip(a,aa)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi, how are you?\"\n",
    "gen_config = GenerationConfig(\n",
    "    pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else None,\n",
    "    output_attentions=True, output_hidden_states=True, return_dict_in_generate=True\n",
    ")\n",
    "text_output, generated_output, gen_tokens = model_generate(\n",
    "    model, tokenizer, prompt, \n",
    "    max_extra_length=2, \n",
    "    config=gen_config, \n",
    "    min_stop_length=1, stopping_tokens=stopgen_tokens\n",
    ")   \n",
    "dfs = {}\n",
    "for emb in [\"input\", \"output\"]:\n",
    "    dfs[emb] = create_hidden_states_df(\n",
    "        model, tokenizer, generated_output, gen_tokens, emb, \n",
    "        include_prompt=True, fix_characters=fix_characters,\n",
    "        multirep=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tok = len(dfs[\"input\"].columns)\n",
    "\n",
    "attentions = compute_batch_complete_padded_attentions(generated_output, range(0, 32))\n",
    "\n",
    "# for key, df in dfs.items():        \n",
    "nodexs = []\n",
    "nodeys = []\n",
    "labels = []\n",
    "# Cycle through every layer of the model, gathering all blocks as nodes\n",
    "for idx, row in dfs[\"input\"].iterrows():\n",
    "    # Generate coordinates for nodes\n",
    "    xs = [i for i in range(len(row))]\n",
    "    ys = [idx] * len(row)\n",
    "\n",
    "    nodexs.append(xs)\n",
    "    nodeys.append(ys)\n",
    "\n",
    "    for x, y in zip(xs, ys):\n",
    "        labels.append(row.iloc[x])\n",
    "\n",
    "s = np.repeat([i for i in range(0, len(labels) - n_tok)], n_tok).astype(int).tolist()\n",
    "t = np.array([[j for j in range(i, i + n_tok)] * n_tok for i in range(n_tok, len(labels), n_tok)]).flatten().astype(int).tolist()\n",
    "v = np.array([[attentions[-1][i][j][::-1] for j in range(0, n_tok)][::-1] for i in reversed(range(0, 32))]).flatten()[::-1].astype(float).tolist()\n",
    "s = [el_s for el_s, el_v in zip(s, v) if el_v > 0]\n",
    "t = [el_t for el_t, el_v in zip(t, v) if el_v > 0]\n",
    "v = [el_v for el_v in v if el_v > 0]\n",
    "\n",
    "#nodexs = np.array(nodexs).flatten() \n",
    "#nodexs = (nodexs / nodexs.max()) * 0.1 + 0.0000001\n",
    "#nodeys = np.array(nodeys).flatten()\n",
    "#nodeys = (nodeys / nodeys.max()) * 3 + 0.0000001\n",
    "\n",
    "# Vertical\n",
    "#fig = go.Figure(go.Sankey(\n",
    "#    orientation = \"v\",\n",
    "#    arrangement='fixed',\n",
    "#    valueformat=\".5r\",\n",
    "#    node=dict(\n",
    "#        align=\"left\",\n",
    "#        label=[f\"{i} {l}\" for i,l in enumerate(labels)],\n",
    "#        y=nodexs,\n",
    "#        x=nodeys[::-1],\n",
    "#        pad=50\n",
    "#    ),\n",
    "#    link=dict(\n",
    "#       source=s,\n",
    "#       target=t,\n",
    "#       value=v\n",
    "#    )\n",
    "#))\n",
    "#fig.update_layout(font_size=10, width=1000, height=2700)\n",
    "\n",
    "nodexs = np.array(nodexs).flatten() \n",
    "nodexs = (nodexs / nodexs.max()) + 0.0000001\n",
    "nodeys = np.array(nodeys).flatten()\n",
    "nodeys = (nodeys / nodeys.max()) + 0.0000001\n",
    "\n",
    "t = go.Sankey(\n",
    "    orientation = \"h\",\n",
    "    arrangement=\"fixed\",\n",
    "    valueformat=\".5r\",\n",
    "    node=dict(\n",
    "        align=\"left\",\n",
    "        label=[f\"{i} {l}\" for i,l in enumerate(labels)],\n",
    "        y=nodexs,\n",
    "        x=nodeys[::-1],\n",
    "        pad=80\n",
    "    ),\n",
    "    link=dict(\n",
    "       source=s,\n",
    "       target=t,\n",
    "       value=v\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.update_layout(font_size=10, width=4000, height=1000)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'remove_trace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(fig[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m fig\u001b[39m.\u001b[39madd_trace(t)\n\u001b[0;32m----> 5\u001b[0m fig\u001b[39m.\u001b[39;49mremove_trace()\n\u001b[1;32m      6\u001b[0m fig\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'remove_trace'"
     ]
    }
   ],
   "source": [
    "fig= go.Figure()\n",
    "fig.update_layout(font_size=10, width=4000, height=1000)\n",
    "print(fig[\"data\"])\n",
    "fig.add_trace(t)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataframe_characters(df, replacements, multirep=False, columns=False):\n",
    "    for old, new in replacements:\n",
    "        df = df.map(lambda x: [i.replace(old, new) for i in x] if multirep else x.replace(old, new))\n",
    "    if columns:\n",
    "        for old, new in replacements:\n",
    "            df.columns = df.columns.str.replace(old, new)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_point = 0.32\n",
    "delta_arrow = 0.22\n",
    "\n",
    "def gen_edges(\n",
    "    edges, permanent_edges, attentions,\n",
    "    generated_output, \n",
    "    nodexs, nodeys, \n",
    "    head, \n",
    "    exclude, \n",
    "    threshold, permanent_threshold,\n",
    "):\n",
    "    # Cycle through every layer of the model, gathering the aggregated coordinates for each node in a layer\n",
    "    for idx, coords in enumerate(zip(nodexs, nodeys)):\n",
    "        xs, ys = coords\n",
    "\n",
    "        # Do not plot attention traces for the starting layer\n",
    "        if idx != 0:\n",
    "            # Compute the attention weights for the current layer and head\n",
    "            attentions_lh = attentions[idx - 1]\n",
    "\n",
    "            # Cycle through every node in the layer, gathering its coordinates \n",
    "            for i, c in enumerate(zip(xs, ys)):\n",
    "                x, y = c\n",
    "\n",
    "                # Cycle through every node in the PREVIOUS layer w.r.t. the current one, gathering its coordinates \n",
    "                for ii, cc in enumerate(zip(nodexs[idx - 1], nodeys[idx - 1])):\n",
    "                    xx, yy = cc\n",
    "                    weight = attentions_lh[ii][i]\n",
    "                    \n",
    "                    if x not in exclude and weight >= threshold:\n",
    "                        if weight >= permanent_threshold:\n",
    "                            # Create single permanent edge representing the attention weight\n",
    "                            permanent_edges[head].append(go.Scattergl(\n",
    "                                x=[x, (x+xx)/2, xx],\n",
    "                                y=[y + delta_arrow, (y + yy + delta_arrow - delta_point)/2, yy - delta_point],\n",
    "                                name=\"pedge\",\n",
    "                                mode=\"markers+lines\",\n",
    "                                marker=dict(size=[4, 0, 8], color=\"black\", opacity=[1, 0, 1]),\n",
    "                                marker_symbol=[1, 0, 5],\n",
    "                                marker_line_width=0,\n",
    "                                text=f\"{str(weight)}\",\n",
    "                                hoverinfo=[\"none\", \"text\", \"none\"],\n",
    "                                line=dict(color=\"rgba(125,125,125,0.7)\", width=2 * weight + 0.25),\n",
    "                                customdata=[{\"type\": \"edge\", \"P1\": {\"x\": x, \"y\": y}, \"P2\": {\"x\": xx, \"y\":yy}}],\n",
    "                                showlegend=False,\n",
    "                            ))\n",
    "                        else:\n",
    "                            # Create single edge representing the attention weight\n",
    "                            edges[head].append(go.Scattergl(\n",
    "                                x=[x, (x+xx)/2, xx],\n",
    "                                y=[y + delta_arrow, (y + yy + delta_arrow - delta_point)/2, yy - delta_point],\n",
    "                                name=\"edge\",\n",
    "                                mode=\"markers+lines\",\n",
    "                                marker=dict(size=[4, 0, 8], color=\"black\", opacity=[1, 0, 1]),\n",
    "                                marker_symbol=[1, 0, 5],\n",
    "                                marker_line_width=0,\n",
    "                                text=f\"{str(weight)}\",\n",
    "                                hoverinfo=[\"none\", \"text\", \"none\"],\n",
    "                                line=dict(color=\"rgba(125,125,125,0.7)\", width=2 * weight + 0.25),\n",
    "                                customdata=[{\"type\": \"edge\", \"P1\": {\"x\": x, \"y\": y}, \"P2\": {\"x\": xx, \"y\":yy}}],\n",
    "                                showlegend=False,\n",
    "                            ))\n",
    "\n",
    "def create_transformer_plot(dfs, generated_output, exclude=[], heads=range(0, 1), multirep=False, max_heads=32, threshold=0.002, permanent_threshold=0.4):\n",
    "    nodes = {key: [] for key in dfs.keys()}\n",
    "    edges = {head: [] for head in heads}\n",
    "    permanent_edges = {head: [] for head in heads}\n",
    "\n",
    "    attentions = compute_batch_complete_padded_attentions(generated_output, range(0, max_heads))\n",
    "\n",
    "    input_len = generated_output.attentions[0][0].size()[-1]\n",
    "    n_layers = len(list(dfs.values())[0])\n",
    "    in_tok =  list(dfs[\"input\"].columns)\n",
    "    out_tok =  list(dfs[\"output\"].columns)\n",
    "    len_tok = len(in_tok)\n",
    "\n",
    "    pnodes = go.Scattergl(\n",
    "        x=[i for i in range(0, len_tok)] * 2,\n",
    "        y=[n_layers] * len_tok + [-1] * len_tok,\n",
    "        name=\"pnode\",\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(\n",
    "            size=22,\n",
    "            color=  ( ([\"#bab0a2\"] * (input_len-1)) + ([\"#ff9914\"] * (len_tok - input_len + 1)) ) + \n",
    "                    ( ([\"#bab0a2\"] * input_len) + ([\"#ff9914\"] * (len_tok - input_len)) ),\n",
    "            opacity=0.5\n",
    "        ),\n",
    "        marker_line_width=0,\n",
    "        marker_symbol=0,\n",
    "        text=out_tok + in_tok,\n",
    "        textfont={\"size\": 12},\n",
    "        textposition=\"middle center\",\n",
    "        hoverinfo=\"none\",\n",
    "        customdata=[{\"type\":\"node\"}],\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    for key, df in dfs.items():\n",
    "        \n",
    "        nodexs = []\n",
    "        nodeys = []\n",
    "\n",
    "        # Cycle through every layer of the model, gathering all blocks as nodes\n",
    "        for idx, row in df.iterrows():\n",
    "\n",
    "                # Generate coordinates for nodes\n",
    "                xs = [i for i in range(len(row))]\n",
    "                ys = [idx] * len(row)\n",
    "\n",
    "                nodexs.append(xs)\n",
    "                nodeys.append(ys)\n",
    "\n",
    "                for x, y in zip(xs, ys):\n",
    "                    srow = row[x]\n",
    "                    color = \"lightblue\"\n",
    "                    if x in exclude:\n",
    "                        color = \"red\"\n",
    "\n",
    "                    # Create nodes\n",
    "                    nodes[key].append(go.Scattergl(\n",
    "                        x=[x],\n",
    "                        y=[y],\n",
    "                        name=\"node\",\n",
    "                        mode=\"markers+text\",\n",
    "                        marker=dict(size=22, color=color, opacity=0.5),\n",
    "                        marker_line_width=0,\n",
    "                        marker_symbol=1,\n",
    "                        text=srow[0] if multirep else srow,\n",
    "                        textfont={\"size\": 11},\n",
    "                        textposition=\"middle center\",\n",
    "                        hoverinfo=\"text\" if multirep else \"none\",\n",
    "                        hovertemplate=\"<br>\".join([el for el in srow]) if multirep else None,\n",
    "                        customdata=[{\"type\": \"node\", \"rep_tokens\": srow}],\n",
    "                        showlegend=False\n",
    "                    ))\n",
    "\n",
    "    for head in heads:\n",
    "        att = attentions[head]\n",
    "        if head == -1:\n",
    "            att = np.mean(attentions, axis=0)\n",
    "        gen_edges(\n",
    "            edges, permanent_edges, attentions[head],\n",
    "            generated_output, \n",
    "            nodexs, nodeys, \n",
    "            head, \n",
    "            exclude, \n",
    "            threshold, permanent_threshold\n",
    "        )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=pnodes)\n",
    "\n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        title=\"Transformer Weights Visualization\",\n",
    "        showlegend=True,\n",
    "        xaxis=dict(showticklabels=False, zeroline=False),\n",
    "        yaxis=dict(showticklabels=False, zeroline=False),\n",
    "        plot_bgcolor='white',\n",
    "        width=1100 + (len_tok * 40), height=1100 + (n_layers * 30),\n",
    "        uirevision=\"const\"\n",
    "    )\n",
    "    \n",
    "    return go.FigureWidget(fig), nodes, (edges, permanent_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_plot(dfs, generated_output, exclude=[], heads=range(0, 1), multirep=False, max_heads=32, threshold=0.002, permanent_threshold=0.4):\n",
    "    \n",
    "    sankey_traces = {}\n",
    "    attentions = compute_batch_complete_padded_attentions(generated_output, range(0, 32))\n",
    "\n",
    "    for key, df in dfs.items():\n",
    "        n_tok = len(df.columns)\n",
    "        \n",
    "        nodexs = []\n",
    "        nodeys = []\n",
    "        labels = []\n",
    "        # Cycle through every layer of the model, gathering all blocks as nodes\n",
    "        for idx, row in df.iterrows():\n",
    "            # Generate coordinates for nodes\n",
    "            xs = [i for i in range(len(row))]\n",
    "            ys = [idx] * len(row)\n",
    "        \n",
    "            nodexs.append(xs)\n",
    "            nodeys.append(ys)\n",
    "        \n",
    "            for x, y in zip(xs, ys):\n",
    "                labels.append(row.iloc[x])\n",
    "\n",
    "        s = np.repeat([i for i in range(0, len(labels) - n_tok)], n_tok).astype(int).tolist()\n",
    "        t = np.array([[j for j in range(i, i + n_tok)] * n_tok for i in range(n_tok, len(labels), n_tok)]).flatten().astype(int).tolist()\n",
    "        v = np.array([[attentions[-1][i][j][::-1] for j in range(0, n_tok)][::-1] for i in reversed(range(0, 32))]).flatten()[::-1].astype(float).tolist()\n",
    "        s = [el_s for el_s, el_v in zip(s, v) if el_v > 0]\n",
    "        t = [el_t for el_t, el_v in zip(t, v) if el_v > 0]\n",
    "        v = [el_v for el_v in v if el_v > 0]\n",
    "\n",
    "        nodexs = np.array(nodexs).flatten() \n",
    "        nodexs = (nodexs / nodexs.max()) + 0.0000001\n",
    "        nodeys = np.array(nodeys).flatten()\n",
    "        nodeys = (nodeys / nodeys.max()) + 0.0000001\n",
    "\n",
    "        multirefs = []\n",
    "        if multirep:\n",
    "            multirefs = labels\n",
    "            labels = [l[0] for l in labels]\n",
    "            labels = [f\"{i} {l}\" for i,l in enumerate(labels)]\n",
    "\n",
    "        sankey_traces[key] = go.Sankey(\n",
    "            orientation = \"h\",\n",
    "            arrangement=\"fixed\",\n",
    "            valueformat=\".5r\",\n",
    "            node=dict(\n",
    "                align=\"left\",\n",
    "                customdata=multirefs,\n",
    "                hovertemplate='%{customdata} - Value: %{value}<extra></extra>',\n",
    "                label=labels,\n",
    "                y=nodexs,\n",
    "                x=nodeys,\n",
    "                pad=80\n",
    "            ),\n",
    "            link=dict(\n",
    "                arrowlen=15,\n",
    "                source=t,\n",
    "                target=s,\n",
    "                value=v\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(font_size=10, width=4000, height=1000)\n",
    "\n",
    "    return go.FigureWidget(fig), sankey_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_precache(nodes, edges, pedges, heads):\n",
    "    return {\n",
    "        (x,y):  { \n",
    "            head: {\n",
    "                \"edges\": traces,\n",
    "                \"color_nodes\": {\n",
    "                    emb: compute_color_nodes(emb_nodes, traces + pedges[head], x, y) \n",
    "                    for emb, emb_nodes in nodes.items()\n",
    "                }\n",
    "            } for head in heads if (traces := compute_add_traces(edges[head], x, y)) or True\n",
    "        }\n",
    "        for x,y in [(x,y) for node_coords in [zip(node_batch.x, node_batch.y) for node_batch in list(nodes.values())[0]] for x,y in node_coords]\n",
    "    }\n",
    "\n",
    "def access_edge_cache(cache, x, y, attention_head, embedding, color_scale):\n",
    "    add_traces = cache[(x,y)][attention_head][\"edges\"]\n",
    "    add_color_nodes = cache[(x,y)][attention_head][\"color_nodes\"][embedding]\n",
    "    add_color_nodes = [apply_color(node, trace, color_scale) for node, trace in add_color_nodes]\n",
    "    return add_traces, add_color_nodes\n",
    "\n",
    "def compute_add_traces(edges, x, y):\n",
    "    return [\n",
    "        el for el in edges if (\n",
    "            el.customdata[0][\"P1\"][\"x\"] == x and el.customdata[0][\"P1\"][\"y\"] == y\n",
    "        ) or (\n",
    "            el.customdata[0][\"P2\"][\"x\"] == x and el.customdata[0][\"P2\"][\"y\"] == y\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def compute_color_nodes(nodes, add_traces, x, y):\n",
    "    return [\n",
    "        (node, trace)\n",
    "        for trace in add_traces for node in filter(lambda n: n.y[0] == y - 1 and n.x[0] <= x, nodes) \n",
    "        if trace.customdata[0][\"P1\"][\"x\"] in node.x and trace.customdata[0][\"P1\"][\"y\"] in node.y and\n",
    "            trace.customdata[0][\"P2\"][\"x\"] == x and trace.customdata[0][\"P2\"][\"y\"] == y\n",
    "    ]\n",
    "\n",
    "def apply_color(node, trace, color_scale):\n",
    "    node[\"marker\"][\"color\"] = pc.sample_colorscale(color_scale, float(trace.text))\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate(model, tokenizer, prompt, max_extra_length, config, min_stop_length, stopping_tokens):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_len = len(inputs.input_ids.squeeze().tolist())\n",
    "    max_len = input_len + max_extra_length\n",
    "    \n",
    "    gen_config = config\n",
    "    stopping_criteria = generate_stopping_criteria(stopping_tokens, input_len + min_stop_length)\n",
    "    \n",
    "    generated_output = model.generate(inputs.input_ids, generation_config=gen_config, max_length=max_len, stopping_criteria=stopping_criteria)\n",
    "    outputs = generated_output.sequences.squeeze()\n",
    "    text_output = tokenizer.decode(generated_output.sequences.squeeze()[input_len:])\n",
    "    \n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(generated_output.sequences[0])\n",
    "    input_tokens = all_tokens[0:input_len]\n",
    "    generated_tokens = all_tokens[input_len:]\n",
    "    \n",
    "    return text_output, generated_output, {\"in\": input_tokens, \"gen\": generated_tokens}\n",
    "\n",
    "def create_hidden_states_df(model, tokenizer, generated_output, gen_tokens, embedding, include_prompt, fix_characters, multirep=False):\n",
    "    predictions = embed_hidden_states(model, generated_output.hidden_states, embedding, include_prompt=include_prompt, multirep=multirep, max_rep=5)\n",
    "    rows = [[tokenizer.convert_ids_to_tokens(pred) for pred in pred_list] for pred_list in predictions]\n",
    "    rows = rows if multirep else np.squeeze(rows)\n",
    "    if embedding == \"input\":\n",
    "        cols = gen_tokens[\"in\"] + gen_tokens[\"gen\"][:-1]\n",
    "    else:\n",
    "        cols = gen_tokens[\"in\"][1:] + gen_tokens[\"gen\"]\n",
    "    df = pd.DataFrame(rows).T.sort_index(ascending=False).rename(columns={n: col for n, col in enumerate(cols)})\n",
    "    df = fix_dataframe_characters(df, fix_characters, multirep=multirep, columns=True)\n",
    "    return df\n",
    "\n",
    "def create_attention_visualization(dfs, generated_output, exclude, heads, max_heads, compute_precache=True, multirep=False):\n",
    "    figure, sankey_traces = create_transformer_plot(dfs, generated_output, exclude, heads=heads, max_heads=max_heads, multirep=multirep)\n",
    "    edges, permanent_edges = (None, None)\n",
    "    permanent_traces = None\n",
    "    edge_precache = None\n",
    "    #edges, permanent_edges = edges\n",
    "    #permanent_traces = {head: {\"edges\": permanent_edges[head]} for head in heads}\n",
    "    #edge_precache = compute_edge_precache(nodes, edges, permanent_edges, heads) if compute_precache else None\n",
    "    return figure, sankey_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ngrok\n",
    "import asyncio\n",
    "import dash\n",
    "import diskcache\n",
    "import uuid\n",
    "\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "from dash import dcc, html, ctx, Patch, DiskcacheManager\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "\n",
    "import dash_daq as daq\n",
    "import plotly.colors as pc\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def deploy_ngrok(address=\"http://127.0.0.1:8050\"):\n",
    "    listener = ngrok.forward(addr=address, authtoken=UserSecretsClient().get_secret(\"ngrok_key\"))\n",
    "    await asyncio.wait_for(listener, timeout=10)\n",
    "    public_url = listener.result().url()\n",
    "    print(f\"Deploy URL: {public_url}\")\n",
    "    return listener.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_scale = pc.sequential.Viridis\n",
    "\n",
    "current_els = []\n",
    "current_head = -1\n",
    "current_emb = \"output\"\n",
    "\n",
    "temp_edge_cache = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "cache = diskcache.Cache(\"./cache\")\n",
    "background_callback_manager = DiskcacheManager(cache)\n",
    "app = dash.Dash(\"Test\")\n",
    "\n",
    "# Define the layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            dcc.Textarea(id='model_input', placeholder ='Insert prompt...', style={'width': '100%', 'height': \"50%\"}),\n",
    "            dcc.Loading(id=\"model_loading\", type=\"dot\", color=\"#873ba1\", children =\n",
    "                dcc.Textarea(id='model_output', readOnly=True, style={'width': '100%', 'height': \"50%\"})\n",
    "            )\n",
    "        ], style={\"float\": \"left\", \"width\": \"70%\", \"height\": \"100%\", \"padding\": 2}),\n",
    "        html.Div([\n",
    "            html.P(children=\"# of attention heads to load\", style={\"margin\": \"0\"}),\n",
    "            dcc.Dropdown([{\"value\": i, \"label\": i+1 if i >= 0 else \"Average\"} for i in range(-1, model_config.num_attention_heads)], id='model_generate_heads', value=-1, clearable=False),\n",
    "            daq.NumericInput(id=\"min_stop_tokens\", label=\"Min # tokens for stopping criteria\", value=1, min=0, max=1024, labelPosition=\"right\"),\n",
    "            daq.NumericInput(id=\"max_new_tokens\", label=\"Max # of generated tokens\", value=10, min=0, max=1024, labelPosition=\"right\"),\n",
    "            dcc.Checklist([\"Precache Results\"], id=\"precache_results\", inline=True),\n",
    "            dcc.Checklist([\"Compute Multiple Token Representations\"], id=\"multirep_tokens\", inline=True),\n",
    "            html.Button('Generate', id='model_generate', style={\"width\": \"100%\", \"height\": \"20px\"}),\n",
    "        ], style={\"float\": \"right\", \"height\": \"100%\", \"width\": \"20%\", \"padding\": 2}),\n",
    "    ], style={\"height\": \"140px\"}),\n",
    "    html.Div([\n",
    "        html.P(\"Attention Head Selector\"),\n",
    "        dcc.Slider(marks={}, step=1, value=-1, id='attention_heads'),\n",
    "        html.P(\"Embeddings Selector\"),\n",
    "        dcc.RadioItems(['input', 'output'], value='output', inline=True, id='embeddings'),\n",
    "        dcc.Graph(id='scatterplot'),\n",
    "    ]),\n",
    "    dcc.Store(id=\"run_config\"),\n",
    "    dcc.Store(id=\"notify\"),\n",
    "    dcc.Store(id=\"graph_id\")\n",
    "])\n",
    "\n",
    "# Aggregate run parameters data\n",
    "@app.callback(\n",
    "    Output('run_config', 'data'),\n",
    "    [\n",
    "        Input('model_generate_heads', 'value'),\n",
    "        Input('min_stop_tokens', 'value'),\n",
    "        Input('max_new_tokens', 'value'),\n",
    "        Input('precache_results', 'value'),\n",
    "        Input('multirep_tokens', 'value'),\n",
    "    ]\n",
    ")\n",
    "def update_run_config(gen_heads, min_stop_tokens, max_new_tok, precache, multirep):\n",
    "    return {\n",
    "        \"gen_heads\": gen_heads,\n",
    "        \"min_stop_tokens\": min_stop_tokens,\n",
    "        \"max_new_tok\": max_new_tok,\n",
    "        \"precache\": len(precache) if precache else None,\n",
    "        \"multirep\": len(multirep) if multirep else None,\n",
    "    }\n",
    "\n",
    "\n",
    "@cache.memoize()\n",
    "def model_output(prompt, session, run_config):\n",
    "    prompt = prompt_structure.format(prompt=prompt)\n",
    "    gen_config = GenerationConfig(\n",
    "        pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else None,\n",
    "        output_attentions=True, output_hidden_states=True, return_dict_in_generate=True\n",
    "    )\n",
    "    text_output, generated_output, gen_tokens = model_generate(\n",
    "            model, tokenizer, prompt, \n",
    "            max_extra_length=run_config[\"max_new_tok\"], \n",
    "            config=gen_config, \n",
    "            min_stop_length=run_config[\"min_stop_tokens\"], stopping_tokens=stopgen_tokens\n",
    "    )   \n",
    "    dfs = {}\n",
    "    for emb in [\"input\", \"output\"]:\n",
    "        dfs[emb] = create_hidden_states_df(\n",
    "            model, tokenizer, generated_output, gen_tokens, emb, \n",
    "            include_prompt=True, fix_characters=fix_characters,\n",
    "            multirep=run_config[\"multirep\"],\n",
    "        )\n",
    "    excludes = []\n",
    "    if exclude_token_offset is not None:\n",
    "        excludes.append(len(gen_tokens[\"in\"]) - exclude_token_offset - 1)\n",
    "    figure, sankey_traces = create_attention_visualization(\n",
    "        dfs, generated_output, \n",
    "        exclude=excludes, \n",
    "        heads=range(-1, run_config[\"gen_heads\"]+1), max_heads=model_config.num_attention_heads, \n",
    "        compute_precache=run_config[\"precache\"],\n",
    "        multirep=run_config[\"multirep\"],\n",
    "    )\n",
    "    return text_output, sankey_traces, figure\n",
    "\n",
    "# Define callback to generate output\n",
    "@app.callback(\n",
    "    [\n",
    "        Output('model_output', 'value'),\n",
    "        Output('scatterplot', 'figure', allow_duplicate=True),\n",
    "        Output('attention_heads', 'marks'),\n",
    "        Output('attention_heads', 'value'),\n",
    "        Output('graph_id', 'data'),\n",
    "        Output(\"notify\", \"data\"),\n",
    "    ],\n",
    "    Input('model_generate', 'n_clicks'),\n",
    "    [\n",
    "        State('model_input', 'value'),\n",
    "        State('run_config', 'data'),\n",
    "    ],\n",
    "    running=[(Output(\"model_generate\", \"disabled\"), True, False)],\n",
    "    prevent_initial_call=True,\n",
    "    background=True,\n",
    "    manager=background_callback_manager\n",
    ")\n",
    "def update_model_generation(click_data, prompt, run_config):\n",
    "    if ctx.triggered_prop_ids:\n",
    "        graph_id = str(uuid.uuid4())\n",
    "        slider_marks = {i: f\"Head {i}\" for i in range(0, run_config[\"gen_heads\"] + 1)}\n",
    "        slider_marks.update({-1: \"AVG\"})\n",
    "        text_output, sankey_traces, figure = model_output(prompt, graph_id, run_config)\n",
    "        return text_output, figure, slider_marks, -1, graph_id, True\n",
    "    raise PreventUpdate\n",
    "\n",
    "#Define callback to update scatter plot\n",
    "@app.callback(\n",
    "    Output('scatterplot', 'figure'),\n",
    "    [\n",
    "        Input('scatterplot', 'clickData'),\n",
    "        Input('attention_heads', 'value'),\n",
    "        Input('embeddings', 'value'),\n",
    "        Input('notify', 'data'),\n",
    "    ],[\n",
    "        State('model_input', 'value'),\n",
    "        State('graph_id', 'data'),\n",
    "        State('run_config', 'data'),\n",
    "    ]\n",
    ")\n",
    "def update_scatter_plot(click_data, attention_head, embeddings, notify, prompt, graph_id, run_config):\n",
    "    global current_els\n",
    "    global current_head\n",
    "    global current_emb\n",
    "    global temp_edge_cache\n",
    "    if ctx.triggered_prop_ids:\n",
    "        _, sankey_traces, fig = model_output(prompt, graph_id, run_config)\n",
    "        # Update for new graph available (suppresses other updates)\n",
    "        if \"notify.data\" in ctx.triggered_prop_ids:\n",
    "            pass\n",
    "        # Update for changing attention head visualization\n",
    "        elif \"attention_heads.value\" in ctx.triggered_prop_ids:\n",
    "            print(\"TODO\")\n",
    "        # Update for changing embeddings visualization\n",
    "        elif \"embeddings.value\" in ctx.triggered_prop_ids:\n",
    "            current_emb = embeddings\n",
    "        else:\n",
    "            raise PreventUpdate\n",
    "        fig.add_trace(sankey_traces[current_emb])\n",
    "        return fig\n",
    "    raise PreventUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=\"True\", jupyter_mode=\"_none\", port=8050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T22:43:11.010994Z",
     "iopub.status.busy": "2024-02-09T22:43:11.009938Z",
     "iopub.status.idle": "2024-02-09T22:43:11.700202Z",
     "shell.execute_reply": "2024-02-09T22:43:11.699056Z",
     "shell.execute_reply.started": "2024-02-09T22:43:11.010934Z"
    }
   },
   "outputs": [],
   "source": [
    "#listener = await deploy_ngrok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:21:50.089555Z",
     "iopub.status.busy": "2024-02-09T19:21:50.089096Z",
     "iopub.status.idle": "2024-02-09T19:21:50.122357Z",
     "shell.execute_reply": "2024-02-09T19:21:50.121106Z",
     "shell.execute_reply.started": "2024-02-09T19:21:50.089519Z"
    }
   },
   "outputs": [],
   "source": [
    "# await listener.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4372269,
     "sourceId": 7507450,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
