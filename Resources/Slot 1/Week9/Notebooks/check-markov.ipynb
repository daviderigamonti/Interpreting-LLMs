{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "b1fa4217-533d-4680-b9df-71f9d9ee0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, AutoConfig, StoppingCriteriaList, StoppingCriteria\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import itertools\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05583d-cdbd-4eab-9e9a-3229a4a35374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869f8eb-8056-4985-9c1a-ee87e56b7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = \"\"\n",
    "if model_id in [\"meta-llama/Llama-2-7b-hf\"]:\n",
    "    hf_key = input(\"Hugging Face Key: \")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n",
    "model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n",
    "del hf_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7988e951-f069-4054-aa94-bff57ae3593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_id in [\"microsoft/phi-1_5\"]:\n",
    "    stopgen_tokens = [\n",
    "        torch.tensor([198, 198]),  # \\n\\n\n",
    "        torch.tensor([628])        # \\n\\n\n",
    "    ]\n",
    "    prompt_structure = \"Question: {prompt}\\n\\nAnswer:\"\n",
    "    exclude_token_offset = 3\n",
    "    fix_characters = [(\"Ġ\", \"␣\"), (\"Ċ\", \"\\n\")]\n",
    "elif model_id in [\"meta-llama/Llama-2-7b-hf\", \"mistralai/Mistral-7B-v0.1\"]:\n",
    "    stopgen_tokens = [\n",
    "        torch.tensor([1]),  # <s>\n",
    "        torch.tensor([2])   # </s>\n",
    "    ]\n",
    "    prompt_structure = \"{prompt}\"\n",
    "    exclude_token_offset = None\n",
    "    fix_characters = [(\"<0x0A>\", \"\\n\")]\n",
    "\n",
    "fix_characters += [(\"\\n\", \"\\\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e52cd059-eeb6-4e43-ad41-26557c21df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import urllib \n",
    "  \n",
    "def ngram_query(words, start_year=2018, end_year=2019, corpus=\"en-2019\", smoothing=0): \n",
    "    query = \",\".join([urllib.parse.quote(w) for w in words]) \n",
    "    # creating the URL \n",
    "    url = 'https://books.google.com/ngrams/json?content=' + query + '&year_start=' + str(start_year) + '&year_end=' +\\\n",
    "            str(end_year) + '&corpus=' + str(corpus) + '&smoothing=' + str(smoothing) + '' \n",
    "    # requesting data from the above url \n",
    "    response = requests.get(url) \n",
    "    # extracting the json data from the response we got \n",
    "    output = response.json() \n",
    "    # creating a list to store the ngram data \n",
    "    return_data = [] \n",
    "  \n",
    "    if len(output) == 0: \n",
    "        return None\n",
    "    else: \n",
    "        # if data returned from site, store the data in return_data list \n",
    "        for num in range(len(output)): \n",
    "            # getting the name \n",
    "            return_data.append((\n",
    "                output[num]['ngram'],\n",
    "                # getting ngram data \n",
    "                output[num]['timeseries']\n",
    "            ))\n",
    "    return return_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "79ddb149-d479-4515-8555-039119006da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_order_model(model):\n",
    "    input_emb = model.model.get_input_embeddings()\n",
    "    output_weights = model.lm_head.weight\n",
    "    output_bias = model.lm_head.bias\n",
    "    if output_bias is not None:\n",
    "        print(\"Warning, output bias not utilized\")\n",
    "        bias = 0\n",
    "    else:\n",
    "        bias = 0\n",
    "    output_emb = torch.nn.Embedding.from_pretrained(output_weights, freeze=True)\n",
    "    return input_emb, output_emb, bias\n",
    "\n",
    "def get_fom_prediction(fom, tokens):\n",
    "    in_emb, out_emb, out_bias = fom\n",
    "    all_logits = []\n",
    "    out_tokens = []\n",
    "    for token in tokens:\n",
    "        hidden = in_emb(token)\n",
    "        logits = torch.matmul(hidden, out_emb.weight.T) + out_bias\n",
    "        all_logits.append(logits)\n",
    "        out_tokens.append(logits.argmax(dim=-1))\n",
    "    return all_logits, out_tokens\n",
    "\n",
    "def check_2gram(fom, tokenizer, word, batch=500):\n",
    "    in_emb, _, _ = fom\n",
    "    # Workaround to prevent spaces from being trimmed\n",
    "    words = [word + tokenizer.decode([1, i])[3:] for i in range(in_emb.weight.size()[0])]\n",
    "    # Remove words that contain arithmetic symbols, Google Ngram separators or don't contain the original word\n",
    "    words = [w for w in words if word in w and all(c not in w for c in [\"/\", \"+\", \"-\", \"*\", \",\", \":\"])]\n",
    "\n",
    "    results = []\n",
    "    n_batches = len(words) // batch\n",
    "    n_residual = len(words) % batch\n",
    "\n",
    "    # Probability of original word\n",
    "    original_prob = ngram_query([word])\n",
    "    if original_prob and original_prob[0][-1][-1] > 0:\n",
    "        original_prob = original_prob[0][-1][-1]\n",
    "        for b in tqdm(range(0, n_batches)):\n",
    "            a = ngram_query(words[b*batch : (b+1)*batch])\n",
    "            results.extend(a)\n",
    "        if n_residual != 0:\n",
    "            results.extend(ngram_query(words[-n_residual:]))\n",
    "        # Print words that will be removed due to resulting in more than one token id\n",
    "        for k, v in results:\n",
    "            if len(tokenizer(k, add_special_tokens=False).input_ids[1:]) > 1:\n",
    "                print(k + ' ' + str(tokenizer(k, add_special_tokens=False).input_ids) + ' ' + str(v[-1] if v else 0))\n",
    "        # Create a dictionary containing the latest results and remove words that don't contain the original word or that only consist of the original word\n",
    "        # Normalize by original word probability\n",
    "        # Add token ids\n",
    "        results = {k: (tokenizer(k, add_special_tokens=False).input_ids[1], v[-1] / original_prob) for k, v in results if \n",
    "                   v and word.strip() != k and word.strip() in k and len(tokenizer(k, add_special_tokens=False).input_ids[1:]) == 1}\n",
    "        \n",
    "        return results\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d38d3794-a2ff-4329-ad1e-57cd68509bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [25:04<00:00, 19.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how . \" [910, 842, 345] 4.081721840520913e-07\n",
      "how = \" [910, 327, 345] 1.620947820413221e-09\n",
      "howng [295, 656, 28721] 0\n",
      "howning [295, 656, 288] 0\n",
      "how ? \" [910, 1550, 345] 7.020018415460072e-07\n",
      "howness [295, 656, 409] 5.169509265101624e-09\n",
      "hownot [295, 656, 322] 3.942846049653781e-10\n",
      "howno [295, 656, 28709] 0.0\n",
      "howna [295, 656, 28708] 0\n",
      "how \" ; [910, 345, 2753] 2.6285640331025206e-09\n",
      "how | | [910, 342, 342] 8.761880110341735e-11\n",
      "hownet [295, 656, 299] 2.6285640331025206e-10\n",
      "how ' ; [910, 464, 2753] 1.0514256132410083e-09\n",
      "how . ' [910, 842, 464] 6.38302992683748e-08\n",
      "how U.S. [910, 500, 28723, 28735, 28723] 7.46263140172232e-08\n",
      "how . \" [910, 842, 345] 4.081721840520913e-07\n",
      "how \" \" [910, 345, 345] 7.009504088273388e-10\n",
      "how \" . [910, 345, 842] 1.0470446731858374e-08\n",
      "how = ' [910, 327, 464] 4.9504622623430805e-09\n",
      "how_ . [910, 28730, 842] 1.2493245327149793e-10\n",
      "how ! \" [910, 918, 345] 4.3502733859668297e-08\n",
      "howns [295, 656, 28713] 0.0\n",
      "howni [295, 656, 28710] 0\n",
      "hownow [295, 656, 336] 4.380940055170868e-11\n",
      "how .... [910, 28705, 3406] 1.1202276439803427e-08\n",
      "how \" \" \" [910, 345, 345, 345] 4.6212152160496345e-11\n",
      "how can not [910, 541, 459] 6.0456972761357974e-09\n",
      "how ? ' [910, 1550, 464] 1.2560154516449984e-07\n",
      "how ' . [910, 464, 842] 2.251803188357826e-08\n",
      "hownes [295, 656, 274] 0\n",
      "hownon [295, 656, 266] 4.380940055170868e-11\n",
      "hownumber [295, 656, 1031] 0\n",
      "how ~ ~ [910, 5913, 5913] 4.380940055170868e-11\n",
      "how \\ \" [910, 414, 345] 4.380940055170868e-11\n",
      "how » . [910, 3488, 842] 8.761880110341735e-11\n",
      "how - \" [910, 387, 345] 6.527600504568909e-08\n",
      "how \" . [910, 345, 842] 1.0470446731858374e-08\n",
      "how \" \" [910, 345, 345] 7.009504088273388e-10\n",
      "how ... \" [910, 3850, 345] 5.068747555014852e-08\n",
      "how } \" [910, 443, 345] 4.380940055170868e-11\n",
      "how ! ' [910, 918, 464] 6.001887875584089e-09\n",
      "how | | [910, 342, 342] 8.761880110341735e-11\n",
      "how ........ [910, 28705, 10522] 1.6657659973606798e-10\n",
      "how ? \" [910, 1550, 345] 7.020018415460072e-07\n",
      "how \" # [910, 345, 422] 8.761880110341735e-11\n",
      "how \\ \" [910, 414, 345] 4.380940055170868e-11\n",
      "how \" ; [910, 345, 2753] 2.6285640331025206e-09\n",
      "how \" . [910, 345, 842] 1.0470446731858374e-08\n",
      "how ' # [910, 464, 422] 1.3142820165512603e-10\n",
      "how \\ ' [910, 414, 464] 0.0\n",
      "how \" ' [910, 345, 464] 3.373323842481568e-09\n",
      "how » . [910, 3488, 842] 8.761880110341735e-11\n",
      "how ' . [910, 464, 842] 2.251803188357826e-08\n",
      "how ' \" [910, 464, 345] 2.7599922347576467e-09\n",
      "how ! \" [910, 918, 345] 4.3502733859668297e-08\n",
      "how \" . [910, 345, 842] 1.0470446731858374e-08\n",
      "how . \" [910, 842, 345] 4.081721840520913e-07\n",
      "how \" ; [910, 345, 2753] 2.6285640331025206e-09\n",
      "how ' ; [910, 464, 2753] 1.0514256132410083e-09\n",
      "how \" -- [910, 345, 1939] 4.512368256825994e-09\n",
      "hownatural [295, 656, 270, 1890] 0\n",
      "how ' . [910, 464, 842] 2.251803188357826e-08\n",
      "how . ' [910, 842, 464] 6.38302992683748e-08\n",
      "how \" \" \" [910, 345, 345, 345] 4.6212152160496345e-11\n",
      "how ' ; [910, 464, 2753] 1.0514256132410083e-09\n",
      "how ~ ~ [910, 5913, 5913] 4.380940055170868e-11\n",
      "how \" . [910, 345, 842] 1.0470446731858374e-08\n",
      "how \" ; [910, 345, 2753] 2.6285640331025206e-09\n",
      "how .... [910, 28705, 3406] 1.1202276439803427e-08\n",
      "how \\ \" [910, 414, 345] 4.380940055170868e-11\n",
      "how ' \" [910, 464, 345] 2.7599922347576467e-09\n",
      "how ................ [910, 28705, 22207] 0.0\n",
      "hownice [295, 656, 535] 0\n",
      "how ...... [910, 28705, 3406, 568] 5.41373945672774e-10\n",
      "how ... ' [910, 3850, 464] 1.191615695006476e-08\n",
      "how '' ' [910, 9842, 464] 0.0\n",
      "how . \" [910, 842, 345] 4.081721840520913e-07\n",
      "how \" __ [910, 345, 1848] 4.380940055170868e-11\n",
      "how ; \" [910, 2753, 345] 1.226663215447843e-09\n",
      "how ..... [910, 8072, 1101] 7.912388522157698e-10\n",
      "how '' ' [910, 9842, 464] 0.0\n",
      "how\" & [910, 28739, 567] 0\n",
      "how \" ? [910, 345, 1550] 2.059041825930308e-09\n",
      "how . ' \" [910, 842, 464, 345] 2.4030319956125368e-09\n",
      "how \" ? [910, 345, 1550] 2.059041825930308e-09\n",
      "how ---- [910, 28705, 502] 9.578154935852012e-10\n",
      "how º [910, 28705, 28936] 3.3315319947213595e-10\n",
      "how ª [910, 28705, 28956] 4.1644149934016994e-11\n",
      "how ® [910, 28705, 28974] 0.0\n",
      "how ¡ [910, 28705, 29028] 4.1644149934016994e-11\n",
      "how ■ [910, 28705, 29094] 4.1644149240127604e-10\n",
      "how ● [910, 28705, 29393] 4.1644149240127604e-10\n",
      "how ► [910, 28705, 29413] 0.0\n",
      "how ¯ [910, 28705, 29948] 1.6657659973606798e-10\n",
      "how ¥ [910, 28705, 29986] 0.0\n",
      "how □ [910, 28705, 30618] 1.6657659973606798e-10\n",
      "tensor([[  1, 910]])\n",
      "('<s>', 'ocker')\n",
      "No data available for this Ngram.\n",
      "None\n",
      "('how', 'Pra')\n",
      "No data available for this Ngram.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get n-gram probabilities\n",
    "results_ngram = check_2gram(fom, tokenizer, \" how\", batch=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "df5c6061-c443-455b-af88-84fd3e757241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fom probabilities\n",
    "fom = get_first_order_model(model)\n",
    "token_ids = tokenizer(\"how\", return_tensors=\"pt\").input_ids\n",
    "logits_fom, outputs_fom = get_fom_prediction(fom, token_ids)\n",
    "\n",
    "paired_outputs = []\n",
    "for input_tok, output_tok in zip(token_ids[0], outputs[0]):\n",
    "    original_token = tokenizer.decode(input_tok)\n",
    "    fom_token = tokenizer.decode(output_tok.item())\n",
    "    paired_outputs.append((original_token, fom_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "8a4e740d-6c8d-4232-a4ea-ffb95469809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram\n",
      "'how to' P = 0.1313060090509322\n",
      "Corresponding FOM logits: 0.0004904341185465455\n",
      "--------------------\n",
      "FOM\n",
      "('<s>', 'ocker')\n",
      "Corresponding N-Gram probability: -\n",
      "('how', 'Pra')\n",
      "Corresponding N-Gram probability: -\n"
     ]
    }
   ],
   "source": [
    "# Print most likely tokens \n",
    "print(\"N-Gram\")\n",
    "max_ngram_k = max(results_ngram, key=lambda k: results_ngram[k][1])\n",
    "print(f\"'{max_ngram_k}' P = {results_ngram[max_ngram_k][1]}\")\n",
    "print(f\"Corresponding FOM logits: {logits_fom[0][1][results_ngram[max_ngram_k][0]].detach()}\") # Assuming that source word is at position 1 of logits\n",
    "print(\"--------------------\")\n",
    "print(\"FOM\")\n",
    "for out in paired_outputs:\n",
    "    print(out)\n",
    "    ngram_prob = ngram_query([\"\".join(out)])\n",
    "    print(f\"Corresponding N-Gram probability: {ngram_prob[0][-1][-1] if ngram_prob else '-'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "200d8806-7bbf-4c32-9b1a-25009bfde24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paired logits/probabilities to compare results\n",
    "values_ngram = []\n",
    "values_fom = []\n",
    "for k, v in results_ngram.items():\n",
    "    id, val = v\n",
    "    values_ngram.append(val)\n",
    "    values_fom.append(logits_fom[0][1][id].detach())  # Assuming that source word is at position 1 of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "8dc58e86-8d31-493a-acb7-3679951a782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=-0.01452817825887986, pvalue=0.12338516856542413)\n",
      "PearsonRResult(statistic=0.01366765436597835, pvalue=0.14721156467350693)\n",
      "SignificanceResult(statistic=-0.009783086174432727, pvalue=0.1223985314364765)\n"
     ]
    }
   ],
   "source": [
    "# Print statistics\n",
    "print(spearmanr(values_ngram, values_fom))\n",
    "print(pearsonr(values_ngram, values_fom))\n",
    "print(kendalltau(values_ngram, values_fom))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Thesis Kernel",
   "language": "python",
   "name": "nlp-thesis-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
