{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#!pip install ngrok -q\n","#!pip install dash -q\n","#!pip install \"dash[diskcache]\" -q"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/rigamonti/Thesis/.venv/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, AutoConfig, StoppingCriteriaList, StoppingCriteria\n","from collections import defaultdict\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","\n","import itertools\n","import torch"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:39:29.535319Z","iopub.status.busy":"2024-02-09T22:39:29.534785Z","iopub.status.idle":"2024-02-09T22:39:29.547547Z","shell.execute_reply":"2024-02-09T22:39:29.545783Z","shell.execute_reply.started":"2024-02-09T22:39:29.535286Z"},"trusted":true},"outputs":[],"source":["# https://github.com/oobabooga/text-generation-webui/blob/2cf711f35ec8453d8af818be631cb60447e759e2/modules/callbacks.py#L12\n","class _SentinelTokenStoppingCriteria(StoppingCriteria):\n","    def __init__(self, sentinel_token_ids: list, starting_idx: int):\n","        StoppingCriteria.__init__(self)\n","        self.sentinel_token_ids = sentinel_token_ids\n","        self.starting_idx = starting_idx\n","        self.shortest = min([x.shape[-1] for x in sentinel_token_ids])\n","\n","    def __call__(self, input_ids: torch.LongTensor, _scores: torch.FloatTensor) -> bool:\n","        for sample in input_ids:\n","            trimmed_sample = sample[self.starting_idx:]\n","            trimmed_len = trimmed_sample.shape[-1]\n","            if trimmed_len < self.shortest:\n","                continue\n","\n","            for sentinel in self.sentinel_token_ids:\n","                sentinel_len = sentinel.shape[-1]\n","                if trimmed_len < sentinel_len:\n","                    continue\n","\n","                window = trimmed_sample[-sentinel_len:]\n","                if torch.all(torch.eq(sentinel, window)):\n","                    return True\n","\n","        return False\n","####\n","\n","def generate_stopping_criteria(stopgen_tokens, input_len=0):\n","    return StoppingCriteriaList([\n","        _SentinelTokenStoppingCriteria(\n","            sentinel_token_ids = stopgen_tokens,\n","            starting_idx=input_len\n","        )\n","    ])\n"]},{"cell_type":"markdown","metadata":{},"source":["# CODE"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:40:20.011955Z","iopub.status.busy":"2024-02-09T22:40:20.010503Z","iopub.status.idle":"2024-02-09T22:40:20.019792Z","shell.execute_reply":"2024-02-09T22:40:20.017422Z","shell.execute_reply.started":"2024-02-09T22:40:20.011892Z"},"trusted":true},"outputs":[],"source":["#model_id = \"microsoft/phi-1_5\"\n","model_id = \"meta-llama/Llama-2-7b-hf\"\n","torch.set_default_device(\"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:40:20.084467Z","iopub.status.busy":"2024-02-09T22:40:20.083618Z","iopub.status.idle":"2024-02-09T22:43:10.082097Z","shell.execute_reply":"2024-02-09T22:43:10.078734Z","shell.execute_reply.started":"2024-02-09T22:40:20.084423Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/rigamonti/Thesis/.venv/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","/home/rigamonti/Thesis/.venv/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66f50a60319c4fe08b58f885b96e7fb9","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["hf_key = \"\"\n","if model_id in [\"meta-llama/Llama-2-7b-hf\"]:\n","    hf_key = input(\"Hugging Face Key: \")\n","tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n","model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n","model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True, token=hf_key)\n","del hf_key"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:43:10.086745Z","iopub.status.busy":"2024-02-09T22:43:10.085978Z","iopub.status.idle":"2024-02-09T22:43:10.102320Z","shell.execute_reply":"2024-02-09T22:43:10.100095Z","shell.execute_reply.started":"2024-02-09T22:43:10.086668Z"},"trusted":true},"outputs":[],"source":["if model_id in [\"microsoft/phi-1_5\"]:\n","    stopgen_tokens = [\n","        torch.tensor([198, 198]),  # \\n\\n\n","        torch.tensor([628])        # \\n\\n\n","    ]\n","    prompt_structure = \"Question: {prompt}\\n\\nAnswer:\"\n","    exclude_token_offset = 3\n","    fix_characters = [(\"Ġ\", \"␣\"), (\"Ċ\", \"\\n\")]\n","elif model_id in [\"meta-llama/Llama-2-7b-hf\"]:\n","    stopgen_tokens = [\n","        torch.tensor([1]),  # <s>\n","        torch.tensor([2])   # </s>\n","    ]\n","    prompt_structure = \"{prompt}\\n\"\n","    exclude_token_offset = 0\n","    fix_characters = []"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:45:38.311639Z","iopub.status.busy":"2024-02-09T22:45:38.311000Z","iopub.status.idle":"2024-02-09T22:45:38.337719Z","shell.execute_reply":"2024-02-09T22:45:38.336194Z","shell.execute_reply.started":"2024-02-09T22:45:38.311590Z"},"trusted":true},"outputs":[],"source":["### ALTERATION ### Divided computation for attentions\n","### ALTERATION ###  Added function to compute attentions also for prompt\n","\n","def pad_masked_attentions(attentions, max_len):\n","    \"\"\"\n","    Attention in generative models are masked, we want to plot a heatmap so we must pad all attentions to the same size with 0.0 values\n","    \"\"\"\n","    array_attentions = [np.array(att) for att in attentions]\n","    new_attentions = [np.concatenate([att, np.zeros([max_len - len(att)])]) for att in array_attentions]\n","    return np.array(new_attentions)\n","\n","def compute_complete_padded_attentions(generated_output, layer, head):\n","    single_layer_attentions = []\n","    # Prompt tokens\n","    for single_layer_single_head in torch.squeeze(torch.select(generated_output.attentions[0][layer], 1, head)):\n","        single_layer_attentions.append(single_layer_single_head)\n","    # Response tokens\n","    for attentions_per_token in generated_output.attentions[1:]:\n","        # Take single layer\n","        single_layer = attentions_per_token[layer]\n","        # Take only one head\n","        single_layer_single_head = torch.select(single_layer, 1, head)\n","        single_layer_attentions.append(single_layer_single_head)\n","    # Squeeze dimensions to one a one-dimensional tensor\n","    pure_attentions = [s.squeeze() for s in single_layer_attentions]\n","    max_seq_len  = len(pure_attentions[-1])\n","    # Print last attention heatmap\n","    padded_attentions = pad_masked_attentions(pure_attentions, max_seq_len)\n","    return padded_attentions\n","\n","def compute_batch_complete_padded_attentions(generated_output, heads):\n","    multi_layer_head_attentions = []\n","    for head in heads:\n","        multi_layer_attentions = []\n","        for layer in range(0, len(generated_output.attentions[0])):\n","            # Prompt tokens\n","            prompt_att = [\n","                torch.squeeze(single_head)\n","                for single_head in torch.squeeze(torch.select(generated_output.attentions[0][layer], 1, head))\n","            ]\n","            # Response tokens\n","            response_att = [\n","                torch.squeeze(torch.select(single_layer[layer], 1, head))\n","                for single_layer in generated_output.attentions[1:]\n","            ]\n","            # Pad and merge attentions\n","            multi_layer_attentions.append(pad_masked_attentions( \n","                [att_token for att_token in prompt_att + response_att],\n","                len(response_att[-1])\n","            ))\n","        multi_layer_head_attentions.append(multi_layer_attentions)\n","    return multi_layer_head_attentions\n","\n","def plot_attentions(generated_output, layer, head, generated_tokens, past_tokens):\n","    # Plot \n","    data = compute_padded_attentions(generated_output, layer, head)\n","    fig, ax = plt.subplots(figsize = (12,5))\n","    im = ax.imshow(data)\n","    # Show all ticks and label them with the respective list entries\n","    ax.set_yticks(np.arange(len(generated_tokens)), labels=generated_tokens)\n","    ax.set_xticks(np.arange(len(past_tokens)), labels=past_tokens, fontsize=8)\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","    # Create colorbar\n","    cbar = ax.figure.colorbar(im, ax=ax)\n","\n","    ax.set_title(f\"Heatmap of attention layers: layer {layer} head {head}\")\n","    fig.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:47:41.939600Z","iopub.status.busy":"2024-02-09T22:47:41.937490Z","iopub.status.idle":"2024-02-09T22:47:41.963466Z","shell.execute_reply":"2024-02-09T22:47:41.961338Z","shell.execute_reply.started":"2024-02-09T22:47:41.939531Z"},"trusted":true},"outputs":[],"source":["### ALTERATION ### Adapted functions to work outside model\n","### ALTERATION ### Added option to include hidden states for prompt and ending token in embed_hidden_states\n","def _apply_lm_head(model, hidden_states):\n","    \"\"\"\n","    Function which takes as input the hidden states of the model and returns the prediction of the next token.\n","    Uses the language modeling head of output\n","    \"\"\"\n","    pred_ids = []\n","    per_token_logits = []\n","    for i in range(len(hidden_states)):        \n","        logits = model.lm_head(hidden_states[i])\n","        logits = logits.float()\n","        pred_id = torch.argmax(logits)\n","        pred_ids.append(pred_id)\n","        per_token_logits.append(logits)\n","    return pred_ids, per_token_logits\n","\n","def _apply_input_lm_head(model, hidden_states):\n","    \"\"\"\n","    Function which takes as input the hidden states of the model and returns the prediction of the next token.\n","    Uses the language modeling head of input\n","    \"\"\"\n","    pred_ids = []\n","    per_token_logits = []\n","    for layer in hidden_states:\n","        output = torch.matmul(layer.to(model.model.embed_tokens.weight.device), model.model.embed_tokens.weight.T)\n","        token_id = output.argmax(dim=-1)\n","        pred_ids.append(token_id)\n","        per_token_logits.append(output)\n","    return pred_ids, per_token_logits\n","    \n","def embed_hidden_states(model, hidden_states, embedding=\"output\", include_prompt=False, include_end=True):\n","    if embedding not in ['input', 'output']:\n","        raise ValueError(\"Embedding not valid\")\n","\n","    end_idx = len(hidden_states) if include_end else len(hidden_states) - 1\n","\n","    predictions = []\n","    # Prompt tokens\n","    if include_prompt:\n","        for token_states in torch.stack(hidden_states[0]).swapaxes(0, 2):\n","            if embedding == 'output':\n","                pred_ids, per_token_logits = _apply_lm_head(model, token_states.swapaxes(0, 1))\n","            else:\n","                pred_ids, per_token_logits = _apply_input_lm_head(model, token_states.swapaxes(0, 1))\n","            predictions.append([int(id) for id in pred_ids])\n","    # Response tokens\n","    for n_token in range(1, end_idx):\n","        if embedding == 'output':\n","            pred_ids, per_token_logits = _apply_lm_head(model, hidden_states[n_token])\n","        else:\n","            pred_ids, per_token_logits = _apply_input_lm_head(model, hidden_states[n_token])\n","        predictions.append([int(id) for id in pred_ids])\n","    return predictions"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:45:41.010042Z","iopub.status.busy":"2024-02-09T22:45:41.008576Z","iopub.status.idle":"2024-02-09T22:45:41.018179Z","shell.execute_reply":"2024-02-09T22:45:41.016771Z","shell.execute_reply.started":"2024-02-09T22:45:41.009985Z"},"trusted":true},"outputs":[],"source":["def fix_dataframe_characters(df, replacements, columns=False):\n","    for old, new in replacements:\n","        df = df.applymap(lambda x: x.replace(old, new))\n","    if columns:\n","        for old, new in replacements:\n","            df.columns = df.columns.str.replace(old, new)\n","    return df"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:43:10.199253Z","iopub.status.busy":"2024-02-09T22:43:10.196471Z","iopub.status.idle":"2024-02-09T22:43:10.291595Z","shell.execute_reply":"2024-02-09T22:43:10.288408Z","shell.execute_reply.started":"2024-02-09T22:43:10.199043Z"},"trusted":true},"outputs":[],"source":["delta_point = 0.32\n","delta_marker = 0.22\n","\n","def gen_edges(\n","    edges, permanent_edges, attentions,\n","    generated_output, \n","    nodexs, nodeys, \n","    head, \n","    exclude, \n","    threshold, permanent_threshold,\n","):\n","    # Cycle through every layer of the model, gathering the aggregated coordinates for each node in a layer\n","    for idx, coords in enumerate(zip(nodexs, nodeys)):\n","        xs, ys = coords\n","\n","        # Do not plot attention traces for the starting layer\n","        if idx != 0:\n","            # Compute the attention weights for the current layer and head\n","            attentions_lh = attentions[idx - 1]\n","\n","            # Cycle through every node in the layer, gathering its coordinates \n","            for i, c in enumerate(zip(xs, ys)):\n","                x, y = c\n","\n","                # Cycle through every node in the PREVIOUS layer w.r.t. the current one, gathering its coordinates \n","                for ii, cc in enumerate(zip(nodexs[idx - 1], nodeys[idx - 1])):\n","                    xx, yy = cc\n","                    weight = attentions_lh[ii][i]\n","                    \n","                    if x not in exclude and weight >= threshold:\n","                        if weight >= permanent_threshold:\n","                            # Create single edge representing the attention weigth\n","                            permanent_edges[head].append(go.Scattergl(\n","                                x=[x, (x+xx)/2, xx],\n","                                y=[y + delta_point, (y + yy - delta_marker + delta_point)/2, yy - delta_marker],\n","                                name=\"pedge\",\n","                                mode=\"markers+lines\",\n","                                marker=dict(size=[8, 0, 4], color=\"black\", opacity=[1, 0, 1]),\n","                                marker_symbol=[6, 0, 1],\n","                                marker_line_width=0,\n","                                text=f\"{str(weight)}\",\n","                                hoverinfo=[\"none\", \"text\", \"none\"],\n","                                line=dict(color=\"rgba(125,125,125,0.7)\", width=2 * weight + 0.25),\n","                                customdata=[{\"type\": \"edge\", \"P1\": {\"x\": x, \"y\": y}, \"P2\": {\"x\": xx, \"y\":yy}}],\n","                                showlegend=False,\n","                            ))\n","                        else:\n","                            # Create single edge representing the attention weigth\n","                            edges[head].append(go.Scattergl(\n","                                x=[x, (x+xx)/2, xx],\n","                                y=[y + delta_point, (y + yy - delta_marker + delta_point)/2, yy - delta_marker],\n","                                name=\"edge\",\n","                                mode=\"markers+lines\",\n","                                marker=dict(size=[8, 0, 4], color=\"black\", opacity=[1, 0, 1]),\n","                                marker_symbol=[6, 0, 1],\n","                                marker_line_width=0,\n","                                text=f\"{str(weight)}\",\n","                                hoverinfo=[\"none\", \"text\", \"none\"],\n","                                line=dict(color=\"rgba(125,125,125,0.7)\", width=2 * weight + 0.25),\n","                                customdata=[{\"type\": \"edge\", \"P1\": {\"x\": x, \"y\": y}, \"P2\": {\"x\": xx, \"y\":yy}}],\n","                                showlegend=False,\n","                            ))\n","\n","def create_transformer_plot(dfs, generated_output, exclude=[], heads=range(0, 1), max_heads=32, threshold=0.002, permanent_threshold=0.4):\n","    nodes = {key: [] for key in dfs.keys()}\n","    edges = {head: [] for head in heads}\n","    permanent_edges = {head: [] for head in heads}\n","\n","    attentions = compute_batch_complete_padded_attentions(generated_output, range(0, max_heads))\n","    \n","    for key, df in dfs.items():\n","        \n","        nodexs = []\n","        nodeys = []\n","\n","        # Cycle through every layer of the model, gathering all blocks as nodes\n","        for idx, row in df.iterrows():\n","\n","                # Generate coordinates for nodes\n","                xs = [i for i in range(len(row))]\n","                ys = [idx] * len(row)\n","\n","                nodexs.append(xs)\n","                nodeys.append(ys)\n","\n","                for x, y in zip(xs, ys):\n","\n","                    color = \"lightblue\"\n","                    if x in exclude:\n","                        color = \"red\"\n","\n","                    # Create nodes\n","                    nodes[key].append(go.Scattergl(\n","                        x=[x],\n","                        y=[y],\n","                        name=\"node\",\n","                        mode=\"markers+text\",\n","                        marker=dict(size=22, color=color, opacity=0.5),\n","                        marker_line_width=0,\n","                        marker_symbol=1,\n","                        text=row[x],\n","                        textfont={\"size\": 11},\n","                        textposition=\"middle center\",\n","                        hoverinfo=\"none\",\n","                        customdata=[{\"type\":\"node\"}],\n","                        showlegend=False\n","                    ))\n","\n","    for head in heads:\n","        att = attentions[head]\n","        if head == -1:\n","            att = np.mean(attentions, axis=0)\n","        gen_edges(\n","            edges, permanent_edges, attentions[head],\n","            generated_output, \n","            nodexs, nodeys, \n","            head, \n","            exclude, \n","            threshold, permanent_threshold\n","        )\n","\n","    # Create figure\n","    fig = go.Figure(data=[])\n","\n","    # Customize layout\n","    fig.update_layout(\n","        title=\"Transformer Weights Visualization\",\n","        showlegend=True,\n","        xaxis=dict(showticklabels=False, zeroline=False),\n","        yaxis=dict(showticklabels=False, zeroline=False),\n","        plot_bgcolor='white',\n","        width=1600, height=1000 + (len(list(dfs.values())[0]) * 30),\n","        uirevision=\"const\"\n","    )\n","    \n","    return go.FigureWidget(fig), nodes, (edges, permanent_edges)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:43:10.298262Z","iopub.status.busy":"2024-02-09T22:43:10.295973Z","iopub.status.idle":"2024-02-09T22:43:10.322460Z","shell.execute_reply":"2024-02-09T22:43:10.320953Z","shell.execute_reply.started":"2024-02-09T22:43:10.298196Z"},"trusted":true},"outputs":[],"source":["def compute_edge_precache(nodes, edges, pedges, heads):\n","    return {\n","        (x,y):  { \n","            head: {\n","                \"edges\": traces,\n","                \"color_nodes\": {\n","                    emb: compute_color_nodes(emb_nodes, traces + pedges[head], x, y) \n","                    for emb, emb_nodes in nodes.items()\n","                }\n","            } for head in heads if (traces := compute_add_traces(edges[head], x, y)) or True\n","        }\n","        for x,y in [(x,y) for node_coords in [zip(node_batch.x, node_batch.y) for node_batch in list(nodes.values())[0]] for x,y in node_coords]\n","    }\n","\n","def access_edge_cache(cache, x, y, attention_head, embedding, color_scale):\n","    add_traces = cache[(x,y)][attention_head][\"edges\"]\n","    add_color_nodes = cache[(x,y)][attention_head][\"color_nodes\"][embedding]\n","    add_color_nodes = [apply_color(node, trace, color_scale) for node, trace in add_color_nodes]\n","    return add_traces, add_color_nodes\n","\n","def compute_add_traces(edges, x, y):\n","    return [\n","        el for el in edges if (\n","            el.customdata[0][\"P1\"][\"x\"] == x and el.customdata[0][\"P1\"][\"y\"] == y\n","        ) or (\n","            el.customdata[0][\"P2\"][\"x\"] == x and el.customdata[0][\"P2\"][\"y\"] == y\n","        )\n","    ]\n","\n","def compute_color_nodes(nodes, add_traces, x, y):\n","    return [\n","        (node, trace)\n","        for trace in add_traces for node in filter(lambda n: n.y[0] == y - 1 and n.x[0] <= x, nodes) \n","        if trace.customdata[0][\"P1\"][\"x\"] in node.x and trace.customdata[0][\"P1\"][\"y\"] in node.y and\n","            trace.customdata[0][\"P2\"][\"x\"] == x and trace.customdata[0][\"P2\"][\"y\"] == y\n","    ]\n","\n","def apply_color(node, trace, color_scale):\n","    node[\"marker\"][\"color\"] = pc.sample_colorscale(color_scale, float(trace.text))\n","    return node"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:43:10.326230Z","iopub.status.busy":"2024-02-09T22:43:10.324597Z","iopub.status.idle":"2024-02-09T22:43:10.360414Z","shell.execute_reply":"2024-02-09T22:43:10.357667Z","shell.execute_reply.started":"2024-02-09T22:43:10.326168Z"},"trusted":true},"outputs":[],"source":["def model_generate(model, tokenizer, prompt, max_extra_length, config, min_stop_length, stopping_tokens):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    input_len = len(inputs.input_ids.squeeze().tolist())\n","    max_len = input_len + max_extra_length\n","    \n","    gen_config = config\n","    stopping_criteria = generate_stopping_criteria(stopping_tokens, input_len + min_stop_length)\n","    \n","    generated_output = model.generate(inputs.input_ids, generation_config=gen_config, max_length=max_len, stopping_criteria=stopping_criteria)\n","    \n","    text_output = tokenizer.decode(generated_output.sequences.squeeze()[input_len:])\n","    \n","    all_tokens = tokenizer.convert_ids_to_tokens(generated_output.sequences[0])\n","    input_tokens = all_tokens[0:input_len]\n","    generated_tokens = all_tokens[input_len:]\n","    \n","    return text_output, generated_output, {\"in\": input_tokens, \"gen\": generated_tokens}\n","\n","def create_hidden_states_df(model, tokenizer, generated_output, gen_tokens, embedding, include_prompt, fix_characters):\n","    predictions = embed_hidden_states(model, generated_output.hidden_states, embedding, include_prompt=include_prompt)\n","    rows = [tokenizer.convert_ids_to_tokens(pred) for pred in predictions]\n","    df = pd.DataFrame(rows).T.sort_index(ascending=False).rename(columns={n: col for n, col in enumerate(gen_tokens[\"in\"] + gen_tokens[\"gen\"])})\n","    df = fix_dataframe_characters(df, fix_characters, columns=True)\n","    return df\n","\n","def create_attention_visualization(dfs, generated_output, exclude, heads, max_heads, compute_precache=True):\n","    figure, nodes, edges = create_transformer_plot(dfs, generated_output, exclude, heads=heads, max_heads=max_heads)\n","    edges, permanent_edges = edges\n","    permanent_traces = {head: {\"edges\": permanent_edges[head]} for head in heads}\n","    edge_precache = compute_edge_precache(nodes, edges, permanent_edges, heads) if compute_precache else None\n","    return figure, {\"nodes\": nodes, \"edges\": edges, \"perm\": permanent_traces}, edge_precache"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:43:10.363344Z","iopub.status.busy":"2024-02-09T22:43:10.362892Z","iopub.status.idle":"2024-02-09T22:43:10.799484Z","shell.execute_reply":"2024-02-09T22:43:10.798185Z","shell.execute_reply.started":"2024-02-09T22:43:10.363306Z"},"trusted":true},"outputs":[],"source":["#import ngrok\n","import asyncio\n","import dash\n","import diskcache\n","import uuid\n","\n","#from kaggle_secrets import UserSecretsClient\n","\n","from dash import dcc, html, ctx, Patch, DiskcacheManager\n","from dash.dependencies import Input, Output, State\n","from dash.exceptions import PreventUpdate\n","\n","import dash_daq as daq\n","import plotly.colors as pc\n","\n","import plotly.graph_objects as go"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:43:10.802260Z","iopub.status.busy":"2024-02-09T22:43:10.801166Z","iopub.status.idle":"2024-02-09T22:43:10.809441Z","shell.execute_reply":"2024-02-09T22:43:10.808077Z","shell.execute_reply.started":"2024-02-09T22:43:10.802224Z"},"trusted":true},"outputs":[],"source":["async def deploy_ngrok(address=\"http://127.0.0.1:8050\"):\n","    listener = ngrok.forward(addr=address, authtoken=UserSecretsClient().get_secret(\"ngrok_key\"))\n","    await asyncio.wait_for(listener, timeout=10)\n","    public_url = listener.result().url()\n","    print(f\"Deploy URL: {public_url}\")\n","    return listener.result()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:47:49.202853Z","iopub.status.busy":"2024-02-09T22:47:49.202382Z","iopub.status.idle":"2024-02-09T22:47:49.260494Z","shell.execute_reply":"2024-02-09T22:47:49.258874Z","shell.execute_reply.started":"2024-02-09T22:47:49.202819Z"},"trusted":true},"outputs":[],"source":["color_scale = pc.sequential.Viridis\n","\n","current_els = []\n","current_head = -1\n","current_emb = \"output\"\n","\n","temp_edge_cache = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n","\n","cache = diskcache.Cache(\"./cache\")\n","background_callback_manager = DiskcacheManager(cache)\n","app = dash.Dash(\"Test\")\n","\n","# Define the layout of the app\n","app.layout = html.Div([\n","    html.Div([\n","        html.Div([\n","            dcc.Textarea(id='model_input', placeholder ='Insert prompt...', style={'width': '100%', 'height': \"50%\"}),\n","            dcc.Loading(id=\"model_loading\", type=\"dot\", color=\"#873ba1\", children =\n","                dcc.Textarea(id='model_output', readOnly=True, style={'width': '100%', 'height': \"50%\"})\n","            )\n","        ], style={\"float\": \"left\", \"width\": \"70%\", \"height\": \"100%\", \"padding\": 2}),\n","        html.Div([\n","            html.P(children=\"# of attention heads to load\", style={\"margin\": \"0\"}),\n","            dcc.Dropdown([{\"value\": i, \"label\": i+1 if i >= 0 else \"Average\"} for i in range(-1, model_config.num_attention_heads)], id='model_generate_heads', value=-1, clearable=False),\n","            daq.NumericInput(id=\"min_stop_tokens\", label=\"Min # tokens for stopping criteria\", value=1, min=0, max=1024, labelPosition=\"right\"),\n","            daq.NumericInput(id=\"max_new_tokens\", label=\"Max # of generated tokens\", value=10, min=0, max=1024, labelPosition=\"right\"),\n","            dcc.Checklist([\"Precache Results\"], id=\"precache_results\", inline=True),\n","            html.Button('Generate', id='model_generate', style={\"width\": \"100%\", \"height\": \"20px\"}),\n","        ], style={\"float\": \"right\", \"height\": \"100%\", \"width\": \"20%\", \"padding\": 2}),\n","    ], style={\"height\": \"140px\"}),\n","    html.Div([\n","        html.P(\"Attention Head Selector\"),\n","        dcc.Slider(marks={}, step=1, value=-1, id='attention_heads'),\n","        html.P(\"Embeddings Selector\"),\n","        dcc.RadioItems(['input', 'output'], value='output', inline=True, id='embeddings'),\n","        dcc.Graph(id='scatterplot'),\n","    ]),\n","    dcc.Store(id=\"run_config\"),\n","    dcc.Store(id=\"notify\"),\n","    dcc.Store(id=\"graph_id\")\n","])\n","\n","# Aggregate run parameters data\n","@app.callback(\n","    Output('run_config', 'data'),\n","    [\n","        Input('model_generate_heads', 'value'),\n","        Input('min_stop_tokens', 'value'),\n","        Input('max_new_tokens', 'value'),\n","        Input('precache_results', 'value')\n","    ]\n",")\n","def update_run_config(gen_heads, min_stop_tokens, max_new_tok, precache):\n","    return {\n","        \"gen_heads\": gen_heads,\n","        \"min_stop_tokens\": min_stop_tokens,\n","        \"max_new_tok\": max_new_tok,\n","        \"precache\": len(precache) if precache else None\n","    }\n","\n","\n","@cache.memoize()\n","def model_output(prompt, session, run_config):\n","    prompt = prompt_structure.format(prompt=prompt)\n","    gen_config = GenerationConfig(output_attentions=True, output_hidden_states=True, return_dict_in_generate=True)\n","    text_output, generated_output, gen_tokens = model_generate(\n","            model, tokenizer, prompt, \n","            max_extra_length=run_config[\"max_new_tok\"], \n","            config=gen_config, \n","            min_stop_length=run_config[\"min_stop_tokens\"], stopping_tokens=stopgen_tokens\n","    )   \n","    dfs = {}\n","    for emb in [\"input\", \"output\"]:\n","        dfs[emb] = create_hidden_states_df(\n","            model, tokenizer, generated_output, gen_tokens, emb, \n","            include_prompt=True, fix_characters=fix_characters\n","        )\n","    exclude_prompt_returns = len(gen_tokens[\"in\"]) - exclude_token_offset - 1\n","    figure, fig_els, edge_precache = create_attention_visualization(\n","        dfs, generated_output, \n","        exclude=[0, exclude_prompt_returns], \n","        heads=range(-1, run_config[\"gen_heads\"]+1), max_heads=model_config.num_attention_heads, \n","        compute_precache=run_config[\"precache\"],\n","    )\n","    return text_output, fig_els, edge_precache, figure\n","\n","# Define callback to generate output\n","@app.callback(\n","    [\n","        Output('model_output', 'value'),\n","        Output('scatterplot', 'figure', allow_duplicate=True),\n","        Output('attention_heads', 'marks'),\n","        Output('attention_heads', 'value'),\n","        Output('graph_id', 'data'),\n","        Output(\"notify\", \"data\"),\n","    ],\n","    Input('model_generate', 'n_clicks'),\n","    [\n","        State('model_input', 'value'),\n","        State('run_config', 'data'),\n","    ],\n","    running=[(Output(\"model_generate\", \"disabled\"), True, False)],\n","    prevent_initial_call=True,\n","    background=True,\n","    manager=background_callback_manager\n",")\n","def update_model_generation(click_data, prompt, run_config):\n","    if ctx.triggered_prop_ids:\n","        graph_id = str(uuid.uuid4())\n","        slider_marks = {i: f\"Head {i}\" for i in range(0, run_config[\"gen_heads\"] + 1)}\n","        slider_marks.update({-1: \"AVG\"})\n","        text_output, fig_els, _, figure = model_output(prompt, graph_id, run_config)\n","        return text_output, figure, slider_marks, -1, graph_id, True\n","    raise PreventUpdate\n","\n","#Define callback to update scatter plot\n","@app.callback(\n","    Output('scatterplot', 'figure'),\n","    [\n","        Input('scatterplot', 'clickData'),\n","        Input('attention_heads', 'value'),\n","        Input('embeddings', 'value'),\n","        Input('notify', 'data'),\n","    ],[\n","        State('model_input', 'value'),\n","        State('graph_id', 'data'),\n","        State('run_config', 'data'),\n","    ]\n",")\n","def update_scatter_plot(click_data, attention_head, embeddings, notify, prompt, graph_id, run_config):\n","    global current_els\n","    global current_head\n","    global current_emb\n","    global temp_edge_cache\n","    if ctx.triggered_prop_ids:\n","        _, fig_els, edge_precache, fig = model_output(prompt, graph_id, run_config)\n","        p = Patch()\n","        # Update for new graph available (suppresses other updates)\n","        if \"notify.data\" in ctx.triggered_prop_ids:\n","            default_emb = \"output\"\n","            add_traces = (fig_els[\"nodes\"][default_emb] + fig_els[\"perm\"][attention_head][\"edges\"])\n","            temp_edge_cache = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n","        # Update for hovering over a plot node\n","        elif \"scatterplot.clickData\" in ctx.triggered_prop_ids and click_data and \"customdata\" in click_data[\"points\"][0] and click_data[\"points\"][0][\"customdata\"][\"type\"] == \"node\":\n","            x = click_data['points'][0]['x']\n","            y = click_data['points'][0]['y']\n","            \n","            # Check if all results are precached\n","            if edge_precache:\n","                add_edges, add_color_nodes = access_edge_cache(edge_precache, x, y, current_head, current_emb, color_scale)\n","            # Check if result is in temporary cache\n","            elif (x,y) in temp_edge_cache and attention_head in temp_edge_cache[(x,y)] and current_head in temp_edge_cache[(x,y)][\"color_nodes\"]:\n","                add_edges, add_color_nodes = access_edge_cache(temp_edge_cache, x, y, current_head, current_emb, color_scale)\n","            # Temporary cache miss\n","            else:\n","                add_edges = compute_add_traces(fig_els[\"edges\"][current_head], x, y)\n","                add_color_nodes = compute_color_nodes(\n","                    fig_els[\"nodes\"][current_emb], add_edges + fig_els[\"perm\"][current_head][\"edges\"], x, y\n","                )\n","                temp_edge_cache[(x,y)][current_head][\"edges\"] = add_edges\n","                temp_edge_cache[(x,y)][current_head][\"color_nodes\"][current_emb] = add_color_nodes\n","                add_color_nodes = [apply_color(node, trace, color_scale) for node, trace in add_color_nodes]\n","\n","            _ = [p[\"data\"].remove(el) for el in current_els + add_color_nodes]\n","            \n","            add_traces = add_edges + add_color_nodes\n","            current_els = add_traces \n","        # Update for changing attention head visualization\n","        elif \"attention_heads.value\" in ctx.triggered_prop_ids:\n","            _ = [p[\"data\"].remove(el) for el in fig_els[\"perm\"][current_head][\"edges\"] + current_els]\n","            add_traces = fig_els[\"perm\"][attention_head][\"edges\"]\n","            current_head = attention_head\n","            current_els = []\n","        # Update for changing embeddings visualization\n","        elif \"embeddings.value\" in ctx.triggered_prop_ids:\n","            _ = [p[\"data\"].remove(el) for el in fig_els[\"nodes\"][current_emb] + current_els]\n","            add_traces = fig_els[\"nodes\"][embeddings]\n","            current_emb = embeddings\n","            current_els = []\n","        else:\n","            raise PreventUpdate\n","        p[\"data\"].extend(add_traces)\n","        return p\n","    raise PreventUpdate"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:47:49.929074Z","iopub.status.busy":"2024-02-09T22:47:49.927642Z","iopub.status.idle":"2024-02-09T22:47:50.153947Z","shell.execute_reply":"2024-02-09T22:47:50.152954Z","shell.execute_reply.started":"2024-02-09T22:47:49.929017Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"]}],"source":["# Run the app\n","if __name__ == '__main__':\n","    app.run(debug=True, jupyter_mode=\"_none\", port=8050)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T22:43:11.010994Z","iopub.status.busy":"2024-02-09T22:43:11.009938Z","iopub.status.idle":"2024-02-09T22:43:11.700202Z","shell.execute_reply":"2024-02-09T22:43:11.699056Z","shell.execute_reply.started":"2024-02-09T22:43:11.010934Z"},"trusted":true},"outputs":[],"source":["#listener = await deploy_ngrok()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-09T19:21:50.089555Z","iopub.status.busy":"2024-02-09T19:21:50.089096Z","iopub.status.idle":"2024-02-09T19:21:50.122357Z","shell.execute_reply":"2024-02-09T19:21:50.121106Z","shell.execute_reply.started":"2024-02-09T19:21:50.089519Z"},"trusted":true},"outputs":[],"source":["# await listener.close()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4372269,"sourceId":7507450,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
