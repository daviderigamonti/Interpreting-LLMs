# Previous Meeting Ideas

## 22/02/24 Group Meeting

### Ideas + Tasks

- Try using Mistral7B model

#### Head Attention Contribution

- Cosine Similarity may not be the optimal way to measure the contribution of each head.

- Variance in length of the vectors could be an interesting measure.

- "Forces inputs" are handled differently than tokens generated by the model, and watching attention contributions of the input sentence may be misleading.

- Accumulate more examples to build more statistical significance over observations.

#### Hidden Representation Embedding Multi-Representation

- Try normalizing hidden representations using the model's normalization technique, rather than normalizing them to simply have norm 1.

- Try Elvis musician test and see which "Elvises" are predicted.

#### Attention Interactive Visualization

- Try changing the background of the text directly in the visualization, instead of using square markers.

- Give options to remove thresholds, eventually playing around with minimum and maximum width in order to avoid cluttering the visualization.

- Start working with Nicol√≤ to incorporate visualizations:
    - Refer to the mockup
    - Show both attention, FF and residual contributions

- Final idea is, given any token inside any layer, to make possible to trace back from where it was formed inside the model. To this end, it is only relevant to weight each contribution (attention, attention weights, FF, residuals, ...) functionally to how it contributes to the current token that is being examined.