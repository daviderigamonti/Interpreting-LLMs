{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps as clmp\n",
    "from matplotlib.ticker import AutoMinorLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noadd = lambda x: x\n",
    "addspace = lambda x: \" \" + x\n",
    "addall = lambda x: (x.capitalize(), \" \" + x.capitalize(), x.lower(), \" \" + x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\"gpt2\", \"meta-llama/Llama-2-7b-hf\", \"mistralai/Mistral-7B-v0.1\", \"google/gemma-7b\"]\n",
    "model_names = [ \"GPT 2\", \"LLaMa 2\", \"Mistral\", \"Gemma\"]\n",
    "model_format = [addspace, noadd, noadd, addspace]\n",
    "need_key = [ \"meta-llama/Llama-2-7b-hf\", \"google/gemma-7b\" ]\n",
    "device = \"cuda\"\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model_ids, need_key_list):\n",
    "    tokenizers = []\n",
    "    in_embeddings = []\n",
    "    out_embeddings = []\n",
    "    for model_id, model_name in zip(model_ids, model_names):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        hf_key=None\n",
    "        if model_id in need_key_list:\n",
    "            hf_key = input(\"Hugging Face Key: \")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, token=hf_key, torch_dtype=torch.float16)\n",
    "        tokenizers.append(AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=hf_key))\n",
    "        if hf_key:\n",
    "            del hf_key\n",
    "        in_embeddings.append(model.get_input_embeddings())\n",
    "        out_embeddings.append(get_output_embeddings(model))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"{torch.cuda.memory_allocated(0) / 1024**2} ({torch.cuda.memory_reserved(0) / 1024**2}) / {torch.cuda.get_device_properties(0).total_memory / 1024**2}\")\n",
    "    return tokenizers, in_embeddings, out_embeddings\n",
    "\n",
    "def get_output_embeddings(model):\n",
    "    weights = model.lm_head.weight\n",
    "    bias = model.lm_head.bias\n",
    "    if bias is None:\n",
    "        bias = 0\n",
    "    else:\n",
    "        print(\"Warning, bias not utilized\")\n",
    "    return torch.nn.Embedding.from_pretrained(weights, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers, in_emb, out_emb = extract_embeddings(model_ids, need_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for in_emb_mod, out_emb_mod in zip(in_emb, out_emb):\n",
    "#    in_emb_mod.to(\"cuda\")\n",
    "#    out_emb_mod.to(\"cuda\")\n",
    "#torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiencode(tok, words):\n",
    "    if (isinstance(words, list) or isinstance(words, tuple)) and not isinstance(words, str):\n",
    "        # Encode a list of words\n",
    "        return torch.cat([tok.encode(word, return_tensors=\"pt\", add_special_tokens=False) for word in words], dim=-1)\n",
    "    else:\n",
    "        # Encode a single word\n",
    "        return tok.encode(words, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    \n",
    "def avgencode(emb, word, tok=None, avg=True):\n",
    "    source = word\n",
    "    # If input is a string tokenize it\n",
    "    if (isinstance(word, str) or isinstance(word[0], str)) and tok is not None:\n",
    "        word = emb(multiencode(tok, word))\n",
    "    # Calculate average if avg flag is true and if it is needed\n",
    "    if word.shape[1] != 1 and avg:\n",
    "        word = torch.unsqueeze(torch.mean(word, dim=1), dim=1)\n",
    "    elif word.shape[1] != 1 and not avg:\n",
    "        raise Exception(f\"{source} is not a single token: {word}\")\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(emb, word1, word2, tok=None, avg=True, dist=\"cosine\", multi=False):\n",
    "    # Encode and average (if multi is True, word1 represents the embedding matrix)\n",
    "    if not multi:\n",
    "        word1 = avgencode(emb, word1, tok, avg=avg)\n",
    "    word2 = avgencode(emb, word2, tok, avg=avg)\n",
    "    # Compute distances\n",
    "    if dist == \"L2\":\n",
    "        distances = torch.norm(word1 - word2, dim=2)\n",
    "    elif dist == \"cosine\":\n",
    "        cs = torch.nn.CosineSimilarity(dim=2)\n",
    "        distances = 1 - cs(word1, word2)\n",
    "    else:\n",
    "        raise Exception(\"Unknown distance\")\n",
    "    return distances\n",
    "\n",
    "def get_closest_emb(emb, word, k=1, decode=True, tok=None, avg=True, dist=\"cosine\"):\n",
    "    # Compute distances from matrix\n",
    "    distances = calc_distance(emb, emb.weight.data, word, tok=tok, avg=avg, dist=dist, multi=True)\n",
    "    # Compute top k smalles indices\n",
    "    topk = torch.squeeze(torch.topk(distances, k=k, largest=False).indices)\n",
    "    # If one element, unsqueeze it\n",
    "    if k == 1:\n",
    "        topk = torch.unsqueeze(topk, dim=0)\n",
    "    # Decode closest k\n",
    "    if decode and tok is not None:\n",
    "        topk = [tok.decode(c) for c in topk.tolist()]\n",
    "    return topk\n",
    "\n",
    "def emb_arithmetic(emb, tok, words, k=1, dist=\"cosine\"):\n",
    "    # Compute embeddings\n",
    "    w1 = words[0]\n",
    "    w2 = words[1]\n",
    "    w3 = words[2] if len(words) > 2 else 0\n",
    "    # Do embedding arithmetic\n",
    "    if dist == \"L2\":\n",
    "        w = torch.nn.functional.normalize(w1 - w2 + w3, dim=0)\n",
    "    else:\n",
    "        w = w1 - w2 + w3\n",
    "    # Get closest k\n",
    "    closest = get_closest_emb(emb, w, k=k, decode=True, tok=tok, dist=dist)\n",
    "    return (w, closest)\n",
    "\n",
    "def print_results(res):\n",
    "     for i, r in enumerate(res):\n",
    "        print(f\"{i+1}) {repr(r)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_emb_arithmetic(emb, tok, queries, k=5, avg=True, dist=\"cosine\", out=True):\n",
    "    ret = []\n",
    "    for q in queries:\n",
    "        if out:\n",
    "            print(\"##########################\")\n",
    "            # Print title\n",
    "            title_q = q\n",
    "            if not isinstance(q[0], str):\n",
    "                title_q = [qq[0] for qq in q]\n",
    "            print(f\"{title_q[0]} - {title_q[1]} + {title_q[2]} =\")\n",
    "        # Compute and print results\n",
    "        res = emb_arithmetic(emb, tok, [avgencode(emb, word, tok, avg=avg) for word in q], k=k, dist=dist)\n",
    "        if out:\n",
    "            print_results(res[1])\n",
    "        ret.append(res)\n",
    "    return ret\n",
    "\n",
    "def advanced_arithmetic(emb, tok, queries, k=5, avg=True, dist=\"cosine\", out=True):\n",
    "    ret = []\n",
    "    ref_pairs = []\n",
    "    # Compute reference pairs for delta\n",
    "    for q in queries:\n",
    "        ref_q = q\n",
    "        if not isinstance(q[0], str):\n",
    "            ref_q = [qq[0] for qq in q]\n",
    "        ref_pairs.extend([(ref_q[0], ref_q[1])])\n",
    "    # Remove duplicate ref pairs\n",
    "    ref_pairs = set(frozenset(t) for t in ref_pairs)\n",
    "    delta = torch.mean(torch.stack([emb_arithmetic(emb, tok, [avgencode(emb, word, tok, avg=avg) for word in pair], k=1, dist=dist)[0] for pair in ref_pairs]).squeeze(), dim=0)\n",
    "    for q in queries:\n",
    "        if out:\n",
    "            print(\"##########################\")\n",
    "            # Print title\n",
    "            title_q = q\n",
    "            if not isinstance(q[0], str):\n",
    "                title_q = [qq[0] for qq in q]\n",
    "            print(f\"{title_q[0]} + Î” =\")\n",
    "        # Compute and print results\n",
    "        res = emb_arithmetic(emb, tok, [avgencode(emb, q[0], tok, avg=avg), 0, delta], k=k, dist=dist)\n",
    "        if out:\n",
    "            print_results(res[1])\n",
    "        ret.append(res)\n",
    "    return ret\n",
    "\n",
    "def evaluate_batch(results, solutions, out=True, score=\"rankscore\", k=None, tok=None, subdivide_tokens=False):\n",
    "    \n",
    "    def get_rank(r, s, out=0):\n",
    "        try:\n",
    "            return r.index(s)\n",
    "        except ValueError:\n",
    "            return out\n",
    "    \n",
    "    n = len(results[0][1])\n",
    "    ev = []\n",
    "    for res, sol in zip(list(map(lambda x: x[1], results)), solutions):\n",
    "        # If model encodes solutions with more than one token, add all the tokens separately\n",
    "        if subdivide_tokens and tok:\n",
    "            new_sol = []\n",
    "            for s in sol:\n",
    "                encoded_s = tok.encode(s, return_tensors=\"pt\", add_special_tokens=False).squeeze()\n",
    "                if encoded_s.size():\n",
    "                    new_sol.extend([tok.decode(token) for token in encoded_s])\n",
    "                else:\n",
    "                    new_sol.append(s)\n",
    "            sol = new_sol\n",
    "        # Get rank of each solution for each result outputs\n",
    "        ranks = [get_rank(res, s, out=n) for s in sol]\n",
    "        # Append best rank to final evaluation list\n",
    "        ev.append(min(ranks))\n",
    "    # Return score\n",
    "    if score == \"rankscore\":\n",
    "        score = 1 - ( sum(ev) / (n * len(solutions)) )\n",
    "    elif score == \"topk\":\n",
    "        if not k:\n",
    "            raise Exception(f\"Invalid k for topk\")\n",
    "        score = len([i for i in ev if i < k]) / len(ev)\n",
    "    else:\n",
    "        raise Exception(f\"Unknown Score\")\n",
    "    if out:\n",
    "        print(f\"{ev} -> {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_single = [\n",
    "    [\"Rome\", \"Italy\", \"France\"],\n",
    "    [\"Rome\", \"Italy\", \"Australia\"],\n",
    "    [\"Paris\", \"France\", \"Italy\"],\n",
    "    [\"Paris\", \"France\", \"Australia\"],\n",
    "    [\"Canberra\", \"Australia\", \"Italy\"],\n",
    "    [\"Canberra\", \"Australia\", \"France\"],\n",
    "]\n",
    "capital_advanced = [\n",
    "    [\"Rome\", \"Italy\"],\n",
    "    [\"Paris\", \"France\"],\n",
    "    [\"Canberra\", \"Australia\"],\n",
    "    [\"Ankara\", \"Turkey\"],\n",
    "    [\"Berlin\", \"Germany\"],\n",
    "    [\"Washington\", \"USA\"],\n",
    "    [\"Madrid\", \"Spain\"],\n",
    "    [\"Dublin\", \"Ireland\"],\n",
    "    [\"Copenaghen\", \"Denmark\"],\n",
    "    [\"Amsterdam\", \"Netherlands\"],\n",
    "    [\"Vienna\", \"Austria\"],\n",
    "    [\"Tokyo\", \"Japan\"],\n",
    "    [\"Seoul\", \"South Korea\"],\n",
    "]\n",
    "capital_sol = [\n",
    "    addall(\"Paris\"),\n",
    "    addall(\"Canberra\"),\n",
    "    addall(\"Rome\"),\n",
    "    addall(\"Canberra\"),\n",
    "    addall(\"Rome\"),\n",
    "    addall(\"Paris\"),\n",
    "]\n",
    "capital_advanced_sol = [\n",
    "    addall(\"Italy\"),\n",
    "    addall(\"France\"),\n",
    "    addall(\"Australia\"),\n",
    "    addall(\"Turkey\"),\n",
    "    addall(\"Germany\"),\n",
    "    addall(\"USA\") + addall(\"America\"),\n",
    "    addall(\"Spain\"),\n",
    "    addall(\"Ireland\"),\n",
    "    addall(\"Denmark\"),\n",
    "    addall(\"Netherlands\"),\n",
    "    addall(\"Austria\"),\n",
    "    addall(\"Japan\"),\n",
    "    addall(\"South Korea\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_single = [\n",
    "    [\"king\", \"man\", \"woman\"],\n",
    "    [\"queen\", \"woman\", \"man\"],\n",
    "    [\"prince\", \"man\", \"woman\"],\n",
    "    [\"princess\", \"woman\", \"man\"],\n",
    "    [\"priest\", \"man\", \"woman\"],\n",
    "    [\"nun\", \"woman\", \"man\"],\n",
    "]\n",
    "sex_sol = [\n",
    "    addall(\"Queen\"),\n",
    "    addall(\"King\"),\n",
    "    addall(\"Princess\"),\n",
    "    addall(\"Prince\"),\n",
    "    addall(\"Nun\"),\n",
    "    addall(\"Priest\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and Display Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Capital rankings\")\n",
    "for model_name, tok, in_emb_mod, out_emb_mod, format in zip(model_names, tokenizers, in_emb, out_emb, model_format):\n",
    "    print(\"---------------------------\")\n",
    "    print(model_name)\n",
    "    test_format = [ [format(el) for el in test] for test in capital_single]\n",
    "    test_capital = [\n",
    "        [\"Input Capital\", batch_emb_arithmetic(in_emb_mod, tok, test_format, k=100, out=False)],\n",
    "        [\"Output Capital\", batch_emb_arithmetic(out_emb_mod, tok, test_format, k=100, out=False)],\n",
    "    ]\n",
    "    for name, result in test_capital:\n",
    "        print(\"%%%\")\n",
    "        print(name)\n",
    "        evaluate_batch(result, capital_sol, tok=tok, subdivide_tokens=True)\n",
    "print(\"###############################\")\n",
    "print(\"Capital rankings\")\n",
    "for model_name, tok, in_emb_mod, out_emb_mod, format in zip(model_names, tokenizers, in_emb, out_emb, model_format):\n",
    "    print(\"---------------------------\")\n",
    "    print(model_name)\n",
    "    test_format = [ [format(el) for el in test] for test in sex_single]\n",
    "    test_sex = [\n",
    "        [\"Input Sex\", batch_emb_arithmetic(in_emb_mod, tok, test_format, k=100, out=False)],\n",
    "        [\"Output Sex\", batch_emb_arithmetic(out_emb_mod, tok, test_format, k=100, out=False)],\n",
    "    ]\n",
    "    for name, result in test_sex:\n",
    "        print(\"%%%\")\n",
    "        print(name)\n",
    "        evaluate_batch(result, sex_sol, tok=tok, subdivide_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = batch_emb_arithmetic(in_emb[index], tokenizers[index], [ [model_format[index](el) for el in test] for test in capital_single], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = batch_emb_arithmetic(out_emb[index], tokenizers[index], [ [model_format[index](el) for el in test] for test in capital_single], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = batch_emb_arithmetic(in_emb[index], tokenizers[index], [ [model_format[index](el) for el in test] for test in sex_single], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = batch_emb_arithmetic(out_emb[index], tokenizers[index], [ [model_format[index](el) for el in test] for test in sex_single], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_closest_emb(in_emb[index], model_format[index](\"nun\"), k=10, tok=tokenizers[index])\n",
    "print_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_closest_emb(out_emb[index], model_format[index](\"nun\"), k=10, tok=tokenizers[index])\n",
    "print_results(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_question_words(path):\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    data = {}\n",
    "    current_category = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Check if the line denotes a new category\n",
    "        if line.startswith(':'):\n",
    "            current_category = line[2:]\n",
    "            data[current_category] = []\n",
    "        else:\n",
    "            data[current_category].append(line.split())\n",
    "    # Create DataFrames for each category\n",
    "    dfs = {}\n",
    "    for category, attributes in data.items():\n",
    "        df = pd.DataFrame(attributes, columns=['A', 'B', 'Solution', 'C'])\n",
    "        # Reassign order\n",
    "        df = df.reindex(columns = ['A', 'B', 'C', 'Solution'])\n",
    "        dfs[category] = df\n",
    "    return dfs\n",
    "\n",
    "def change_words(batch, transform=lambda x: x):\n",
    "    return [[transform(word) for word in entry] for entry in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sim = pd.read_csv(datapath('wordsim353.tsv'), sep='\\t', skiprows=2, names=[\"Word1\", \"Word2\", \"Human\"])\n",
    "data_sim[\"Human\"] = round(data_sim[\"Human\"] / 10, 3)\n",
    "data_quest = load_question_words(datapath('questions-words.txt'))\n",
    "\n",
    "print(\"Word-Similarity Data\")\n",
    "print(data_sim.size)\n",
    "print(\"#############################################\")\n",
    "print(\"Question-Words Data\")\n",
    "for category, dataset in data_quest.items():\n",
    "    print(f\"{category:<25} \\t Size: {dataset.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sim_function(emb, tok, x, format, dist=\"cosine\"):\n",
    "    result = torch.squeeze(1 - calc_distance(emb, format(x[\"Word1\"]), format(x[\"Word2\"]), tok=tok, dist=dist))\n",
    "    result = torch.round(result, decimals=3)\n",
    "    return result.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = []\n",
    "for model_name, tok, in_emb_mod, out_emb_mod, format in zip(model_names, tokenizers, in_emb, out_emb, model_format):\n",
    "    # Calculate similarities\n",
    "    data_sim[model_name + \"_in\"] = data_sim[[\"Word1\", \"Word2\"]].apply(lambda x: word_sim_function(in_emb_mod, tok, x, format), axis=1).astype(float)\n",
    "    data_sim[model_name + \"_out\"] = data_sim[[\"Word1\", \"Word2\"]].apply(lambda x: word_sim_function(out_emb_mod, tok, x, format), axis=1).astype(float)\n",
    "    # Generate correlation coefficients\n",
    "    correlations.append((\n",
    "        [\n",
    "            pearsonr(data_sim[model_name + \"_in\"], data_sim[\"Human\"]),\n",
    "            spearmanr(data_sim[model_name + \"_in\"], data_sim[\"Human\"]),\n",
    "            kendalltau(data_sim[model_name + \"_in\"], data_sim[\"Human\"]),\n",
    "        ],\n",
    "        [\n",
    "            pearsonr(data_sim[model_name + \"_out\"], data_sim[\"Human\"]),\n",
    "            spearmanr(data_sim[model_name + \"_out\"], data_sim[\"Human\"]),\n",
    "            kendalltau(data_sim[model_name + \"_out\"], data_sim[\"Human\"]),\n",
    "        ]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, corr in zip(model_names, correlations):\n",
    "    print(\"###################################\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"%%%%\")\n",
    "    print(\"Input Embeddings\")\n",
    "    print(f\"{'Pearson Correlation Coefficient':<40} - {' r '} : {corr[0][0][0]:.3} {'pv'} : {corr[0][0][1]:.3}\")\n",
    "    print(f\"{'Spearman Correlation Coefficient':<40} - {'rho'} : {corr[0][1][0]:.3} {'pv'} : {corr[0][1][1]:.3}\")\n",
    "    print(f\"{'Kendall Correlation Coefficient':<40} - {'tau'} : {corr[0][2][0]:.3} {'pv'} : {corr[0][2][1]:.3}\")\n",
    "    print(\"%%%%\")\n",
    "    print(\"Output Embeddings\")\n",
    "    print(f\"{'Pearson Correlation Coefficient':<40} - {' r '} : {corr[1][0][0]:.3} {'pv'} : {corr[1][0][1]:.3}\")\n",
    "    print(f\"{'Spearman Correlation Coefficient':<40} - {'rho'} : {corr[1][1][0]:.3} {'pv'} : {corr[1][1][1]:.3}\")\n",
    "    print(f\"{'Kendall Correlation Coefficient':<40} - {'tau'} : {corr[1][2][0]:.3} {'pv'} : {corr[1][2][1]:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = []\n",
    "for model_name, tok, in_emb_mod, out_emb_mod, format in zip(model_names, tokenizers, in_emb, out_emb, model_format):\n",
    "    test_questions.append([\n",
    "        {\n",
    "            category: batch_emb_arithmetic(in_emb_mod, tok, change_words(dataset.values.tolist(), format), k=50, out=False)\n",
    "            for category, dataset in tqdm(data_quest.items())\n",
    "        },\n",
    "        {\n",
    "            category: batch_emb_arithmetic(out_emb_mod, tok, change_words(dataset.values.tolist(), format), k=50, out=False)\n",
    "            for category, dataset in tqdm(data_quest.items())\n",
    "        },\n",
    "    ])\n",
    "\n",
    "test_question_sol = [[addall(entry) for entry in dataset.iloc[:, -1].to_list()] for dataset in data_quest.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_k = [5, 10, 15, 25, 50]\n",
    "for category, sol in zip(data_quest.keys(),test_question_sol):\n",
    "    print(\"########################\")\n",
    "    print(category)\n",
    "    for model_name, tq, trace_idx in zip(model_names, test_questions, range(0, len(model_names)*2, 2)):\n",
    "        print(\"---------------------------\")\n",
    "        print(f\"{model_name} Input Rank Score: {evaluate_batch(tq[0][category], sol, out=False, score='rankscore'):.2f}\")\n",
    "        print(f\"{model_name} Output Rank Score: {evaluate_batch(tq[1][category], sol, out=False, score='rankscore'):.2f}\")\n",
    "        plt.plot(test_k, [evaluate_batch(tq[0][category], sol, out=False, score='topk', k=k) for k in test_k], \n",
    "                 marker='o', alpha=0.9, label=f'{model_name} Input Embeddings', c=clmp[\"Paired\"](trace_idx))\n",
    "        plt.plot(test_k, [evaluate_batch(tq[1][category], sol, out=False, score='topk', k=k) for k in test_k], \n",
    "                 marker='o', alpha=0.9, label=f'{model_name} Output Embeddings', c=clmp[\"Paired\"]((trace_idx+1)))\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xticks(test_k)\n",
    "        plt.title(category + \" Top-K Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_minor_locator(AutoMinorLocator(1))\n",
    "    plt.grid(linestyle = '--', linewidth = 0.5, which=\"minor\")\n",
    "    plt.grid(linestyle = '--', linewidth = 1, which=\"major\")\n",
    "    plt.savefig(f\"analogy_{category}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Capital rankings\")\n",
    "for model_name, tok, in_emb_mod, out_emb_mod, format in zip(model_names, tokenizers, in_emb, out_emb, model_format):\n",
    "    print(\"---------------------------\")\n",
    "    print(model_name)\n",
    "    test_format = [ [format(el) for el in test] for test in capital_advanced]\n",
    "    test_capital = [\n",
    "        [\"Input Capital\", advanced_arithmetic(in_emb_mod, tok, test_format, k=100, out=False)],\n",
    "        [\"Output Capital\", advanced_arithmetic(out_emb_mod, tok, test_format, k=100, out=False)],\n",
    "    ]\n",
    "    for name, result in test_capital:\n",
    "        print(\"%%%\")\n",
    "        print(name)\n",
    "        evaluate_batch(result, capital_advanced_sol, tok=tok, subdivide_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = []\n",
    "for model_name, tok, in_emb_mod, out_emb_mod, format in zip(model_names, tokenizers, in_emb, out_emb, model_format):\n",
    "    test_questions.append([\n",
    "        {\n",
    "            category: advanced_arithmetic(in_emb_mod, tok, change_words(dataset.values.tolist(), format), k=50, out=False)\n",
    "            for category, dataset in tqdm(data_quest.items())\n",
    "        },\n",
    "        {\n",
    "            category: advanced_arithmetic(out_emb_mod, tok, change_words(dataset.values.tolist(), format), k=50, out=False)\n",
    "            for category, dataset in tqdm(data_quest.items())\n",
    "        },\n",
    "    ])\n",
    "\n",
    "test_question_sol = [[addall(entry) for entry in dataset.iloc[:, 1].to_list()] for dataset in data_quest.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_k = [5, 10, 15, 25, 50]\n",
    "for category, sol in zip(data_quest.keys(),test_question_sol):\n",
    "    print(\"########################\")\n",
    "    print(category)\n",
    "    for model_name, tq, trace_idx in zip(model_names, test_questions, range(0, len(model_names)*2, 2)):\n",
    "        print(\"---------------------------\")\n",
    "        print(f\"{model_name} Input Rank Score: {evaluate_batch(tq[0][category], sol, out=False, score='rankscore'):.2f}\")\n",
    "        print(f\"{model_name} Output Rank Score: {evaluate_batch(tq[1][category], sol, out=False, score='rankscore'):.2f}\")\n",
    "        plt.plot(test_k, [evaluate_batch(tq[0][category], sol, out=False, score='topk', k=k) for k in test_k], \n",
    "                 marker='o', alpha=0.9, label=f'{model_name} Input Embeddings', c=clmp[\"Paired\"](trace_idx))\n",
    "        plt.plot(test_k, [evaluate_batch(tq[1][category], sol, out=False, score='topk', k=k) for k in test_k], \n",
    "                 marker='o', alpha=0.9, label=f'{model_name} Output Embeddings', c=clmp[\"Paired\"]((trace_idx+1)))\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xticks(test_k)\n",
    "        plt.title(category + \" Top-K Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_minor_locator(AutoMinorLocator(1))\n",
    "    plt.grid(linestyle = '--', linewidth = 0.5, which=\"minor\")\n",
    "    plt.grid(linestyle = '--', linewidth = 1, which=\"major\")\n",
    "    plt.savefig(f\"delta_analogy_{category}.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "NLP Thesis Kernel",
   "language": "python",
   "name": "nlp-thesis-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
