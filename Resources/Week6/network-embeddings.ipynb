{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7477792,"sourceType":"datasetVersion","datasetId":4352655}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops -q\n!pip install git+https://github.com/huggingface/transformers -q\n!pip install gradio==4.15.0 -q\n!pip install ngrok -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-25T11:53:28.714432Z","iopub.execute_input":"2024-01-25T11:53:28.715035Z","iopub.status.idle":"2024-01-25T11:55:02.765875Z","shell.execute_reply.started":"2024-01-25T11:53:28.714998Z","shell.execute_reply":"2024-01-25T11:55:02.764648Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\njupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflowjs 4.15.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport torch\nimport copy\nimport types\nimport traceback\nimport ngrok\nimport asyncio\nimport pickle\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm.notebook import tqdm\n\nfrom gensim.test.utils import datapath\n\nfrom scipy.stats import pearsonr, spearmanr\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n\nfrom kaggle_secrets import UserSecretsClient\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:02.767761Z","iopub.execute_input":"2024-01-25T11:55:02.768085Z","iopub.status.idle":"2024-01-25T11:55:15.610435Z","shell.execute_reply.started":"2024-01-25T11:55:02.768060Z","shell.execute_reply":"2024-01-25T11:55:15.609295Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Import Models","metadata":{}},{"cell_type":"code","source":"use_result_files = True\nresult_files_prefix = \"/kaggle/input/networkembeddingsresults/\"\ndevice = \"cpu\"\ntorch.set_default_device(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:15.611516Z","iopub.execute_input":"2024-01-25T11:55:15.612004Z","iopub.status.idle":"2024-01-25T11:55:15.621448Z","shell.execute_reply.started":"2024-01-25T11:55:15.611980Z","shell.execute_reply":"2024-01-25T11:55:15.620211Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"raw_phi = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\ntokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\nconfig_phi = AutoConfig.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n\nraw_phi = raw_phi.cpu()\nraw_phi = raw_phi.eval()\nraw_phi.zero_grad()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:15.625000Z","iopub.execute_input":"2024-01-25T11:55:15.625391Z","iopub.status.idle":"2024-01-25T11:55:33.360147Z","shell.execute_reply.started":"2024-01-25T11:55:15.625363Z","shell.execute_reply":"2024-01-25T11:55:33.358956Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/864 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3c807361a6f496589388102330e7bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6125635872314e18a7313607c3b9761a"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-1_5:\n- configuration_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc2daaecf114726818008ff68c055df"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-1_5:\n- modeling_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3acd6666b04a41bb971605d5a394d909"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead0fb427e564e77a2b20ed1de17b79c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d607da56d44ddd9733f4ba617d4618"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3fcad8249c5406d9e7e5fdaa99e141b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"449d0d14177a4b9c8f7d28907e629381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a6012152df47d48884a3aaf14bd4aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69dbe88e2eb84cb499ca1cb56440962a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6641794417b64d6484e5b5d464662ec1"}},"metadata":{}}]},{"cell_type":"code","source":"raw_gpt = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\nconfig_gpt = AutoConfig.from_pretrained(\"gpt2\", trust_remote_code=True)\n\nraw_gpt = raw_gpt.cpu()\nraw_gpt.eval()\nraw_gpt.zero_grad()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:33.361468Z","iopub.execute_input":"2024-01-25T11:55:33.361752Z","iopub.status.idle":"2024-01-25T11:55:38.270801Z","shell.execute_reply.started":"2024-01-25T11:55:33.361728Z","shell.execute_reply":"2024-01-25T11:55:38.269187Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7adbf3128249728dd32408538b70f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a8ebb660e74ace83471c23554a4dc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67801efaa0d24c0a83bb5f5c6ec3f1a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de321e8afbaf458380b1154967869282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57a1495a022d4f4bad3d4b711746f126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8fad0d47c9442d8a0031ca659f2a42"}},"metadata":{}}]},{"cell_type":"code","source":"class ModelWrapper:\n    def __init__(self, name, model, tokenizer=None, access_options={}):\n        self.__dict__[\"name\"] = name\n        self.__dict__[\"model\"] = model\n        self.__dict__[\"tokenizer\"] = tokenizer\n        self.__dict__[\"access_options\"] = access_options\n\n    def __getattr__(self, name):\n        if name in self.__dict__[\"access_options\"]:\n            opt = self.__dict__[\"access_options\"][name]\n            obj = self.__dict__[\"model\"]\n            if \"pre\" in opt:\n                obj = opt[\"pre\"](obj)\n            obj = getattr(obj, opt[\"get\"])\n            if isinstance(obj, types.MethodType): obj = obj()\n            return obj\n        return super().__getattr__(name)\n\n    def __setattr__(self, name, value):\n        if name in self.__dict__[\"access_options\"]:\n            opt = self.__dict__[\"access_options\"][name]\n            obj = self.__dict__[\"model\"]\n            if \"pre\" in opt:\n                obj = opt[\"pre\"](obj)\n            if isinstance(obj, types.MethodType): obj = obj()\n            return setattr(obj, opt[\"set\"], value) \n        return super().__setattr__(name, value)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:38.272681Z","iopub.execute_input":"2024-01-25T11:55:38.273130Z","iopub.status.idle":"2024-01-25T11:55:38.281928Z","shell.execute_reply.started":"2024-01-25T11:55:38.273095Z","shell.execute_reply":"2024-01-25T11:55:38.281054Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"phi = ModelWrapper(\"Phi 1.5\", raw_phi, tokenizer=tokenizer_phi, access_options={\n    \"transformer\": {\"get\": \"model\", \"set\": \"model\"}, \n    \"layers\": {\"get\": \"layers\", \"set\": \"layers\", \"pre\": lambda x: x.get_decoder()},\n    \"input_embeddings\": {\"get\": \"embed_tokens\", \"set\": \"embed_tokens\", \"pre\": lambda x: x.get_decoder()},\n    \"layer_norm\": {\"get\": \"final_layernorm\", \"set\": \"final_layernorm\", \"pre\": lambda x: x.get_decoder()}\n})\n\ngpt = ModelWrapper(\"GPT 2\", raw_gpt, tokenizer=tokenizer_gpt, access_options={\n    \"transformer\": {\"get\": \"transformer\", \"set\": \"transformer\"}, \n    \"layers\": {\"get\": \"h\", \"set\": \"h\", \"pre\": lambda x: x.transformer},\n    \"input_embeddings\": {\"get\": \"wte\", \"set\": \"wte\", \"pre\": lambda x: x.transformer},\n    \"layer_norm\": {\"get\": \"ln_f\", \"set\": \"ln_f\", \"pre\": lambda x: x.transformer}\n})","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:38.282836Z","iopub.execute_input":"2024-01-25T11:55:38.283150Z","iopub.status.idle":"2024-01-25T11:55:40.726607Z","shell.execute_reply.started":"2024-01-25T11:55:38.283125Z","shell.execute_reply":"2024-01-25T11:55:40.725463Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"models = [phi, gpt]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.728323Z","iopub.execute_input":"2024-01-25T11:55:40.728871Z","iopub.status.idle":"2024-01-25T11:55:40.739970Z","shell.execute_reply.started":"2024-01-25T11:55:40.728832Z","shell.execute_reply":"2024-01-25T11:55:40.738801Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Define Helper Functions","metadata":{}},{"cell_type":"markdown","source":"## Model Reduction","metadata":{}},{"cell_type":"code","source":"def model_reduction(model: ModelWrapper, max_layer=0, remove_norm=True, reset=False):\n    raw_model = model.model\n    if not hasattr(raw_model, '_original_layers'):\n        raw_model._original_layers = model.layers\n    if not hasattr(raw_model, '_layer_norm'):\n        raw_model._layer_norm = model.layer_norm\n    if reset:\n        model.layers = raw_model._original_layers\n        model.layer_norm = raw_model._layer_norm\n        return\n    current_list = model.layers\n    new_list = torch.nn.ModuleList()\n    included_layers = []\n    for i, layer in enumerate(current_list):\n        if i > max_layer:\n            break\n        included_layers.append(i)\n        new_list.append(layer)\n    model.layers = new_list\n    if remove_norm:\n        model.layer_norm = torch.nn.Identity()\n    return included_layers","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.741080Z","iopub.execute_input":"2024-01-25T11:55:40.742146Z","iopub.status.idle":"2024-01-25T11:55:40.754249Z","shell.execute_reply.started":"2024-01-25T11:55:40.742113Z","shell.execute_reply":"2024-01-25T11:55:40.752746Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def generate_layer_embeddings(model, layers, split_size=10000):\n    n = model.input_embeddings.num_embeddings\n    d = model.input_embeddings.embedding_dim \n    included_layers = model_reduction(model, max(layers), remove_norm=True)\n    layers_emb = [torch.nn.Embedding(n, d) for _ in included_layers]\n    try:\n        indexes = torch.tensor([[i] for i in range(0, n)])\n        chunked_indexes = torch.split(indexes, split_size)\n        emb_matrix = torch.Tensor().cpu()\n        with torch.no_grad():\n            for indexes in chunked_indexes:\n                hidden_states = model.transformer.forward(indexes, output_hidden_states=True).hidden_states\n                t = torch.stack([hidden_states[layer].detach().cpu() for layer in included_layers]).cpu()\n                del hidden_states\n                torch.cuda.empty_cache()\n                emb_matrix = torch.cat((emb_matrix, t), 1)\n            for layer in included_layers:\n                #layers_emb[layer].weight = torch.nn.Parameter(emb_matrix[layer].squeeze().cuda())\n                layers_emb[layer].weight = torch.nn.Parameter(emb_matrix[layer].squeeze())\n            del emb_matrix\n    except Exception as error:\n        print(\"An error occurred: \", error)\n        traceback.print_exc()\n    torch.cuda.empty_cache()\n    model_reduction(model, reset=True)\n    return layers_emb","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.757641Z","iopub.execute_input":"2024-01-25T11:55:40.758153Z","iopub.status.idle":"2024-01-25T11:55:40.772253Z","shell.execute_reply.started":"2024-01-25T11:55:40.758121Z","shell.execute_reply":"2024-01-25T11:55:40.770965Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Embedding Computations","metadata":{}},{"cell_type":"code","source":"def multiencode(tok, words, return_tensors=\"pt\"):\n    if (isinstance(words, list) or isinstance(words, tuple)) and not isinstance(words, str):\n        return torch.cat([tok.encode(word, return_tensors=\"pt\") for word in words], dim=-1)\n    else:\n        return tok.encode(words, return_tensors=\"pt\")\n    \ndef avgencode(emb, word, tok=None, avg=True):\n    source = word\n    # If input is a string tokenize it\n    if (isinstance(word, str) or isinstance(word[0], str)) and tok is not None:\n        word = emb(multiencode(tok, word))\n    # Calculate average if avg flag is true and if it is needed\n    if word.shape[1] != 1 and avg:\n        word = torch.unsqueeze(torch.mean(word, dim=1), dim=1)\n    elif word.shape[1] != 1 and not avg:\n        raise Exception(f\"{source} is not a single token: {word}\")\n    return word","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.773695Z","iopub.execute_input":"2024-01-25T11:55:40.774058Z","iopub.status.idle":"2024-01-25T11:55:40.787284Z","shell.execute_reply.started":"2024-01-25T11:55:40.774027Z","shell.execute_reply":"2024-01-25T11:55:40.786130Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def calc_distance(emb, word1, word2, tok=None, avg=True, dist=\"cosine\", multi=False):\n    # Encode and average (if multi is True, word1 represents the embedding matrix)\n    if not multi:\n        word1 = avgencode(emb, word1, tok, avg=avg)\n    word2 = avgencode(emb, word2, tok, avg=avg)\n    # Compute distances\n    if dist == \"L2\":\n        distances = torch.norm(word1 - word2, dim=2)\n    elif dist == \"cosine\":\n        cs = torch.nn.CosineSimilarity(dim=2)\n        distances = 1 - cs(word1, word2)\n    else:\n        raise Exception(\"Unknown distance\")\n    return distances\n\ndef get_closest_emb(emb, word, k=1, decode=True, tok=None, avg=True, dist=\"cosine\"):\n    # Compute distances\n    distances = calc_distance(emb, emb.weight.data, word, tok=tok, avg=avg, dist=dist, multi=True)\n    # Compute top k smalles indices\n    topk = torch.squeeze(torch.topk(distances, k=k, largest=False).indices)\n    # If one element, unsqueeze it\n    if k == 1:\n        topk = torch.unsqueeze(topk, dim=0)\n    # Decode closest k\n    if decode and tok is not None:\n        topk = [tok.decode(c) for c in topk.tolist()]\n    return topk\n\ndef emb_arithmetic(emb, tok, words, k=1, avg=True, dist=\"cosine\", return_solution=False):\n    # Encode and average\n    words_emb = [avgencode(emb, word, tok, avg=avg) for word in words]\n    # Compute embeddings\n    w1 = words_emb[0]\n    w2 = words_emb[1]\n    w3 = words_emb[2]\n    # Do embedding arithmetic\n    if dist == \"L2\":\n        w = torch.nn.functional.normalize(w1 - w2 + w3, dim=0)\n    else:\n        w = w1 - w2 + w3\n    # Get closest k\n    closest = get_closest_emb(emb, w, k=k, decode=True, tok=tok, dist=dist)\n    return (w, closest) if return_solution else closest\n\ndef word_sim_function(emb, tok, x, dist=\"cosine\"):\n    result = torch.squeeze(1 - calc_distance(emb, *x, tok=tok, dist=dist))\n    return result\n\ndef print_results(res):\n     for i, r in enumerate(res):\n        print(f\"{i+1}) {r}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.789191Z","iopub.execute_input":"2024-01-25T11:55:40.789477Z","iopub.status.idle":"2024-01-25T11:55:40.804651Z","shell.execute_reply.started":"2024-01-25T11:55:40.789453Z","shell.execute_reply":"2024-01-25T11:55:40.803793Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def batch_emb_arithmetic(emb, tok, queries, k=5, avg=True, dist=\"cosine\", out=True):\n    ret = []\n    for q in queries:\n        if out:\n            print(\"##########################\")\n            # Print title\n            title_q = q\n            if not isinstance(q[0], str):\n                title_q = [qq[0] for qq in q]\n            print(f\"{title_q[0]} - {title_q[1]} + {title_q[2]} =\")\n        # Compute and print results\n        res = emb_arithmetic(emb, tok, q, k=k, avg=avg, dist=dist)\n        if out:\n            print_results(res[1])\n        ret.append(res)\n    return ret\n\ndef evaluate_batch(results, solutions, out=True, score=\"rankscore\", k=None):\n    \n    def get_rank(r, s, out=0):\n        try:\n            return r.index(s)\n        except ValueError:\n            return out\n    \n    n = len(results[0])\n    ev = []\n    for res, sol in zip(results, solutions):\n        # Get rank of each solution for each result outputs\n        ranks = [get_rank(res, s, out=n) for s in sol]\n        # Append best rank to final evaluation list\n        ev.append(min(ranks))\n    # Return score\n    if score == \"rankscore\":\n        score = 1 - ( sum(ev) / (n * len(solutions)) )\n    elif score == \"topk\":\n        if not k:\n            raise Exception(f\"Invalid k for topk\")\n        score = len([i for i in ev if i < k]) / len(ev)\n    else:\n        raise Exception(f\"Unknown Score\")\n    if out:\n        print(f\"{ev} -> {score}\")\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.806014Z","iopub.execute_input":"2024-01-25T11:55:40.806575Z","iopub.status.idle":"2024-01-25T11:55:40.820297Z","shell.execute_reply.started":"2024-01-25T11:55:40.806536Z","shell.execute_reply":"2024-01-25T11:55:40.819260Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def test_arithmetic(models, questions, reduced_model_layers=[0], k=100, index_batch_size=10000):\n    results = {}\n    for model in tqdm(models):\n        torch.cuda.empty_cache()\n        model.model = model.model.cuda()\n        model_name = model.name\n        model_tokenizer = model.tokenizer\n        print(f\"Computing model {model_name}\")\n        results[model_name] = {}\n        reduced_embeddings = generate_layer_embeddings(model, reduced_model_layers, split_size=index_batch_size)\n        model.model = model.model.cpu()\n        torch.cuda.empty_cache()\n        reduced_embeddings = [emb.cuda() for emb in reduced_embeddings]\n        for question_name, question in tqdm(questions.items()):\n            question, solution = question\n            interest_layers = [layer for layer in reduced_model_layers if layer < len(reduced_embeddings)]\n            results[model_name][question_name] = {top_layer: batch_emb_arithmetic(reduced_embeddings[top_layer], model_tokenizer, question, k=k, out=False) for top_layer in interest_layers}\n        del reduced_embeddings\n    return results\n\ndef test_similarities(models, word_pairs, reduced_model_layers=[0], index_batch_size=10000, emb_batch_size=10):\n    results = {}\n    for model in tqdm(models):\n        torch.cuda.empty_cache()\n        model.model = model.model.cuda()\n        model_name = model.name\n        model_tokenizer = model.tokenizer\n        print(f\"Computing model {model_name}\")\n        reduced_embeddings = generate_layer_embeddings(model, reduced_model_layers, split_size=index_batch_size)\n        model.model = model.model.cpu()\n        torch.cuda.empty_cache()\n        reduced_embeddings = [reduced_embeddings[i:i + emb_batch_size] for i in range(0, len(reduced_embeddings), emb_batch_size)]\n        results[model_name] = {}\n        for i, emb_batch in enumerate(reduced_embeddings):\n            emb_batch = [emb.cuda() for emb in emb_batch]\n            delta_layer = i * emb_batch_size\n            max_layer = i * delta_layer + len(emb_batch)\n            interest_layers = [layer - delta_layer for layer in reduced_model_layers if layer >= delta_layer and layer < max_layer]\n            results[model_name] |= {top_layer + delta_layer: [word_sim_function(emb_batch[top_layer], model_tokenizer, word_pair).detach().cpu() for word_pair, solution in word_pairs] for top_layer in interest_layers}\n            del emb_batch\n            torch.cuda.empty_cache()\n        del reduced_embeddings\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.821450Z","iopub.execute_input":"2024-01-25T11:55:40.821912Z","iopub.status.idle":"2024-01-25T11:55:40.836321Z","shell.execute_reply.started":"2024-01-25T11:55:40.821863Z","shell.execute_reply":"2024-01-25T11:55:40.835109Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def save_results(results, filepath):\n    with open(filepath, 'wb') as fp:\n        pickle.dump(results, fp)\n        \ndef load_results(filepath):\n    with open(filepath, 'rb') as fp:\n        results = pickle.load(fp)\n        return results","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.837983Z","iopub.execute_input":"2024-01-25T11:55:40.838278Z","iopub.status.idle":"2024-01-25T11:55:40.856953Z","shell.execute_reply.started":"2024-01-25T11:55:40.838253Z","shell.execute_reply":"2024-01-25T11:55:40.855570Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"markdown","source":"## Load Gensim Data","metadata":{}},{"cell_type":"code","source":"addspace = lambda x: \" \" + x\naddall = lambda x: (x.capitalize(), \" \" + x.capitalize(), x.lower(), \" \" + x.lower())","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.858484Z","iopub.execute_input":"2024-01-25T11:55:40.859403Z","iopub.status.idle":"2024-01-25T11:55:40.924470Z","shell.execute_reply.started":"2024-01-25T11:55:40.859363Z","shell.execute_reply":"2024-01-25T11:55:40.922933Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def load_question_words(path):\n    with open(path, 'r') as file:\n        lines = file.readlines()\n    data = {}\n    current_category = None\n    for line in lines:\n        line = line.strip()\n        # Check if the line denotes a new category\n        if line.startswith(':'):\n            current_category = line[2:]\n            data[current_category] = []\n        else:\n            data[current_category].append(line.split())\n    # Create DataFrames for each category\n    dfs = {}\n    for category, attributes in data.items():\n        df = pd.DataFrame(attributes, columns=['A', 'B', 'Solution', 'C'])\n        # Reassign order\n        df = df.reindex(columns = ['A', 'B', 'C', 'Solution'])\n        dfs[category] = df\n    return dfs\n\ndef change_words(batch, transform=lambda x: x):\n    return [[transform(word) for word in entry] for entry in batch]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.926161Z","iopub.execute_input":"2024-01-25T11:55:40.926753Z","iopub.status.idle":"2024-01-25T11:55:40.988678Z","shell.execute_reply.started":"2024-01-25T11:55:40.926720Z","shell.execute_reply":"2024-01-25T11:55:40.987236Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data_quest = load_question_words(datapath('questions-words.txt'))\ndata_quest = {category: (dataset[[\"A\", \"B\", \"C\"]].apply(addspace), dataset[\"Solution\"].apply(addall)) for category, dataset in data_quest.items()}\nquestions = {category: (dataset[0][[\"A\", \"B\", \"C\"]].values.tolist(), dataset[1].values.tolist()) for category, dataset in data_quest.items()}","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:40.990727Z","iopub.execute_input":"2024-01-25T11:55:40.991417Z","iopub.status.idle":"2024-01-25T11:55:41.182941Z","shell.execute_reply.started":"2024-01-25T11:55:40.991373Z","shell.execute_reply":"2024-01-25T11:55:41.181231Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data_sim = pd.read_csv(datapath('wordsim353.tsv'), sep='\\t', skiprows=2, names=[\"Word1\", \"Word2\", \"Human\"])\ndata_sim = (data_sim[[\"Word1\", \"Word2\"]].apply(addspace).values.tolist(), (data_sim[\"Human\"] / 10).values.tolist())\nword_pairs = [(word_pair, solution) for word_pair, solution in zip(*data_sim)]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:41.184128Z","iopub.execute_input":"2024-01-25T11:55:41.185166Z","iopub.status.idle":"2024-01-25T11:55:41.202935Z","shell.execute_reply.started":"2024-01-25T11:55:41.185113Z","shell.execute_reply":"2024-01-25T11:55:41.201504Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Gradio Visualization","metadata":{}},{"cell_type":"code","source":"async def deploy_gradio(interface):\n    interface.launch(inline=False, share=False, prevent_thread_lock=True, show_error=True)\n    address = \"http://localhost:\" + str(interface.server_port)\n    listener = ngrok.forward(addr=address, authtoken=UserSecretsClient().get_secret(\"ngrok_key\"))\n    await asyncio.wait_for(listener, timeout=10)\n    public_url = listener.result().url()\n    print(f\"Deploy URL: {public_url}\")\n    return listener.result()\n\nasync def close_gradio(listener, interface):\n    interface.close()\n    await listener.close()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:41.204371Z","iopub.execute_input":"2024-01-25T11:55:41.204660Z","iopub.status.idle":"2024-01-25T11:55:41.337557Z","shell.execute_reply.started":"2024-01-25T11:55:41.204634Z","shell.execute_reply":"2024-01-25T11:55:41.336470Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def update_ar_plots(model_names, question_name, n_layers, score, k, update_average=False):\n    figures= []\n    f1 = plt.figure()\n    layers = [[layer for layer in ar_tests[model_name][question_name].keys() if layer < n_layers] for model_name in model_names]\n    layer_ticks = range(0, n_layers)\n    for model_name, model_layers in zip(model_names, layers):\n        r = []\n        for layer in model_layers:\n            r.append(evaluate_batch(ar_tests[model_name][question_name][layer], questions[question_name][1], out=False, score=score, k=k))\n        plt.plot(model_layers, r, marker='o', alpha=0.6, label=model_name)\n    plt.xticks(layer_ticks)\n    plt.title(question_name)\n    plt.legend()\n    figures.append(f1)\n    \n    if update_average:\n        f2 = plt.figure()\n        layers = [[layer for layer in ar_tests[model_name][(list(questions.keys())[0])].keys() if layer < n_layers] for model_name in model_names]\n        layer_ticks = range(0, n_layers)\n        for model_name, model_layers in zip(model_names, layers):\n            m = np.array([])\n            s = np.array([])\n            for layer in model_layers:\n                values = [evaluate_batch(ar_tests[model_name][question_name][layer], questions[question_name][1], out=False, score=score, k=k) for question_name in questions.keys()]\n                m = np.append(m, np.mean(values))\n                s = np.append(s, np.std(values))\n            plt.plot(model_layers, m, marker='o', alpha=0.6, label=model_name)\n            plt.fill_between(model_layers, m - s, m + s, alpha=0.3)\n        plt.xticks(layer_ticks)\n        plt.title(\"Average Plot\")\n        plt.legend()\n        figures.append(f2)\n    else:\n        figures.append(None)\n        \n    return figures\n\ndef update_sim_plots(model_names, n_layers, correlation):\n    f = plt.figure()\n    \n    layers = [[layer for layer in sim_tests[model_name] if layer < n_layers] for model_name in model_names]\n    layer_ticks = range(0, n_layers)\n    \n    ax1 = plt.gca()\n    ax1.set_ylabel(\"Correlation\")\n    \n    plt.xticks(layer_ticks)\n    ax2 = ax1.twinx()\n    ax2.set_ylabel(\"Similarity σ\")\n    plt.title(\"Similarity Plot\")\n    \n    for model_name, model_layers in zip(model_names, layers):\n        m = np.array([])\n        s = np.array([])\n        for layer in model_layers:\n            #print(sim_tests[model_name][layer])\n            if correlation == \"pearson\":\n                r, _ = pearsonr(sim_tests[model_name][layer], [sol for pair, sol in word_pairs])\n            elif correlation == \"spearman\":\n                r, _ = spearmanr(sim_tests[model_name][layer], [sol for pair, sol in word_pairs], nan_policy=\"propagate\")\n            s = np.append(s, np.std(sim_tests[model_name][layer]))\n            m = np.append(m, r)\n            #s = np.append(s, p)\n        p = ax1.plot(model_layers, m, marker='o', alpha=0.6, label=model_name)\n        ax2.plot(model_layers, s, \"--\", alpha=0.6, label=f\"{model_name} Similarity σ\", color=p[-1].get_color())\n        #plt.fill_between(model_layers, m - s, m + s, alpha=0.3)\n    ax1.legend()\n    return f","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:41.339115Z","iopub.execute_input":"2024-01-25T11:55:41.340429Z","iopub.status.idle":"2024-01-25T11:55:41.444947Z","shell.execute_reply.started":"2024-01-25T11:55:41.340354Z","shell.execute_reply":"2024-01-25T11:55:41.443958Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\n\nwith gr.Blocks(analytics_enabled=False, title=\"Test\") as demo:\n    \n    model_names = [model.name for model in models]\n    correlations = [\"pearson\", \"spearman\"]\n    scores = [\"rankscore\", \"topk\"]\n    \n    with gr.Accordion(\"Arithmetic\"):\n    \n        with gr.Row():\n\n            with gr.Column():\n                update_average = gr.Checkbox(label=\"Display Average Graph\", value=False)\n                question_options = gr.Dropdown(questions.keys(), label=\"Questions\", value=list(questions.keys())[0])\n                models_options = gr.CheckboxGroup(model_names, label=\"Models\")\n                n_layers = gr.Slider(minimum=0, maximum=24, step=1, label=\"Number of Layers\")\n                scoring_options = gr.Radio(scores, label=\"Score\", value=list(scores)[1])\n                topkscore_k = gr.Slider(minimum=1, maximum=100, step=1, label=\"Top-K Score Threshold\")\n\n            with gr.Column():\n                plot = gr.Plot(label=\"Plot\")\n                avg_plot = gr.Plot(label=\"Average Plot\")\n\n        inputs = [models_options, question_options, n_layers, scoring_options, topkscore_k, update_average]\n\n        # Update plot\n        update_average.change(update_ar_plots, inputs=inputs, outputs=[plot, avg_plot])\n        models_options.change(update_ar_plots, inputs=inputs, outputs=[plot, avg_plot])\n        question_options.change(update_ar_plots, inputs=inputs, outputs=[plot, avg_plot])\n        n_layers.change(update_ar_plots, inputs=inputs, outputs=[plot, avg_plot])\n        scoring_options.change(update_ar_plots, inputs=inputs, outputs=[plot, avg_plot])\n        topkscore_k.change(update_ar_plots, inputs=inputs, outputs=[plot, avg_plot])\n\n        # Element Visibility\n        scoring_options.change(lambda score: gr.update(visible=(score == \"topk\")), inputs=[scoring_options], outputs=[topkscore_k])\n        update_average.change(lambda average: gr.update(visible=average), inputs=[update_average], outputs=[avg_plot])\n        \n        # On load\n        demo.load(update_ar_plots, inputs=inputs, outputs=[plot, avg_plot])\n    \n    with gr.Accordion(\"Similarity\"):\n            \n        with gr.Row():\n\n            with gr.Column():\n                models_options = gr.CheckboxGroup(model_names, label=\"Models\")\n                n_layers = gr.Slider(minimum=0, maximum=24, step=1, label=\"Number of Layers\")\n                correlation_options = gr.Radio(correlations, label=\"Correlation\", value=list(correlations)[1])\n\n            with gr.Column():\n                plot = gr.Plot(label=\"Plot\")\n                \n        inputs = [models_options, n_layers, correlation_options]\n        \n        # Update plot\n        models_options.change(update_sim_plots, inputs=inputs, outputs=[plot])\n        n_layers.change(update_sim_plots, inputs=inputs, outputs=[plot])\n        correlation_options.change(update_sim_plots, inputs=inputs, outputs=[plot])\n    \n        # On load\n        demo.load(update_sim_plots, inputs=inputs, outputs=[plot])","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:41.447251Z","iopub.execute_input":"2024-01-25T11:55:41.448394Z","iopub.status.idle":"2024-01-25T11:55:44.844653Z","shell.execute_reply.started":"2024-01-25T11:55:41.448354Z","shell.execute_reply":"2024-01-25T11:55:44.843731Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Run Tests","metadata":{}},{"cell_type":"code","source":"if not use_result_files:\n    ar_tests = test_arithmetic(models, questions, reduced_model_layers=range(0, 15), k=100, index_batch_size=5000)\n    save_results(ar_tests, \"ar_results.pkl\")\nelse:\n    ar_tests = load_results(result_files_prefix + \"ar_results.pkl\")","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:44.845872Z","iopub.execute_input":"2024-01-25T11:55:44.846201Z","iopub.status.idle":"2024-01-25T11:55:59.113678Z","shell.execute_reply.started":"2024-01-25T11:55:44.846174Z","shell.execute_reply":"2024-01-25T11:55:59.112975Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"if not use_result_files:\n    sim_tests = test_similarities(models, word_pairs, reduced_model_layers=range(0, 24), index_batch_size=1100)\n    save_results(sim_tests, \"sim_results.pkl\")\nelse:\n    sim_tests = load_results(result_files_prefix + \"sim_results.pkl\")","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:55:59.114813Z","iopub.execute_input":"2024-01-25T11:55:59.115090Z","iopub.status.idle":"2024-01-25T11:56:00.231256Z","shell.execute_reply.started":"2024-01-25T11:55:59.115067Z","shell.execute_reply":"2024-01-25T11:56:00.230248Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"demo_listener = await deploy_gradio(demo)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:56:00.233343Z","iopub.execute_input":"2024-01-25T11:56:00.233839Z","iopub.status.idle":"2024-01-25T11:56:02.948296Z","shell.execute_reply.started":"2024-01-25T11:56:00.233785Z","shell.execute_reply":"2024-01-25T11:56:02.947664Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\nDeploy URL: https://c7e2-35-199-185-102.ngrok-free.app\n","output_type":"stream"}]},{"cell_type":"code","source":"# await close_gradio(demo_listener, demo)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:56:02.949252Z","iopub.execute_input":"2024-01-25T11:56:02.950434Z","iopub.status.idle":"2024-01-25T11:56:02.954597Z","shell.execute_reply.started":"2024-01-25T11:56:02.950404Z","shell.execute_reply":"2024-01-25T11:56:02.953718Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#model_names = [model.name for model in models]\n#question_names = [name for name in questions.keys()]\n#layers = [[layer for layer in tests[model_name][question_names[0]].keys()] for model_name in model_names]\n#max_layers = layers[np.argmax([len(l) for l in layers])]\n#plots = []\n#for question_name, quest_pair in questions.items():\n#    question, solution = quest_pair\n#    res = []\n#    for model, model_layers in zip(models, layers):\n#        r = []\n#        for layer in model_layers:\n#            r.append(evaluate_batch(tests[model.name][question_name][layer], solution, out=False))\n#            #r.append(evaluate_batch(tests[model.name][question_name][layer], solution, out=False, score=\"topk\", k=50))\n#        plt.plot(model_layers, r, marker='o', alpha=0.6, label=model.name)\n#    plt.xticks(max_layers)\n#    plt.title(question_name)\n#    plt.legend()\n#    plots.append(plt.gcf())\n#    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T11:56:02.956124Z","iopub.execute_input":"2024-01-25T11:56:02.957376Z","iopub.status.idle":"2024-01-25T11:56:02.968980Z","shell.execute_reply.started":"2024-01-25T11:56:02.957299Z","shell.execute_reply":"2024-01-25T11:56:02.967956Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_42/3547440256.py:3: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  f1 = plt.figure()\n","output_type":"stream"}]}]}