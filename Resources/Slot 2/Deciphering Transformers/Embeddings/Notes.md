# Meeting 30/5/24

- Ideas
    - Compare results to Word2Vec as a baseline
    - Point out the fact that there is no way to transform a transformer in Word2Vec, therefore, we are checking known properties of embeddings, but the fact that we are applying them to transformers is releveant and makes it novel
    - New embedding experiment
        - Create new embedding as "paris - france + italy" or similar
        - Ask "<embedding> is the capital of" to the model