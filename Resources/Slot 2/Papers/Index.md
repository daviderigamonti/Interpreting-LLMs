# A Primer on The Inner Workings of Transformer-based Language Models
## Topics
Complete survey on transformer interpretability techniques.
## Status
- Published: 2024
- Read: 18/05/24
- Annotated: 19/05/24
- Notes written: 23/05/24
## Type
Survey paper
## Resources
- [PDF file](./a_primer_on_the_inner_workings_of_transformer-based_language_models.pdf)
- [Annotated PDF file](./Annotated/a_primer_on_the_inner_workings_of_transformer-based_language_models_annotated.pdf)
- [Notes](./Notes/a_primer_on_the_inner_workings_of_transformer-based_language_models.md)

# Interpreting GPT: the logit lens
## Topics
Introduction of the logit lens to look and interpret transformer intermediate representations.
## Status
- Published: 2020
- Read: 20/05/24
- Annotated: 20/05/24
- Notes written: 21/05/24
## Type
Blog post
## Resources
- [Original Link](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)
- [PDF file](./interpreting_gpt_the_logit_lens.pdf)
- [Annotated PDF file](./Annotated/interpreting_gpt_the_logit_lens_annotated.pdf)
- [Notes](./Notes/interpreting_gpt_the_logit_lens.md)

# Eliciting Latent Predictions from Transformers with the Tuned Lens
## Topics
Introduction of the tuned lens as an improvement on the logit lens, using a learned affine transformation and a learned bias.
## Status
- Published: 2023
- Read: 20/05/24
- Annotated: 20/05/24
- Notes written: 21/05/24
## Type
Paper
## Resources
- [PDF file](./eliciting_latent_predictions_from_transformers_with_the_tuned_lens.pdf)
- [Annotated PDF file](./Annotated/eliciting_latent_predictions_from_transformers_with_the_tuned_lens_annotated.pdf)
- [Notes](./Notes/eliciting_latent_predictions_from_transformers_with_the_tuned_lens.md)

# Do Llamas Work in English? On the Latent Language of Multilingual Transformers
## Topics
Investigation on the transformers' internal hidden representation space bias towards english in the context of translation-oriented tasks.
## Status
- Published: 2024
- Read: 20/05/24
- Annotated: 21/05/24
- Notes written: 21/05/24
## Type
Paper
## Resources
- [PDF file](./do_llamas_work_in_english.pdf)
- [Annotated PDF file](./Annotated/do_llamas_work_in_english_annotated.pdf)
- [Notes](./Notes/do_llamas_work_in_english.md)

# All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality
## Topics
Analysis of the presence, influence and solutions for rogue dimensions inside the embeddings space.
## Status
- Published: 2021
- Read: 20/05/24
- Annotated: 20/05/24
- Notes written: 22/05/24
## Type
Paper
## Resources
- [PDF file](./all_bark_and_no_bite_rogue_dimensions_in_transformer_language.pdf)
- [Annotated PDF file](./Annotated/all_bark_and_no_bite_rogue_dimensions_in_transformer_language_annotated.pdf)
- [Notes](./Notes/all_bark_and_no_bite_rogue_dimensions_in_transformer_language.md)

# Emergent and Predictable Memorization in Large Language Models
## Topics
Scaling analysis for predicting and limiting memorization in Large Language Models.
## Status
- Published: 2023
- Read: 20/05/24
- Annotated: 20/05/24
- Notes written: 22/05/24
## Type
Paper
## Resources
- [PDF file](./emergent_and_predictable_memorization_in_large_language_models.pdf)
- [Annotated PDF file](./Annotated/emergent_and_predictable_memorization_in_large_language_models_annotated.pdf)
- [Notes](./Notes/emergent_and_predictable_memorization_in_large_language_models.md)